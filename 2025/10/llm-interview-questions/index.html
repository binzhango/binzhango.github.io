<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Bin Zhang"><link href=http://binzhango.github.io/2025/10/llm-interview-questions/ rel=canonical><link href=../llm-training-epoch/ rel=prev><link rel=icon href=../../../assets/favicon.ico><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.9"><title>LLM Interview Questions - B~~~~~Z</title><link rel=stylesheet href=../../../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CFira+Code:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Fira Code"}</style><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-HLMVZHNEWM"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-HLMVZHNEWM",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-HLMVZHNEWM",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="LLM Interview Questions - B~~~~~Z"><meta property=og:description content=None><meta property=og:image content=http://binzhango.github.io/assets/images/social/posts/2025/10-30-LLM-Interview.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta content=http://binzhango.github.io/2025/10/llm-interview-questions/ property=og:url><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="LLM Interview Questions - B~~~~~Z"><meta name=twitter:description content=None><meta name=twitter:image content=http://binzhango.github.io/assets/images/social/posts/2025/10-30-LLM-Interview.png> <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=light-blue data-md-color-accent=orange> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#questions class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title=B~~~~~Z class="md-header__button md-logo" aria-label=B~~~~~Z data-md-component=logo> <img src=../../../assets/favicon.ico alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> B~~~~~Z </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LLM Interview Questions </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=light-blue data-md-color-accent=orange aria-label="Dark Mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Dark Mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=orange aria-label="Light Mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Light Mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/binzhango/binzhango.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> binzhango/binzhango.github.io </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class="md-tabs__item md-tabs__item--active"> <a href=../../.. class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../ class=md-tabs__link> Archive </a> </li> <li class=md-tabs__item> <a href=../../../category/azure/ class=md-tabs__link> Categories </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title=B~~~~~Z class="md-nav__button md-logo" aria-label=B~~~~~Z data-md-component=logo> <img src=../../../assets/favicon.ico alt=logo> </a> B~~~~~Z </label> <div class=md-nav__source> <a href=https://github.com/binzhango/binzhango.github.io title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg> </div> <div class=md-source__repository> binzhango/binzhango.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Blog </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../ class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=../../../2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../../2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../2020/ class=md-nav__link> <span class=md-ellipsis> 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../2012/ class=md-nav__link> <span class=md-ellipsis> 2012 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../category/azure/ class=md-nav__link> <span class=md-ellipsis> Azure </span> </a> </li> <li class=md-nav__item> <a href=../../../category/llm/ class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=../../../category/ml/ class=md-nav__link> <span class=md-ellipsis> ML </span> </a> </li> <li class=md-nav__item> <a href=../../../category/scala/ class=md-nav__link> <span class=md-ellipsis> Scala </span> </a> </li> <li class=md-nav__item> <a href=../../../category/snowflake/ class=md-nav__link> <span class=md-ellipsis> Snowflake </span> </a> </li> <li class=md-nav__item> <a href=../../../category/airflow/ class=md-nav__link> <span class=md-ellipsis> airflow </span> </a> </li> <li class=md-nav__item> <a href=../../../category/k8s/ class=md-nav__link> <span class=md-ellipsis> k8s </span> </a> </li> <li class=md-nav__item> <a href=../../../category/python/ class=md-nav__link> <span class=md-ellipsis> python </span> </a> </li> <li class=md-nav__item> <a href=../../../category/spark/ class=md-nav__link> <span class=md-ellipsis> spark </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src=../../../assets/favicon.ico alt="Bin Zhang"> </span> <span class=md-profile__description> <strong> Bin Zhang </strong> <br> Data Engineer </span> </div> </div> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2025-10-30 00:00:00+00:00" class=md-ellipsis>October 30, 2025</time> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=../../../category/llm/ >LLM</a></span> </div> </li> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 17 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#machine-learning class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=#llm-fundamentals class=md-nav__link> <span class=md-ellipsis> LLM Fundamentals </span> </a> </li> <li class=md-nav__item> <a href=#fundamentals-of-large-language-models-llms class=md-nav__link> <span class=md-ellipsis> Fundamentals of Large Language Models (LLMs) </span> </a> </li> </ul> </nav> </div> </div> </div> <article class="md-content__inner md-typeset"> <div><h1 id=questions>Questions<a class=headerlink href=#questions title="Permanent link">#</a></h1> <!-- more --> <h2 id=machine-learning>Machine Learning<a class=headerlink href=#machine-learning title="Permanent link">#</a></h2> <details class=tip> <summary>Machine Learning Concepts</summary> <details class=question> <summary>How would you describe the concept of machine learning in your own words?</summary> <p>Machine learning focuses on creating systems that improve their performance on a task by learning patterns from data rather than relying on explicit programming.</p> </details> <details class=question> <summary>Can you give a few examples of real-world areas where machine learning is particularly effective?</summary> <p>Machine learning is especially valuable for solving complex problems without clear rule-based solutions, automating decision-making instead of hand-crafted logic, adapting to changing environments, and extracting insights from large datasets.</p> </details> <details class=question> <summary>What are some typical problems addressed with unsupervised learning methods?</summary> <p>Typical unsupervised learning tasks include clustering, data visualization, dimensionality reduction, and association rule mining.</p> </details> <details class=question> <summary>Would detecting spam emails be treated as a supervised or unsupervised learning problem, and why?</summary> <p>Spam filtering is an example of a supervised learning problem because the model learns from examples of emails labeled as "spam" or "not spam".</p> </details> <details class=question> <summary>What does the term ‘out-of-core learning’ refer to in machine learning?</summary> <p>Out-of-core learning enables training on datasets too large to fit in memory by processing them in smaller chunks (mini-batches) and updating the model incrementally.</p> </details> <details class=question> <summary>How can you distinguish between model parameters and hyperparameters?</summary> <ul> <li> <p><strong>Model parameters</strong> define how the model behaves and are learned during training (e.g., weights in linear regression).</p> </li> <li> <p><strong>Hyperparameters</strong> are external settings chosen before training, such as the learning rate or regularization strength.</p> </li> </ul> </details> <details class=question> <summary>What are some major difficulties or limitations commonly faced when building machine learning systems?</summary> <div class="language-text highlight"><pre><span></span><code>Key challenges in machine learning include

- insufficient or low-quality data
- poor feature selection
- non-representative samples
- models that either underfit (too simple) or overfit (too complex)
</code></pre></div> </details> <details class=question> <summary>If a model performs well on training data but poorly on unseen data, what issue is occurring, and how might you address it?</summary> <p>When a model performs well on training data but poorly on unseen examples, it’s overfitting. This can be mitigated by collecting more diverse data, simplifying the model, applying regularization, or cleaning noisy data.</p> </details> <details class=question> <summary>What is a test dataset used for, and why is it essential in evaluating a model’s performance?</summary> <p>A test set provides an unbiased estimate of how well a model will perform on new, real-world data before deployment.</p> </details> <details class=question> <summary>What role does a validation set play during the model development process?</summary> <p>A validation set helps compare multiple models and tune hyperparameters, ensuring better generalization to unseen data.</p> </details> <details class=question> <summary>What is a train-dev dataset, in what situations would you create one, and how is it applied during model evaluation?</summary> <p>The train-dev set is a small portion of the training data set aside to identify mismatches between the training distribution and the validation/test distributions. You use it when you suspect that your production data may differ from your training data. The model is trained on most of the training data and evaluated on the train-dev set to detect overfitting or data mismatch before comparing results on the validation set.</p> </details> <details class=question> <summary>Why is it problematic to adjust hyperparameters based on test set performance?</summary> <p>If you tune hyperparameters using the test set, you risk overfitting to that specific test data, making your performance results misleadingly high. As a result, the model might perform worse in real-world scenarios because the test set is no longer an unbiased measure of generalization.</p> </details> </details> <h2 id=llm-fundamentals>LLM Fundamentals<a class=headerlink href=#llm-fundamentals title="Permanent link">#</a></h2> <details class=question> <summary>Explain bias-variance tradeoff. How does it manifest in LLMs?</summary> <ul> <li> <p><span class=def-mono-red>Bias</span>: Error from incorrect assumption in the model</p> <ul> <li>High bias leads to underfitting, where the model fails to capture patterns in the training data</li> </ul> </li> <li> <p><span class=def-mono-red>Variance</span>: Error from sensitivity to small fluctuations in the training data</p> <ul> <li>High variance leads to overfitting, where the model memorizes noise instead of learning generalization patterns</li> </ul> </li> </ul> <p>The bias-variance tradeoff in ML describe the tension between <strong>ability to fit training data</strong> and <strong>ability to generalize the new data</strong></p> <p><span class=def-mono-blue>bias-variance in LLM</span>:</p> <ul> <li> <p><span class=def-mono-gold>Model Parameters: Capacity vs. Overfitting</span></p> <ul> <li><strong>Too few parameters</strong>: A model with insufficient (e.g. small transformer) cannot capture complex patterns in the data, leading to high bias.<blockquote> <p>A small LLM might fail to understand language or generate coherent long texts.</p> </blockquote> </li> <li><strong>Too many parameters</strong>: A model with excessive capacity risks overfitting to training data, memorizing noise and details instead of learning generalizable patterns<blockquote> <p>A large LLM fine-tuned on a small dataset may generate text that is statistically similar to the training data but lack coherence and factual accuracy. (e.g. <span class=def-mono-red>hallucinations</span>)</p> </blockquote> </li> <li><strong>Balancing Act</strong>:<blockquote> <p>More parameters reduce bias by enabling the model to capture complex patterns but increase variance if not regularized. Regularization techniques: (e.g dropout, weight decay) help mitigate overfitting in high-parameter models</p> </blockquote> </li> </ul> </li> <li> <p><span class=def-mono-gold>Training Epochs: Learning Duration vs. Overfitting</span></p> <ul> <li><strong>Too few epochs</strong>: The model hasn't learned enough from the data, leading to high bias.<blockquote> <p>A transformer trained for only 1 epoch may fail to capture meaningful relationships in the text.</p> </blockquote> </li> <li><strong>Too many epochs</strong>: The model starts memorizing training data, increasing variance. This is common in transformer with high capacity and small datasets<blockquote> <p>A transformer fine-tuned on a medical dataset for 100 epochs may overfit to rare cases, leading to poor generalization.</p> </blockquote> </li> <li><strong>Tradeoff in Transformers</strong><blockquote> <p>Training loss decreases with epochs (low bias), but validation loss eventually increase (high variance).</p> <p>Early stopping is critical for transformers to avoid overfitting, especially when training on small or noisy datasets.</p> </blockquote> </li> </ul> </li> <li><span class=def-mono-gold>Noise vs Representativeness </span><ul> <li><strong>Low-quality data</strong>: Noisy, biased, or incomplete data prevents the model from learning accurate patterns, increasing biase.<blockquote> <p>A transformer trained on a dataset with limited examples of rare diseases may fail to diagnose them accurately</p> </blockquote> </li> <li><strong>Noisy/unrepresentative data</strong>: The model learns inconsistent patterns, increasing variance.<blockquote> <p>A dataset with duplicate or corrupted text may cause the model to overfit. A transformer trained on a dataset with biased political content may generate polarized outputs. Data augmentation (e.g. paraphrasing, back-translation) increases diversity, mitigating overfitting</p> </blockquote> </li> </ul> </li> </ul> </details> <details class=question> <summary>What is the difference between L1 and L2 regularization? When would you use elastic net in an LLM fine-tune?</summary> <p><span class=def-mono-gold>Regularization</span> adds a penalty term to the loss function so that the optimizer favours simpler or smoother solutions. In practice it is usually added to a model‑level loss (cross‑entropy, MSE, …) as a separate scalar that scales with the weights.</p> <div class=arithmatex>\[ \text{Loss}_{\text{regularized}} = \text{Loss}_{\text{original}} + \lambda \cdot \text{Penalty}(w) \]</div> <table> <thead> <tr> <th style="text-align: right;">Feature</th> <th style="text-align: center;">L1 (Lasso)</th> <th style="text-align: center;">L2 (Ridge)</th> </tr> </thead> <tbody> <tr> <td style="text-align: right;">Weight Behavior</td> <td style="text-align: center;">Many → 0 (sparse)</td> <td style="text-align: center;">"All → small, non-zero"</td> </tr> <tr> <td style="text-align: right;">Feature Selection</td> <td style="text-align: center;">Yes</td> <td style="text-align: center;">No</td> </tr> <tr> <td style="text-align: right;">Solution</td> <td style="text-align: center;">Not always unique</td> <td style="text-align: center;">Always unique</td> </tr> <tr> <td style="text-align: right;">Robust to Outliers</td> <td style="text-align: center;">Less</td> <td style="text-align: center;">More</td> </tr> </tbody> </table> <p><span class=def-mono-red> Key Insight:</span></p> <ul> <li><span class=def-mono-blue>L1 regularization</span> is more robust to outliers in the <strong>data (target outliers)</strong></li> <li><span class=def-mono-blue>L2 regularization</span> is more robust to outliers in the <strong>features (collinearity)</strong></li> </ul> <p><span class=def-mono-red>L1/L2 in LLM</span>:</p> <ul> <li>Use L2 by default. Use L1 if you want sparse, interpretable updates.</li> <li>L2 keeps updates smooth. L1 keeps updates minimal — and that’s often better for deployment.</li> <li>Use L2 to win benchmarks. Use L1 to ship to users.<blockquote> <ol> <li>Sparse LoRA = Tiny Adapters</li> <li>Faster Inference (Real Speedup!)</li> <li>Better Generalization (Less Overfitting)</li> <li>Interpretable Fine-Tuning</li> <li>Clean Model Merging</li> </ol> </blockquote> </li> </ul> <div class="language-sh highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=w>  </span>┌──────────────────────┐
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=w>  </span>│<span class=w> </span>Fine-tuning<span class=w> </span>an<span class=w> </span>LLM?<span class=w>  </span>│
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=w>  </span>└────────┬─────────────┘
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=w>          </span>│
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a><span class=w>          </span>▼
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a><span class=w>  </span>┌──────────────────────┐<span class=w>     </span>YES<span class=w> </span>→<span class=w> </span>Use<span class=w> </span>L2<span class=w> </span><span class=o>(</span><span class=nv>weight_decay</span><span class=o>=</span><span class=m>0</span>.01<span class=o>)</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a><span class=w>  </span>│<span class=w> </span>Large,<span class=w> </span>clean<span class=w> </span>data?<span class=w>   </span>│───NO──►┐
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a><span class=w>  </span>└────────┬─────────────┘<span class=w>        </span>│
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a><span class=w>          </span>│<span class=w>                    </span>│
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a><span class=w>          </span>▼<span class=w>                    </span>▼
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a><span class=w>  </span>┌──────────────────────┐<span class=w> </span>┌──────────────────────────┐
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a><span class=w>  </span>│<span class=w> </span>Need<span class=w> </span>max<span class=w> </span>accuracy?<span class=w>   </span>│<span class=w> </span>│<span class=w> </span>Want<span class=w> </span>small/fast<span class=w> </span>model?<span class=w>   </span>│
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=w>  </span>└────────┬─────────────┘<span class=w> </span>└────────────┬─────────────┘
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=w>          </span>│<span class=w>                          </span>│
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a><span class=w>          </span>YES<span class=w>                        </span>YES
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a><span class=w>          </span>│<span class=w>                          </span>│
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a><span class=w>          </span>▼<span class=w>                          </span>▼
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a><span class=w>      </span>Use<span class=w> </span>L2<span class=w>                     </span>Use<span class=w> </span>L1<span class=w> </span><span class=o>(</span>+<span class=w> </span>pruning<span class=o>)</span>
</span></code></pre></div> </details> <details class=question> <summary>Prove that dropout is equivalent to an ensemble during inference (hint: geometric distribution).</summary> <ul> <li>Where dropout appears in a Transformer<ul> <li>Attention dropout</li> <li>Feedforward dropout</li> <li>Residual dropout</li> </ul> </li> <li>The ensemble view of dropout in a Transformer<ul> <li>Each layer (and even each neuron) may be dropped independently.</li> <li>A particular dropout mask <span class=arithmatex>\(m = (m^{(1)}, m^{(2)}, \dots, m^{(L)})\)</span> defines one specific subnetwork (one “member” of the ensemble).</li> </ul> </li> </ul> <p><strong>During training:</strong> Randomly turn off some neurons (like flipping a coin for each one). This forces the network to learn many different "sub-networks" — each time you train, a different combination of neurons is active.</p> <p><strong>During testing (inference):</strong> Instead of picking one sub-network, we use all neurons, but scale down their strength (usually by half if dropout rate is 50%). This is the "mean network."</p> <p><span class=def-mono-gold>Why this is like an ensemble:</span> Imagine you could run the model 1,000 times (or <span class=arithmatex>\(2^{(N)}\)</span> times for N neurons), each time with a different random set of neurons turned off, and then average all their predictions. <strong>That would be a huge ensemble of sub-networks</strong> — very accurate, but way too slow. Dropout’s trick: Using the scaled "mean network" at test time gives exactly the same prediction as if you had averaged the geometric mean of all those possible sub-networks.</p> <p><span class=def-mono-blue>Dropout = training lots of sub-networks, inference = using their collective average — fast and smart.</span></p> </details> <details class=question> <summary>What is the curse of dimensionality? How do positional encodings mitigate it in Transformers?</summary> <p><span class=def-mono-blue>Higher dimensions → sparser data → harder to learn meaningful relationships.</span></p> <p>The curse of dimensionality refers to the set of problems that arise when data or model representations exist in high-dimensional spaces.</p> <ul> <li><strong>Data sparsity:</strong> Points become exponentially sparse — distances between points tend to concentrate, making similarity less meaningful.</li> <li><strong>Combinatorial explosion:</strong> The volume of the space grows exponentially <span class=arithmatex>\(O(k^{(d)})\)</span>, so covering it requires exponentially more data.</li> <li><strong>Poor generalization:</strong> Models struggle to learn smooth mappings because there’s too little data to constrain the high-dimensional space.</li> </ul> <p><strong>Token in Transformers</strong> Transformers process tokens as vectors in a <strong>high-dimensional</strong> embedding space (e.g., 768 or 4096 dimensions). However — <em>self-attention</em> treats each token as a set element rather than a sequence element. The attention mechanism itself has no built-in sense of order. <em>The model only knows “content similarity,” not which token came first or last.</em></p> <p>Without order, the model would need to <strong>learn positional relationships implicitly</strong> across high-dimensional embeddings. That’s hard — and it exacerbates the curse of dimensionality because:</p> <ul> <li>There’s no geometric bias for position.</li> <li>Each token embedding can drift freely in a massive space.</li> <li>The model must infer ordering purely from statistical co-occurrence — <em>requiring more data and more parameters.</em></li> </ul> <p><strong>How Positional Encodings Help</strong></p> <p><strong>Positional encodings (PEs)</strong> inject structured, low-dimensional information about sequence order directly into the embeddings. - Adds a geometric bias to embeddings — nearby positions have nearby encodings. - Reduces the effective search space — positions are no longer independent random vectors. - Enables extrapolation: the sinusoidal pattern generalizes beyond training positions. - The model can compute relative positions via linear operations (e.g., dot products of PEs reflect distance).</p> </details> <details class=question> <summary>Explain maximum likelihood estimation for language modeling.</summary> <p>Training a neural LM (like a Transformer) by minimizing the negative log-likelihood (NLL) is the same as maximizing the likelihood:</p> <div class=arithmatex>\[\boxed{ \text{Maximizing likelihood} \;\; \Leftrightarrow \;\; \text{Maximizing log-likelihood} \;\; \Leftrightarrow \;\; \text{Minimizing negative log-likelihood} }\]</div> <p><strong>Example</strong></p> <blockquote> <p>Sentence: "The cat sat on the mat."</p> <p>The MLE objective trains the model to maximize: <span class=arithmatex>\(P(\text{The}) \cdot P(\text{cat}|\text{The}) \cdot P(\text{sat}|\text{The cat}) \cdot P(\text{on}|\text{The cat sat}) \cdot \dots\)</span></p> </blockquote> </details> <details class=question> <summary>What is negative log-likelihood? Write the per-token loss for GPT.</summary> <div class=arithmatex>\[\boxed{ \ell(\theta) = \sum_{t=1}^T \log P(x_t \mid x_{&lt;t}; \theta) } \]</div> <div class=arithmatex>\[\boxed{ \text{NLL}(\theta) = -\ell(\theta) } \]</div> <div class=arithmatex>\[\boxed{ \text{NLL}(\theta) = -\sum_{t=1}^T \log P(x_t \mid x_{&lt;t}; \theta) } \]</div> <p><strong>where</strong> <span class=arithmatex>\(x_{&lt;t}\)</span> means <strong>All tokens</strong> before <span class=arithmatex>\(t\)</span>: <span class=arithmatex>\(x_1, \dots, x_{t-1}\)</span> <!-- <span class="arithmatex">\(klzzwxh:0058\)</span> --></p> <p>This is the heart of autoregressive language modeling — like GPT!</p> </details> <details class=question> <summary>Compare cross-entropy, perplexity, and BLEU. When is perplexity misleading?</summary> <ol> <li><strong>Cross-Entropy:</strong> Cross-entropy measures how well a probabilistic model predicts a target distribution — in LM, how well the model assigns high probability to the correct next tokens.</li> <li><strong>Perplexity:</strong> Perplexity (PPL) is simply the exponentiation of the cross-entropy</li> <li><strong>BLEU (Bilingual Evaluation Understudy):</strong> BLEU is an n-gram overlap metric for evaluating machine translation or text generation quality against reference texts</li> </ol> <p><span class=def-mono-blue>Perplexity is rephrasing cross-entropy in a more intuitive, more human-readable.</span></p> <blockquote> <p><strong>Perplexity = "How predictable is the language?"</strong></p> <p><strong>BLEU = "How much does the output match a reference?"</strong></p> <p>Example:</p> <p>Reference: "The cat is on the mat."</p> <p>Model output: "The dog is on the mat."</p> <p>→ Low perplexity (grammatical, fluent)</p> <p>→ Low BLEU (wrong content) <strong>BLEU is non-probabilistic and reference-based — unlike cross-entropy and perplexity.</strong></p> </blockquote> <p>⚠️ <span class=def-mono-red>When Perplexity Is Misleading???</span></p> <p><strong>Perplexity only measures how well the model predicts tokens probabilistically — not how meaningful or correct the generated text is.</strong></p> <ul> <li>Different tokenizations or vocabularies<ul> <li>A model with smaller tokens or subwords might have lower perplexity just because predictions are more granular, not actually better linguistically.</li> </ul> </li> <li>Domain mismatch<ul> <li>A model trained on Wikipedia might have low perplexity on Wikipedia text but produce incoherent answers to questions — it knows probabilities, not task structure.</li> </ul> </li> <li>Human-aligned vs statistical objectives<ul> <li>A model can assign high likelihood to grammatical but dull continuations (e.g., “The cat sat on the mat”) while rejecting creative or rare but correct continuations — good perplexity, poor real-world usefulness.</li> </ul> </li> <li>Non-autoregressive or non-likelihood models<ul> <li>For encoder-decoder or retrieval-augmented systems, perplexity may not correlate with generation quality because these models are not optimized purely for next-token prediction.</li> </ul> </li> <li>Overfitting<ul> <li>A model with very low perplexity on training data may memorize text, but generalize poorly (BLEU or human eval drops).</li> </ul> </li> </ul> </details> <details class=question> <summary>Why is label smoothing used in LLMs? Derive its modified loss?</summary> <p><strong>Label smoothing is used in LLMs to prevent overconfidence and improve generalization.</strong></p> <p>Instead of training on a <strong>one-hot</strong> target (where the correct token has probability 1 and all others 0), a small portion ε of that probability is spread across all other tokens.</p> <p>So the true token gets (1 − ε) probability, and the rest share ε uniformly.</p> <p>This changes the loss from the usual “−log p(correct token)” to a mix of:</p> <ul> <li><code>(1 − ε) × loss</code> for the correct token, and</li> <li><code>ε × average loss</code> over all tokens.</li> </ul> </details> <details class=question> <summary>What is the difference between hard and soft attention?</summary> <ul> <li>Hard attention → discrete, selective, non-differentiable.</li> <li>Soft attention → continuous, weighted, differentiable.</li> </ul> </details> <h2 id=fundamentals-of-large-language-models-llms>Fundamentals of Large Language Models (LLMs)<a class=headerlink href=#fundamentals-of-large-language-models-llms title="Permanent link">#</a></h2> <div class="admonition abstract"> <p class=admonition-title>Question Bank</p> <ul> <li><span class=def-mono-red>Fundamentals of Large Language Models (LLMs)</span></li> </ul> <details class=tip> <summary>LLM Basic</summary> <details class=question> <summary>What are the main open-source LLM families currently available?</summary> <ul> <li>Llama: Decoder-Only</li> <li>Mistral: Decoder-Only (MoE in Mixtral)</li> <li>Gemma: Decoder-Only</li> <li>Phi: Decoder-Only</li> <li>Qwen: Decoder-Only (dense + MoE)</li> <li>DeepSeek: Decoder-Only (MoE in V2)</li> <li>Falcon: Decoder-Only</li> <li>OLMo: Decoder-Only</li> </ul> </details> <details class=question> <summary>What’s the difference between prefix decoder, causal decoder, and encoder-decoder architectures?</summary> <ul> <li><strong>Causal Decoder (Decoder-Only)</strong>: Autoregressive model that generates text left-to-right, attending only to previous tokens.</li> <li><strong>Prefix Decoder (PrefixLM)</strong>: Causal decoder with a bidirectional prefix (input context) followed by autoregressive generation.</li> <li><strong>Encoder-Decoder (Seq2Seq)</strong>: Two separate Transformer stacks(Encoder &amp; Decoder)</li> </ul> <details class=example> <summary>Causal Decoder</summary> <ul> <li>Prompt<blockquote> <p>Translate to French: The cat is on the mat.</p> </blockquote> </li> <li>Generation (autoregressive, causal mask):<blockquote> <p>Le [only sees "Le"]</p> <p>Le chat [sees "Le chat"]</p> <p>Le chat est [sees "Le chat est"]</p> <p>Le chat est sur [sees up to "sur"]</p> <p>Le chat est sur le [sees up to "le"]</p> <p>Le chat est sur le tapis. [final]</p> </blockquote> </li> <li>Summary<blockquote> <p><strong>Cannot see future tokens</strong></p> <p><strong>Cannot see full input bidirectionally — but works via prompt engineering</strong></p> </blockquote> </li> </ul> </details> <details class=example> <summary>Prefix Decoder</summary> <ul> <li>Input Format<blockquote> <p>[Prefix] The cat is on the mat. [SEP] Translate to French: [Generate] Le chat est sur le tapis.</p> </blockquote> </li> <li>Attention<blockquote> <p><strong>Prefix</strong> (The cat is on the mat. [SEP] Translate to French:) → bidirectional</p> </blockquote> </li> </ul> </details> <details class=example> <summary>Encoder-Decoder</summary> </details> </details> <details class=question> <summary>What is the training objective of large language models?</summary> <p>LLMs are trained to predict the next token in a sequence.</p> </details> <details class=question> <summary>Why are most modern LLMs decoder-only architectures?</summary> <p>Most modern LLMs are decoder-only because this architecture is the simplest, fastest, and most flexible for large-scale text generation. Below is the full reasoning, broken into the fundamental, engineering, and use-case levels.</p> <ul> <li>Decoder-only naturally matches the training objective</li> <li>Simpler architecture → easier scaling</li> <li>Better for long-context generation</li> <li>Fits universal multitask learning with a single text stream</li> <li>Aligns with inference needs<ul> <li>streaming output</li> <li>token-by-token generation</li> <li>low latency</li> <li>high throughput</li> <li>continuous prompts</li> </ul> </li> </ul> </details> <details class=question> <summary>Explain the difference between encoder-only, decoder-only, and encoder-decoder models.</summary> <ul> <li><span class=def-mono-blue>Encoder-only Models (BERT, RoBERTa, DeBERTa, ELECTRA)</span><ul> <li>classification (sentiment, fraud detection)</li> <li>named entity recognition</li> <li>sentence similarity</li> <li>search / embeddings</li> <li>anomaly or pattern detection</li> </ul> </li> <li><span class=def-mono-blue>Decoder-only Models (GPT, Llama, Mixtral, Gemma, Qwen)</span><ul> <li>Text generation</li> <li>Multi-task language modeling</li> <li>Anything that treats tasks as text → text in one stream</li> </ul> </li> <li><span class=def-mono-blue>Encoder–Decoder (Seq2Seq) Models (T5, FLAN-T5, BART, mT5, early Transformer models)</span><ul> <li>Translation</li> <li>Summarization</li> <li>Text-to-text tasks with clear input → output mapping</li> </ul> </li> </ul> </details> <details class=question> <summary>What’s the difference between prefix LM and causal LM?</summary> <ul> <li><span class=def-mono-red>Causal LM</span>: every token can only attend to previous tokens.</li> <li><span class=def-mono-red>Prefix LM</span>: the prefix can be fully bidirectional, while the rest is generated causally.</li> </ul> <table> <thead> <tr> <th>Feature</th> <th>Causal LM</th> <th>Prefix LM</th> </tr> </thead> <tbody> <tr> <td>Attention</td> <td>Strictly left-to-right</td> <td>Prefix: full; Generation: causal</td> </tr> <tr> <td>Use case</td> <td>Free-form generation</td> <td>Conditional generation, prefix tuning</td> </tr> <tr> <td>Examples</td> <td>GPT, Llama, Mixtral</td> <td>T5 (prefix mode), UL2, some prompt-tuning models</td> </tr> <tr> <td>Future access?</td> <td>No</td> <td>Only inside prefix</td> </tr> <tr> <td>Mask complexity</td> <td>Simple</td> <td>Mixed masks</td> </tr> </tbody> </table> </details> </details> <details class=tip> <summary>Layer Normalization Variants</summary> <details class=question> <summary>Comparison of LayerNorm vs BatchNorm vs RMSNorm?</summary> <table> <thead> <tr> <th>Norm</th> <th>Formula</th> <th>Pros</th> <th>Cons</th> </tr> </thead> <tbody> <tr> <td>BatchNorm</td> <td>Normalize across batch</td> <td>Great for CNNs</td> <td>Bad for variable batch / autoregressive decoding</td> </tr> <tr> <td>LayerNorm</td> <td>Normalize across hidden dim</td> <td>Stable for Transformers</td> <td>Slightly more compute than RMSNorm</td> </tr> <tr> <td>RMSNorm</td> <td>Normalize only scale</td> <td>Faster, more stable in LLMs</td> <td>No centering → sometimes slightly less expressive</td> </tr> </tbody> </table> </details> <details class=question> <summary>What’s the core idea of DeepNorm?</summary> <p><strong>DeepNorm keeps the Transformer stable at extreme depths by scaling the residual connections proportionally to the square root of the model depth.</strong></p> </details> <details class=question> <summary>What are the advantages of DeepNorm?</summary> <p><strong>DeepNorm = deep models that actually train and perform well, without tricks.</strong></p> <ul> <li>Enables Extremely Deep Transformers (1,000+ layers)</li> <li>Superior Training Stability</li> <li>Improved Optimization Landscape</li> <li>Better Performance on Downstream Tasks</li> <li>No Architectural Overhead</li> <li>Robust Across Scales and Tasks</li> </ul> </details> <details class=question> <summary>What are the differences when applying LayerNorm at different positions in LLMs?</summary> <ul> <li><span class=def-mono-red><del>Pre-NormPost-Norm</del> (Original Transformer, 2017)</span>: Normalizes after adding the residual.<ul> <li>Pros:<ul> <li>Fairly stable for shallow models (&lt;12 layers)</li> <li>Works well in classic NMT models</li> </ul> </li> <li>Cons:<ul> <li>Fails to train deep models (vanishing/exploding gradients)</li> <li>Poor gradient flow</li> <li>Not used in modern LLMs</li> </ul> </li> </ul> </li> <li>Pre-Norm (Current Standard in GPT/LLaMA): Normalize before attention or feed-forward<ul> <li>Pros:<ul> <li>Much more stable for deep Transformers</li> <li>Great training stability up to hundreds of layers</li> <li>Works well with small batch sizes</li> <li>Default in GPT-⅔, LLaMA, Mistral, Gemma, Phi-3, Qwen2</li> </ul> </li> <li>Cons:<ul> <li>Residual stream grows in magnitude unless controlled (→ RMSNorm or DeepNorm often added)</li> <li>Slightly diminished expressive capacity compared to Post-Norm (but negligible in practice)</li> </ul> </li> </ul> </li> <li> <ul> <li>Pros:<ul> <li>Extra stability &amp; smoothness</li> <li>Improved optimization in some NMT models</li> </ul> </li> </ul> <p>Sandwich-Norm: LayerNorm applied before AND after sublayers.</p> <ul> <li>Cons:<ul> <li>Expensive (two norms per sublayer)</li> <li>Rarely used in large decoder-only LLMs</li> </ul> </li> </ul> </li> </ul> <p>🧠 Why LayerNorm position matters</p> <div class="language-text highlight"><pre><span></span><code>1. Training Stability
    •   Pre-Norm prevents exploding residuals
    •   Post-Norm accumulates errors → unstable for deep models
2. Gradient Flow
    - Residuals in Pre-Norm allow gradients to bypass the sublayers directly.
</code></pre></div> </details> <details class=question> <summary>Which normalization method is used in different LLM architectures?</summary> <p><strong>Large decoder-only LLMs almost universally use RMSNorm + Pre-Norm.</strong></p> </details> </details> <details class=tip> <summary>Activation Functions in LLMs</summary> <details class=question> <summary>What’s the formula for the FFN (Feed-Forward Network) block?</summary> <ul> <li> <p><span class=def-mono-red>Standard FFN Formula</span></p> <div class=arithmatex>\[\text{FFN}(x) = W_2 \, \sigma(W_1 x + b_1) + b_2\]</div> <div class=arithmatex>\[W_1 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{mode$$l}}}\]</div> <div class=arithmatex>\[b_1 \in \mathbb{R}^{d_{\text{ff}}}\]</div> <div class=arithmatex>\[W_2 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}\]</div> <div class=arithmatex>\[b_2 \in \mathbb{R}^{d_{\text{model}}}\]</div> <div class=arithmatex>\[\sigma = \text{activation} \text{ } \text{function} \text{(ReLU in original Transformer, GELU in GPT, SwiGLU/GeLU-linear in modern LLMs)}\]</div> </li> <li> <p><span class=def-mono-blue>Gated FFN in LLMs</span></p> <div class=arithmatex>\[\text{FFN}(x) = W_3 \left( \text{Swish}(W_1x) \odot W_2x \right)\]</div> <div class=arithmatex>\[\text{Swish}(u) = u \cdot \sigma(u)\]</div> <div class=arithmatex>\[W_1, W_2 \in \mathbb{R}^{d_{\text{ff}} \times d_{\text{model}}}\]</div> <div class=arithmatex>\[W_3 \in \mathbb{R}^{d_{\text{model}} \times d_{\text{ff}}}\]</div> </li> </ul> </details> <details class=question> <summary>What’s the GeLU formula?</summary> <p><strong>Gaussian Error Linear Unit (GeLU)</strong></p> <div class=arithmatex>\[\text{GeLU}(x) = \frac{x}{2}\left(1 + \operatorname{erf}\left(\frac{x}{\sqrt{2}}\right)\right)\]</div> <div class=arithmatex>\[\operatorname{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} \, dt\]</div> </details> <details class=question> <summary>What’s the Swish formula?</summary> <p><strong>Swish is a smooth, non-monotonic activation.</strong></p> <div class=arithmatex>\[\text{Swish}(x) = \frac{x}{1 + e^{-x}}\]</div> </details> <details class=question> <summary>What’s the formula of an FFN block with GLU (Gated Linear Unit)?</summary> </details> <details class=question> <summary>What’s the formula of a GLU block using GeLU?</summary> </details> <details class=question> <summary>What’s the formula of a GLU block using Swish?</summary> </details> <details class=question> <summary>Which activation functions do popular LLMs use?</summary> </details> <details class=question> <summary>What are the differences between Adam and SGD optimizers?</summary> </details> </details> <details class=tip> <summary>Attention Mechanisms — Advanced Topics</summary> <details class=question> <summary>What are the problems with traditional attention?</summary> </details> <details class=question> <summary>What are the directions of improvement for attention?</summary> </details> <details class=question> <summary>What are the attention variants?</summary> </details> <details class=question> <summary>What issues exist in multi-head attention?</summary> </details> <details class=question> <summary>Explain Multi-Query Attention (MQA).</summary> </details> <details class=question> <summary>Compare Multi-head, Multi-Query, and Grouped-Query Attention.</summary> </details> <details class=question> <summary>What are the benefits of MQA?</summary> </details> <details class=question> <summary>Which models use MQA or GQA?</summary> </details> <details class=question> <summary>Why was FlashAttention introduced? Briefly explain its core idea.</summary> </details> <details class=question> <summary>What are FlashAttention advantages?</summary> </details> <details class=question> <summary>Which models implement FlashAttention?</summary> </details> <details class=question> <summary>What is parallel transformer block?</summary> </details> <details class=question> <summary>What’s the computational complexity of attention and how can it be improved?</summary> </details> <details class=question> <summary>Compare MHA, GQA, and MQA — what are their key differences?</summary> </details> </details> <details class=tip> <summary>Cross-Attention</summary> <details class=question> <summary>Why do we need Cross-Attention?</summary> </details> <details class=question> <summary>Explain Cross-Attention.</summary> </details> <details class=question> <summary>Compare Cross-Attention and Self-Attention — similarities and differences.</summary> </details> <details class=question> <summary>Provide a code implementation of Cross-Attention.</summary> </details> <details class=question> <summary>What are its application scenarios?</summary> </details> <details class=question> <summary>What are the advantages and challenges of Cross-Attention?</summary> </details> </details> <details class=tip> <summary>Transformer Operations</summary> <details class=question> <summary>How to load a BERT model using transformers?</summary> </details> <details class=question> <summary>How to output a specific hidden_state from BERT using transformers?</summary> </details> <details class=question> <summary>How to get the final or intermediate layer vector outputs of BERT?</summary> </details> </details> <details class=tip> <summary>LLM Loss Functions</summary> <details class=question> <summary>What is KL divergence?</summary> </details> <details class=question> <summary>Write the cross-entropy loss and explain its meaning.</summary> </details> <details class=question> <summary>What’s the difference between KL divergence and cross-entropy?</summary> </details> <details class=question> <summary>How to handle large loss differences in multi-task learning?</summary> </details> <details class=question> <summary>Why is cross-entropy preferred over MSE for classification tasks?</summary> </details> <details class=question> <summary>What is information gain?</summary> </details> <details class=question> <summary>How to compute softmax and cross-entropy loss (and binary cross-entropy)?</summary> </details> <details class=question> <summary>What if the exponential term in softmax overflows the float limit?</summary> </details> </details> <details class=tip> <summary>Similarity &amp; Contrastive Learning</summary> <details class=question> <summary>Besides cosine similarity, what other similarity metrics exist?</summary> </details> <details class=question> <summary>What is contrastive learning?</summary> </details> <details class=question> <summary>How important are negative samples in contrastive learning, and how to handle costly negative sampling?</summary> </details> </details> <ul> <li><span class=def-mono-red>Advanced Topics in LLMs</span></li> </ul> <details class=tip> <summary>Advanced LLM</summary> <details class=question> <summary>What is a generative large model?</summary> </details> <details class=question> <summary>How do LLMs make generated text diverse and non-repetitive?</summary> </details> <details class=question> <summary>What is the repetition problem (LLM echo problem)? Why does it happen? How can it be mitigated?</summary> </details> <details class=question> <summary>Can LLaMA handle infinitely long inputs? Explain why?</summary> </details> <details class=question> <summary>When should you use BERT vs. LLaMA / ChatGLM models?</summary> </details> <details class=question> <summary>Do different domains require their own domain-specific LLMs? Why?</summary> </details> <details class=question> <summary>How to enable an LLM to process longer texts?</summary> </details> </details> <ul> <li><span class=def-mono-red>Fine-Tuning Large Models</span></li> </ul> <details class=tip> <summary>General Fine-Tuning</summary> <details class=question> <summary>Why does the loss drop suddenly in the second epoch during SFT?</summary> </details> <details class=question> <summary>How much VRAM is needed for full fine-tuning?</summary> </details> <details class=question> <summary>Why do models seem dumber after SFT?</summary> </details> <details class=question> <summary>How to construct instruction fine-tuning datasets?</summary> </details> <details class=question> <summary>How to improve prompt representativeness?</summary> </details> <details class=question> <summary>How to increase prompt data volume?</summary> </details> <details class=question> <summary>How to select domain data for continued pretraining?</summary> </details> <details class=question> <summary>How to prevent forgetting general abilities after domain tuning?</summary> </details> <details class=question> <summary>How to make the model learn more knowledge during pretraining?</summary> </details> <details class=question> <summary>When performing SFT, should the base model be Chat or Base?</summary> </details> <details class=question> <summary>What’s the input/output format for domain fine-tuning?</summary> </details> <details class=question> <summary>How to build a domain evaluation set?</summary> </details> <details class=question> <summary>Is vocabulary expansion necessary? Why?</summary> </details> <details class=question> <summary>How to train your own LLM?</summary> </details> <details class=question> <summary>What are the benefits of instruction fine-tuning?</summary> </details> <details class=question> <summary>During which stage — pretraining or fine-tuning — is knowledge injected?</summary> </details> </details> <details class=tip> <summary>SFT Tricks</summary> <details class=question> <summary>What’s the typical SFT workflow?</summary> </details> <details class=question> <summary>What are key aspects of training data?</summary> </details> <details class=question> <summary>How to choose between large and small models?</summary> </details> <details class=question> <summary>How to ensure multi-task training balance?</summary> </details> <details class=question> <summary>Can SFT learn knowledge at all?</summary> </details> <p>??? question "How to select datasets effectively?</p> </details> <details class=tip> <summary>Training Experience</summary> <details class=question> <summary>How to choose a distributed training framework?</summary> </details> <details class=question> <summary>What are key LLM training tips?</summary> </details> <details class=question> <summary>How to choose model size?</summary> </details> <details class=question> <summary>How to select GPU accelerators?</summary> </details> </details> <ul> <li><span class=def-mono-red>LangChain and Agent-Based Systems</span></li> </ul> <details class=tip> <summary>LangChain Core</summary> <details class=question> <summary>What is LangChain?</summary> </details> <details class=question> <summary>What are its core concepts?</summary> </details> <details class=question> <summary>Components and Chains</summary> </details> <details class=question> <summary>Prompt Templates and Values</summary> </details> <details class=question> <summary>Example Selectors</summary> </details> <details class=question> <summary>Output Parsers</summary> </details> <details class=question> <summary>Indexes and Retrievers</summary> </details> <details class=question> <summary>Chat Message History</summary> </details> <details class=question> <summary>Agents and Toolkits</summary> </details> </details> <details class=tip> <summary>Long-Term Memory in Multi-Turn Conversations</summary> <details class=question> <summary>How can Agents access conversation context?</summary> </details> <details class=question> <summary>Retrieve full history</summary> </details> <details class=question> <summary>Use sliding window for recent context</summary> </details> <p>??? question "</p> </details> <details class=tip> <summary>Practical RAG Q&amp;A using LangChain</summary> <details class=question> <summary>(Practical implementation questions about RAG apps in LangChain)</summary> </details> </details> <ul> <li><span class=def-mono-red>Retrieval-Augmented Generation (RAG)</span></li> </ul> <details class=tip> <summary>RAG Basics</summary> <details class=question> <summary>Why do LLMs need an external (vector) knowledge base?</summary> </details> <details class=question> <summary>What’s the overall workflow of LLM+VectorDB document chat?</summary> </details> <details class=question> <summary>What are the core technologies?</summary> </details> <details class=question> <summary>How to build an effective prompt template?</summary> </details> </details> <details class=tip> <summary> RAG Concepts</summary> <details class=question> <summary>What are the limitations of base LLMs that RAG solves?</summary> </details> <details class=question> <summary>What is RAG?</summary> </details> <details class=question> <summary>How to obtain accurate semantic representations?</summary> </details> <details class=question> <summary>How to align query/document semantic spaces?</summary> </details> <details class=question> <summary>How to match retrieval model output with LLM preferences?</summary> </details> <details class=question> <summary>How to improve results via post-retrieval processing?</summary> </details> <details class=question> <summary>How to optimize generator adaptation to inputs?</summary> </details> <details class=question> <summary>What are the benefits of using RAG?</summary> </details> </details> <details class=tip> <summary>RAG Layout Analysis</summary> <details class=question> <summary>Why is PDF parsing necessary?</summary> </details> <details class=question> <summary>What are common methods and their differences?</summary> </details> <details class=question> <summary>What problems exist in PDF parsing?</summary> </details> <details class=question> <summary>Why is table recognition important?</summary> </details> <details class=question> <summary>What are the main methods?</summary> </details> <details class=question> <summary>Traditional methods</summary> </details> <details class=question> <summary>pdfplumber extraction techniques</summary> </details> <details class=question> <summary>Why do we need text chunking?</summary> </details> <details class=question> <summary>What are common chunking strategies (regex, Spacy, LangChain, etc.)?</summary> </details> </details> <details class=tip> <summary>RAG Retrieval Strategies</summary> <details class=question> <summary>Why use LLMs to assist recall?</summary> </details> <details class=question> <summary>HYDE approach: idea and issues</summary> </details> <details class=question> <summary>FLARE approach: idea and recall strategies</summary> </details> <details class=question> <summary>Why construct hard negative samples?</summary> </details> <details class=question> <summary>Random sampling vs. Top-K hard negative sampling</summary> </details> </details> <details class=tip> <summary>RAG Evaluation</summary> <details class=question> <summary>Why evaluate RAG?</summary> </details> <details class=question> <summary>What are the evaluation methods, metrics, and frameworks?</summary> </details> </details> <details class=tip> <summary>RAG Optimization</summary> <details class=question> <summary>What are the optimization strategies for retrieval and generation modules?</summary> </details> <details class=question> <summary>How to enhance context using knowledge graphs (KGs)?</summary> </details> <details class=question> <summary>What are the problems with vector-based context augmentation?</summary> </details> <details class=question> <summary>How can KG-based methods improve it?</summary> </details> <details class=question> <summary>What are the main pain points in RAG and their solutions?</summary> </details> <details class=question> <summary>Content missing</summary> </details> <details class=question> <summary>Top-ranked docs missed</summary> </details> <details class=question> <summary>Context loss</summary> </details> <details class=question> <summary>Failure to extract answers</summary> </details> <details class=question> <summary>Explain RAG-Fusion. Why it’s needed,Core technologies,Workflow, and Advantages</summary> </details> <details class=question> <summary></summary> </details> </details> <details class=tip> <summary>Graph RAG</summary> <details class=question> <summary>Why do we need Graph RAG?</summary> </details> <details class=question> <summary>What is Graph RAG and how does it work? Show a code example and use case.</summary> </details> <details class=question> <summary>How to improve ranking optimization in Graph RAG?</summary> </details> </details> <ul> <li><span class=def-mono-red>Parameter-Efficient Fine-Tuning (PEFT)</span></li> </ul> <details class=tip> <summary>PEFT Fundamentals</summary> <details class=question> <summary>What is fine-tuning, and how is it performed?</summary> </details> <details class=question> <summary>Why do we need PEFT?</summary> </details> <details class=question> <summary>What is PEFT and its advantages?</summary> </details> </details> <details class=tip> <summary>Adapter Tuning</summary> <details class=question> <summary>Why use adapter-tuning?</summary> </details> <details class=question> <summary>What’s the core idea behind adapter-tuning?</summary> </details> <details class=question> <summary>How does it differ from full fine-tuning?</summary> </details> </details> </div></div> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../llm-training-epoch/ class="md-footer__link md-footer__link--prev" aria-label="Previous: LLM Training Epoch"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> LLM Training Epoch </div> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> </div> <div class=md-social> <a href=https://github.com/binzhango target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> <a href=mailto:zhangbinengr@hotmail.com target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m20 8-8 5-8-5V6l8 5 8-5m0-2H4c-1.11 0-2 .89-2 2v12a2 2 0 0 0 2 2h16a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2"/></svg> </a> <a href=None target=_blank rel=noopener title class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["header.autohide", "navigation.footer", "navigation.indexes", "navigation.top", "navigation.path", "navigation.tabs", "navigation.tabs.sticky", "navigation.expand", "navigation.tracking", "content.code.copy", "content.tabs.link", "content.code.annotate", "search.highlight", "search.share", "search.suggest", "toc.follow", "toc.integrate"], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>