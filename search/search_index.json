{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"#blog","title":"Blog","text":"<ul> <li> <p> Knowledge</p> <p>  LLM Summary   Updating</p> </li> </ul>"},{"location":"2020/02/airflow/","title":"Airflow","text":"","tags":["Data Engineer"]},{"location":"2020/02/airflow/#code-snippet","title":"Code snippet","text":"<pre><code>import airflow\nfrom airflow.models import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'ABC',\n    'start_date': airflow.utils.dates.days_ago(1),\n    'depends_on_past': False,\n    # failure email\n    'email': ['abc@xxx.com'],\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'pool': 'data_hadoop_pool',\n    'priority_weight': 900,\n    'queue': '66.66.0.66:8080'\n}\n\ndag = DAG(\n    dag_id='daily', \n    default_args=default_args,\n    schedule_interval='0 13 * * *')\n\ndef fetch_data_from_hdfs_function(ds, **kwargs):\n    pass\n\ndef push_data_to_mysql_function(ds, **kwargs):\n    pass\n\nfetch_data_from_hdfs = PythonOperator(\n    task_id='fetch_data_from_hdfs',\n    provide_context=True,\n    python_callable=fetch_data_from_hdfs_function,\n    dag=dag)\n\npush_data_to_mysql = PythonOperator(\n    task_id='push_data_to_mysql',\n    provide_context=True,\n    python_callable=push_data_to_mysql_function,\n    dag=dag)\n\nfetch_data_from_hdfs &gt;&gt; push_data_to_mysql\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/airflow/#update","title":"update","text":"<pre><code>#default parameters\nfetch_data_from_hdfs = PythonOperator(\n    task_id='fetch_data_from_hdfs',\n    provide_context=True,\n    python_callable=fetch_data_from_hdfs_function,\n    dag=dag)\n\n#overwrite parameters\npush_data_to_mysql = PythonOperator(\n    task_id='push_data_to_mysql',\n    queue='77.66.0.66:8080', #update\n    pool='data_mysql_pool', #update\n    provide_context=True,\n    python_callable=push_data_to_mysql_function,\n    dag=dag)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/airflow/#decouple","title":"decouple","text":"<pre><code>import xx.fetch_data_from_hdfs \n\ndef fetch_data_from_hdfs_function(ds, **kwargs):\n    if not fetch_data_from_hdfs: \n        raise AirflowException('run fail: fetch_data_from_hdfs')\n\nfetch_data_from_hdfs = PythonOperator(\n    task_id='fetch_data_from_hdfs',\n    provide_context=True,\n    python_callable=fetch_data_from_hdfs_function,\n    dag=dag)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/","title":"Azure Data Factory (Data Flow)","text":"<p>Recently I'm working in Azure to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work.</p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#task1","title":"Task1","text":"<p>Process CSV files and merge different system files into one file</p> <ul> <li>Source: CSV files with filename format (abcd_yyyymmdd_uuid.csv), where abcd is system id.<ul> <li>a_20180101_9ca2bed1-2ed0-eaeb-8401-784f43755025.csv</li> <li>a_20180101_cca2bed1-aed0-11eb-8401-784f73755025.csv</li> <li>b_20190202_ece2bed1-2ed0-abeb-8401-784f43755025.csv</li> <li>c_20180101_ada2bed1-2ed0-22eb-8401-784f43755025.csv</li> </ul> </li> <li>Sink: yyyymmdd.csv<ul> <li>20180101.csv</li> <li>20190202.csv</li> </ul> </li> </ul>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#adf-pipeline","title":"ADF Pipeline","text":"","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#activities","title":"Activities","text":"","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#get-metadata","title":"Get Metadata","text":"<ul> <li>Input: source directory/parameters</li> <li>Output: metadata of each object</li> </ul> <p>Get Metadata activity iterate source directory to obtain each object. The most important one is Argument </p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#foreach","title":"ForEach","text":"<ul> <li>Input: output of Get Metadata</li> <li>Output: None</li> </ul> <p>ForEach activity is used to process each object in source direcoty.</p> <p></p><pre><code>@activity('Get Metadata1').output.childItems\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#set-variables","title":"Set Variables","text":"<p>It's convenient to predefine a value used in next step.</p> <p></p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#dataflow","title":"Dataflow","text":"<p>The dataflow merge all files with same date, and source1 and sink are the same destination. So, initially source1 is empty and check this options. </p> <p>The only configuration in Sink is the  File name option </p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#aggregation-of-filenames","title":"Aggregation of filenames","text":"<p>The last problem in dataflow is how to merge files with same date in dataflow, which means we firstly find out all these files. The solution to this problems is regex expression.</p> <p></p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#task2","title":"Task2","text":"<p>Generally CSV file has a header and we can process it easily in ADF. However, a special case is a large CSV file has multiple different headers and we need to automatically split it into regular csv files with headers respectively.</p> <ul> <li> <p>Sample data:     &gt; h1,h1_col1,h1_col2,h1_col3     &gt; h2,h2_col1,h2_col2,h2_col3,h2_col4,h2_col5     &gt; h3,h3_col1,h3_col2     &gt; h1,h1_row1_1,h1_row1_2,h1_row1_3     &gt; h1,h1_row2_1,h1_row2_2,h1_row2_3     &gt; h1,h1_row3_1,h1_row3_2,h1_row3_3     &gt; h2,h2_row1_1,h2_row1_2,h2_row1_3,h2_row1_4,h2_row1_5     &gt; h2,h2_row2_1,h2_row2_2,h2_row2_3,h2_row2_4,h2_row2_5     &gt; h2,h2_row3_1,h2_row3_2,h2_row3_3,h2_row3_4,h2_row3_5     &gt; h2,h2_row4_1,h2_row4_2,h2_row4_3,h2_row4_4,h2_row4_5     &gt; h2,h2_row5_1,h2_row5_2,h2_row5_3,h2_row5_4,h2_row5_5     &gt; h3,h3_row1_1,h3_row1_2     &gt; h3,h3_row2_1,h3_row2_2</p> </li> <li> <p>Explanation:</p> <ul> <li>header format: header name, columns names</li> <li>3 headers : h1, h2 and h3</li> <li>the 1<sup>st</sup> column of each row is header name and rest of columns are values</li> </ul> </li> <li> <p>Output:</p> <ul> <li>h1 file     &gt; h1_col1,h1_col2,h1_col3     &gt; h1_row1_1,h1_row1_2,h1_row1_3     &gt; h1_row2_1,h1_row2_2,h1_row2_3     &gt; h1_row3_1,h1_row3_2,h1_row3_3</li> <li>h2 file     &gt; h2_col1,h2_col2,h2_col3,h2_col4,h2_col5     &gt; h2_row1_1,h2_row1_2,h2_row1_3,h2_row1_4,h2_row1_5     &gt; h2_row2_1,h2_row2_2,h2_row2_3,h2_row2_4,h2_row2_5     &gt; h2_row3_1,h2_row3_2,h2_row3_3,h2_row3_4,h2_row3_5     &gt; h2_row4_1,h2_row4_2,h2_row4_3,h2_row4_4,h2_row4_5     &gt; h2_row5_1,h2_row5_2,h2_row5_3,h2_row5_4,h2_row5_5</li> <li>h3 file     &gt; h3_col1,h3_col2     &gt; h3_row1_1,h3_row1_2     &gt; h3_row2_1,h3_row2_2</li> </ul> </li> </ul>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#dataflow_1","title":"Dataflow","text":"<p>The dataset used in source and sink must uncheck this </p> <p></p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#derivedcolumn","title":"DerivedColumn","text":"<p>Because no header is in the dataset, ADF automatically assign a column name to each one. The column name format is _colindex_</p> <p>In this task the header column is _col0_ and we can map this one to another name like filename </p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#sink","title":"Sink","text":"<p>This dataflow will automatically split composite CSV file into different files and save them at container root path. To save them at another directory, you can add folder name to the mapping column name in DerivedColumn activity.</p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#trigger","title":"Trigger","text":"<p>We use blob event trigger to implement automation. Once uploading a new file is done, these pipeline will process it automatically. How to create event trigger</p> <p>Two values in trigger are used by pipeline - @triggerBody().folderPath : /container name/folder/ - @triggerBody().fileName : blob name</p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#pandas-processing","title":"Pandas Processing","text":"<pre><code>import pandas as pd\nimport csv\n\ndf = pd.read_csv('sample.csv', sep='^([^,]+),',engine='python', header=None)\ndf.drop(df.columns[0], axis=1, inplace=True)\n\nheads = df[df.columns[0]].unique()\nd = dict(tuple(df.groupby(df.columns[0])))\n\nfor h in heads:\n    outputfile = d[h]\n    outputfile.drop(outputfile.columns[0], axis=1, inplace=True)\n    outputfile.to_csv('{0}.csv'.format(h), sep=' ', index=False, header=False)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/batch-normalization/","title":"Batch Normalization","text":"<p>Batch Normalization is one of important parts in our NN.</p>"},{"location":"2020/02/batch-normalization/#why-need-normalization","title":"Why need Normalization","text":"<p>This paper title tells me the reason Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - accelerating traning - reduce internal covariate shift</p>"},{"location":"2020/02/batch-normalization/#independent-and-identically-distributed-iid","title":"Independent and identically distributed (IID)","text":"<p>If our data is independent and identically distributed, training model can be simplified and its predictive ability is improved. One important step of data preparation is whitening which is used to</p>"},{"location":"2020/02/batch-normalization/#whitening","title":"Whitening","text":"<ul> <li>reduce features' coralation     =&gt; Independent</li> <li>all features have zero mean and unit variances =&gt; Identically distributed</li> </ul>"},{"location":"2020/02/batch-normalization/#internal-covariate-shift-ics","title":"Internal Covariate Shift (ICS)","text":"<p>What is problem of ICS? Generally data is not IID - Previous layer should update hyper-parameters to adjust new data so that reduce learning speed - Get stuck in the saturation region as the network grows deeper and network stop learning earlier</p>"},{"location":"2020/02/batch-normalization/#covariate-shift","title":"Covariate Shift","text":"<p>What is covariate shift? While in the process \\(X \\rightarrow Y\\) \\(\\(P^{train}(y|x) = P^{test}(y|x)\\)\\) \\(\\(but\\; P^{train}(x) \\neq P^{test}(x)\\)\\)</p>"},{"location":"2020/02/batch-normalization/#todo","title":"ToDo","text":""},{"location":"2020/02/batch-normalization/#normalizations","title":"Normalizations","text":"<ul> <li>weight scale invariance</li> <li>data scale invariance</li> </ul>"},{"location":"2020/02/batch-normalization/#batch-normalization","title":"Batch Normalization","text":""},{"location":"2020/02/batch-normalization/#layer-normalization","title":"Layer Normalization","text":""},{"location":"2020/02/batch-normalization/#weight-normalization","title":"Weight Normalization","text":""},{"location":"2020/02/batch-normalization/#cosine-normalization","title":"Cosine Normalization","text":""},{"location":"2020/02/gradient-descent/","title":"Gradient Descent","text":""},{"location":"2020/02/gradient-descent/#gradient-based-optimization-algorithms","title":"gradient-based optimization algorithms","text":""},{"location":"2020/02/gradient-descent/#gradient-descent-variants","title":"Gradient Descent variants","text":""},{"location":"2020/02/gradient-descent/#batch-gradient-descent-bgd","title":"Batch Gradient Descent (BGD)","text":"<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \u03b8</p> <p>Batch gradient descent is guaranteed to converge  - to the global minimum for convex error surfaces - to a local minimum for non-convex surfaces</p>"},{"location":"2020/02/gradient-descent/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, - enables it to jump to new and potentially better local minima - this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting</p> <p>when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.</p>"},{"location":"2020/02/gradient-descent/#mini-batch-gradient-descent-mb-gd","title":"Mini-batch Gradient Descent (MB-GD)","text":"<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples</p> <ul> <li>reduces the variance of the parameter updates, which can lead to more stable convergence</li> <li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient</li> <li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li> </ul>"},{"location":"2020/02/gradient-descent/#challenges","title":"Challenges","text":"<ul> <li> <p>Choosing a proper learning rate can be difficult.</p> <p>A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p> </li> <li> <p>Learning rete schedules try to adjust the learning rate during training</p> <p>e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics</p> </li> <li> <p>The same learning rate applies to all parameter updates</p> <p>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features</p> </li> <li> <p>Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima</p> <p>The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p> </li> </ul>"},{"location":"2020/02/gradient-descent/#gradient-descent-optimization-algorithms","title":"Gradient Descent Optimization Algorithms","text":"<p>We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton's method.</p>"},{"location":"2020/02/gradient-descent/#momentum","title":"Momentum","text":"<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima.</p> <p>Some implementations exchange the signs in the equations. The momentum term \u03b3 is usually set to 0.9 or a similar value.</p> <p>When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill,  becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \u03b3&lt;1).  The same thing happens to our parameter updates: </p> <p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>"},{"location":"2020/02/gradient-descent/#nesterov-accelerated-gradient","title":"Nesterov Accelerated Gradient","text":"<p>We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again. Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience.  We know that we will use our momentum term \u03b3v\u03b8<sub>t-1</sub> to move the parameters \u03b8.  Computing \u03b8\u2212\u03b3v<sub>t-1</sub> thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update),  a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient  not w.r.t. to our current parameters \u03b8 but w.r.t. the approximate future position of our parameters</p> <p>we are able to adapt our updates to the slope of our error function and speed up SGD in turn,  we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance</p> <p>The distinction between Momentum method and Nesterov Accelerated Gradient updates was - Both methods are distinct only when the learning rate \u03b7 is reasonably large.  - When the learning rate \u03b7 is relatively large, Nesterov Accelerated Gradients allows larger decay rate \u03b1 than Momentum method, while preventing oscillations.  - Both Momentum method and Nesterov Accelerated Gradient become equivalent when \u03b7 is small</p>"},{"location":"2020/02/gradient-descent/#adagrad","title":"Adagrad","text":"<p>Adagrad is an algorithm for gradient-based optimization that does just this:  It adapts the learning rate to the parameters,  - performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features,  - and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</p> <p>For this reason, it is well-suited for dealing with sparse data.</p> <p>Previously, we performed an update for all parameters \u03b8 at once as every parameter \u03b8<sub>i</sub> used the same learning rate \u03b7.  As Adagrad uses a different learning rate for every parameter \u03b8<sub>i</sub> at every time step t, we first show Adagrad's per-parameter update, which we then vectorize. For brevity, we use gt to denote the gradient at time step t. g<sub>t,i</sub> is then the partial derivative of the objective function w.r.t. to the parameter \u03b8<sub>i</sub> at time step t</p> <p>In its update rule, Adagrad modifies the general learning rate \u03b7 at each time step t for every parameter \u03b8<sub>i</sub> based on the past gradients that have been computed for \u03b8<sub>i</sub></p> <p>\u03b8<sub>t+1,i</sub>=\u03b8<sub>t,i</sub>\u2212\u03b7/\u221a(G<sub>t,ii</sub>+\u03f5)\u22c5g<sub>t,i</sub></p> <p>G<sub>t</sub>\u2208R<sup>d\u00d7d</sup> here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. \u03b8<sub>i</sub> up to time step t, while \u03f5 is a smoothing term that avoids division by zero.  Interestingly, without the square root operation, the algorithm performs much worse.</p> <ul> <li>One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate</li> <li>Adagrad's main weakness is its accumulation of the squared gradients in the denominator <p>Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p> </li> </ul>"},{"location":"2020/02/gradient-descent/#adadelta","title":"Adadelta","text":"<p>Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p> <p>Instead of inefficiently storing w previous squared gradients,  the sum of gradients is recursively defined as a decaying average of all past squared gradients. </p>"},{"location":"2024/09/how-to-execute-python-modules/","title":"How to execute python modules","text":""},{"location":"2024/09/how-to-execute-python-modules/#runpy-module","title":"runpy module","text":"<p>We can use internal <code>runpy</code> to execute different moduls in our project.</p> <p>This is used in my pyspark project.</p> submit.py<pre><code>import runpy\nimport sys\n\nif __name__ == '__main__':\n    module_name = sys.argv[1]\n    function_name = sys.argv[2]\n    sys.argv = sys.argv[2:] # this is important for next python entry point\n    runpy.run_module(module_name, run_name=function_name)\n</code></pre> <p>Now, the spark job can be invoked by</p> <pre><code>spark-submit submit.py \"&lt;module_name&gt;\" \"&lt;function_name&gt;\"\n</code></pre> <p>Also, we can wrapper this shell command into a script.</p> run.sh<pre><code>module_name=$1\nfunction_name=$2\nspark-submit submit.py \"$module_name\" \"$function_name\" \"${@:3}\"\n</code></pre>"},{"location":"2021/07/setup-minikube/","title":"Setup Minikube","text":"","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#setup-minikube","title":"Setup Minikube","text":"","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#installation","title":"Installation","text":"<pre><code>brew upgrade \nbrew install minikube\nbrew install kubectl\n# minikube kubectl -- get pods -A\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#startstop-cluster","title":"Start/Stop Cluster","text":"<pre><code>#minikube start\nminikube start --driver=hyperkit\nminikube stop\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#interact-with-cluster","title":"Interact with Cluster","text":"<pre><code>minikube dashboard --alsologtostderr\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#deploy-application","title":"Deploy application","text":"<pre><code>kubectl create deployment balanced --image=k8s.gcr.io/echoserver:1.4  \nkubectl expose deployment balanced --type=LoadBalancer --port=8080\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#spark-on-k8s","title":"Spark on K8s","text":"<p>Official Reference</p>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#build-spark-image","title":"Build Spark image","text":"<ul> <li>brew install spark</li> <li>build image via docker-image-tool.sh <pre><code>sh /usr/local/Cellar/apache-spark/3.1.2/bin/docker-image-tool.sh -m -t spark-docker build\n</code></pre></li> <li>check image <pre><code>minikube ssh\ndocker image ls\n</code></pre> </li> </ul>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#submit-a-sample-job","title":"Submit a sample job","text":"<ul> <li>Find master node of K8s <pre><code>kubectl cluster-info\n</code></pre></li> <li>submit job <pre><code># spark-submit --master k8s://https://192.168.64.2:8443 --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=3 --conf spark.kubernetes.container.image=gcr.io/spark-operator/spark:v2.4.5 --conf spark.kubernetes.namespace=default local:///usr/local/opt/apache-spark/libexec/examples/jars/spark-examples_2.12-2.4.5.jar\n\nbin/spark-submit \\\n--master k8s://https://192.168.99.100:8443 \\\n--deploy-mode cluster \\\n--name spark-pi \\\n--class org.apache.spark.examples.SparkPi \\\n--conf spark.driver.cores=1 \\\n--conf spark.driver.memory=512m \\\n--conf spark.executor.instances=2 \\\n--conf spark.executor.memory=512m \\\n--conf spark.executor.cores=1 \\\n--conf spark.kubernetes.container.image=gcr.io/spark-operator/spark:v2.4.5 \\\n--conf spark.kubernetes.container.image.pullPolicy=IfNotPresent \\\n--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\nlocal:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar\n</code></pre></li> </ul>","tags":["Data Engineer"]},{"location":"2024/08/model-registry/","title":"Model Registry","text":"<p>Problem: How to introduce ml-based production/features to cross-functional teams.</p> <p>Question: - Where can we find the best version of this model?   - audit   - test   - deploy   - reuse - How was this model trained? - How can we track this docs for each models? - How can we review models? - How can we integrate with tools and services?</p>"},{"location":"2024/08/model-registry/#model-registry","title":"Model Registry","text":"<p>An model registry serves as a centralized repository, enabling effective model management and documentation.</p>"},{"location":"2024/10/snowflake-data-science-training-summary/","title":"Snowflake Data Science Training Summary","text":"","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#snowflake-data-science","title":"Snowflake Data Science","text":"<p>I have enrolled in a private Snowflake Data Science Training. Let me list what I learned from it.</p> <ul> <li>SQL worksheets</li> <li>Snowpark in notebook</li> </ul> <pre><code>mkdocs build\nmkdocs serve\nmkdocs gh-deploy --force\n</code></pre>","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#sql-worksheets","title":"SQL Worksheets","text":"<p>ML functions:</p> <ul> <li>forecast</li> <li>anomaly_detection</li> <li>classification</li> <li>top_insights</li> </ul>","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#add-object-name-into-session","title":"Add object name into session","text":"<pre><code>show parameters like 'SEARCH_PATH';\n\nset cur_search_path = (select \"value\" from table(result_scan(-1)));\nset new_search_path = (select $cur_search_path || ', snowflake.ml'); -- append `snowflake.ml` into search_path\n\nalter session set search_path = $new_search_path;\n\n-- now below two statements are interchangeable \nshow snowflake.ml.forecast;\nshow forecast;\n</code></pre>","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#snowpark-notebook","title":"Snowpark Notebook","text":"","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#snowpark-configuration-snowflak-spark-configuration","title":"Snowpark Configuration &amp; Snowflak-Spark Configuration","text":"<p> Attributes are different</p> <ul> <li>Snowpark config   <pre><code>{\n  \"account\":\"\",\n  \"user\":\"\",\n  \"authenticator\":\"externalbrowser\",\n  \"role\":\"\",\n  \"warehouse\":\"\",\n  \"database\":\"\",\n  \"schema\":\"\"\n}\n</code></pre></li> <li>Snowflake Spark config   <pre><code>{\n  \"sfURL\":\"\",\n  \"sfRole\":\"\",\n  \"sfWarehouse\":\"\",\n  \"sfDatabase\":\"\",\n  \"sfSchema\":\"\",\n  \"sfUser\":\"\",\n  \"sfPassword\":\"\",\n  \"authenticator\":\"externalbrowser\",\n}\n</code></pre></li> </ul>","tags":["Data Science"]},{"location":"2020/03/spark-dataframe-window-function/","title":"Spark Dataframe window function","text":"<p>scala ref</p>","tags":["Data Engineer"]},{"location":"2020/03/spark-dataframe-window-function/#create-dataframe","title":"create dataframe","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/","title":"Spark Optimization","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#spark-run-faster-and-faster","title":"Spark run faster and faster","text":"<ul> <li>Cluster Optimization</li> <li>Parameters Optimization</li> <li>Code Optimization</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#cluster-optimization","title":"Cluster Optimization","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#locality-level","title":"Locality Level","text":"<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data\u2019s current location. In order from closest to farthest:</p> <ul> <li>PROCESS_LOCAL data is in the same JVM as the running code. This is the best locality possible</li> <li>NODE_LOCAL data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li> <li>NO_PREF data is accessed equally quickly from anywhere and has no locality preference</li> <li>RACK_LOCAL data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li> <li>ANY data is elsewhere on the network and not in the same rack</li> </ul> <p>Performance: PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#locality-settting","title":"Locality settting","text":"<ul> <li>spark.locality.wait.process</li> <li>spark.locality.wait.node</li> <li>spark.locality.wait.rack</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#data-format","title":"Data Format","text":"<ul> <li>text</li> <li>orc</li> <li>parquet</li> <li>avro</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#format-setting","title":"format setting","text":"<ul> <li>spark.sql.hive.convertCTAS</li> <li>spark.sql.sources.default</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#parallelising","title":"parallelising","text":"<ul> <li>spark.sql.shuffle.partitions : default is 200</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#computing","title":"computing","text":"<ul> <li>--executor-memory : default is 1G</li> <li>--executor-cores : default is 1 if large memory cause resource throtle in cluster, if small memory cause task termination if more cores cause IO issue, if less cores slow dow computing</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#memory","title":"memory","text":"<ul> <li>spark.executor.overhead.memory</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#table-join","title":"table join","text":"<ul> <li>spark.sql.autoBroadcastJoinThreshold : default 10M</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#predicate-push-down-in-spark-sql-queries","title":"predicate push down in Spark SQL queries","text":"<ul> <li>spark.sql.parquet.filterPushdown : default True</li> <li>spark.sql.orc.filterPushdown=true : default False</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#reuse-rdd","title":"reuse RDD","text":"<pre><code>    df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#spark-operators","title":"Spark operators","text":"<ul> <li>shuffle operators</li> <li>avoid using  reduceByKey, join, distinct, repartition etc</li> <li> <p>Broadcast small dataset</p> </li> <li> <p>High performance operator</p> </li> <li>reduceByKey &gt; groupByKey (reduceByKey works at map side)</li> <li>mapPartitions &gt; map (reduce function calls)</li> <li>treeReduce &gt; reduce (treeReduce works at executor not driver)<ul> <li>treeReduce &amp; reduce return some result to driver</li> <li>treeReduce does more work on the executors while reduce bring everything back to the driver.</li> </ul> </li> <li>foreachPartitions &gt; foreach (reduce function calls)</li> <li>filter -&gt; coalesce (reduce number of partitions and reduce tasks)</li> <li>repartitionAndSortWithinPartitions &gt; repartition &amp; sort</li> <li>broadcast (100M)</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#shuffle","title":"shuffle","text":"<ul> <li>spark.shuffle.sort.bypassMergeThreshold</li> <li>spark.shuffle.io.retryWait</li> <li>spark.shuffle.io.maxRetries</li> </ul> <p>TBC</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/","title":"Spark Structured Streaming","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#spark-structured-streaming","title":"Spark Structured Streaming","text":"<p>Recently reading a blog Structured Streaming in PySpark It's implemented in Databricks platform. Then I try to implement in my local Spark. Some tricky issue happened during my work.</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#reading-data","title":"Reading Data","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField\n\nspark = SparkSession.builder.appName(\"Test Streaming\").enableHiveSupport().getOrCreate()\n\njson_schema = StructType([\n    StructField(\"time\", TimestampType(), True),\n    StructField(\"customer\", StringType(), True),\n    StructField(\"action\", StringType(), True),\n    StructField(\"device\", StringType(), True)\n])\n\nfile_path = \"local_file_path&lt;file:///...\"\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#read-json-as-same-as-method-in-the-blog","title":"read json as same as method in the blog","text":"<pre><code>input = spark.read.schema(json_schema).json(file_path)\n\ninput.show()\n# +----+--------+------+------+\n# |time|customer|action|device|\n# +----+--------+------+------+\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# +----+--------+------+------+\ninput.count()\n# 20000\n</code></pre> All values are null, however, the count is right. It means spark has already read all data but the schema is not correctly mapped.","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#read-a-single-json-file-to-check-schema","title":"read a single json file to check schema","text":"<pre><code>input = spark.read.schema(json_schema).json(file_path+'/1.json')\n\ninput.show()\n\n# +----+--------+------+------+\n# |time|customer|action|device|\n# +----+--------+------+------+\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# +----+--------+------+------+\n\n# same error\n# Then I drop schema option and use inferSchema\ninput = spark.read.json(file_path+'/1.json')\n\ninput.show()\n\n# +--------------------+-----------+-----------------+--------------------+---------------+\n# |     _corrupt_record|     action|         customer|              device|           time|\n# +--------------------+-----------+-----------------+--------------------+---------------+\n# |[{\"time\":\"3:57:09...|       null|             null|                null|           null|\n# |                null|  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|\n# |                null|   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|\n# |                null|  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|\n# |                null|  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|\n# |                null|  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|\n# |                null|low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|\n# |                null|   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|\n# |                null|  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|\n# |                null|  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|\n# |                null|   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|\n# |                null|low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|\n# |                null|low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|\n# |                null|  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|\n# |                null|   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|\n# |                null|  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|\n# |                null|   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|\n# |                null|low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|\n# |                null|  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|\n# |                null|low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|\n# +--------------------+-----------+-----------------+--------------------+---------------+\n</code></pre> A weird column is _corrupt_record and first value is [{\"time\":\"3:57:09... in this column. Go back to check source file and notice that it's a list of object in json file.","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#remove-and-in-source-file","title":"Remove   [  and ]  in source file","text":"<pre><code>input = spark.read.json(file_path+'/1.json')\n\ninput.show()\n\n# +-----------+-----------------+--------------------+---------------+\n# |     action|         customer|              device|           time|\n# +-----------+-----------------+--------------------+---------------+\n# |  power off|      Alexi Barts|  GreenIQ Controller| 3:57:09.000 PM|\n# |  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|\n# |   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|\n# |  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|\n# |  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|\n# |  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|\n# |low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|\n# |   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|\n# |  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|\n# |  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|\n# |   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|\n# |low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|\n# |low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|\n# |  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|\n# |   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|\n# |  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|\n# |   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|\n# |low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|\n# |  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|\n# |low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|\n# +-----------+-----------------+--------------------+---------------+\n</code></pre> Woo, the dataframe is correct. Let's check schema <pre><code>input.printSchema()\n# root\n#  |-- action: string (nullable = true)\n#  |-- customer: string (nullable = true)\n#  |-- device: string (nullable = true)\n#  |-- time: string (nullable = true)\n</code></pre> So far I manually modify source file and drop external schema to obtain a corret dataframe. Is there anyway to read these files without these steps.","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#add-one-feature-multiline","title":"add one feature multiLine","text":"<p>Read the file without schema but add one feature multiLine</p> <pre><code>input = spark.read.json(\"file:///path/pyspark_test_data\", multiLine=True)\n\n# OR input = spark.read.option('multiLine', True).json(\"file:///path/pyspark_test_data\")\n\n# +-----------+--------------------+--------------------+---------------+\n# |     action|            customer|              device|           time|\n# +-----------+--------------------+--------------------+---------------+\n# |   power on|     Raynor Blaskett|Nest T3021US Ther...| 3:35:09.000 AM|\n# |   power on|Stafford Blakebrough|  GreenIQ Controller|10:59:46.000 AM|\n# |   power on|      Alex Woolcocks|Nest T3021US Ther...| 6:26:36.000 PM|\n# |   power on|      Clarice Nayshe|Footbot Air Quali...| 4:46:28.000 AM|\n# |  power off|      Killie Pirozzi|Footbot Air Quali...| 8:58:43.000 AM|\n# |   power on|    Lynne Dymidowicz|Footbot Air Quali...| 4:20:49.000 PM|\n# |   power on|       Shaina Dowyer|             ecobee4| 3:41:33.000 AM|\n# |low battery|       Barbee Melato| August Doorbell Cam|10:40:24.000 PM|\n# |  power off|        Clem Westcot|Nest T3021US Ther...|11:13:38.000 PM|\n# |  power off|       Kerri Galfour|         Amazon Echo|10:12:15.000 PM|\n# |low battery|        Trev Ashmore|  GreenIQ Controller|11:04:41.000 AM|\n# |   power on|      Coral Jahnisch| August Doorbell Cam| 3:06:31.000 AM|\n# |   power on|      Feliza Cowdrey|Nest T3021US Ther...| 2:49:02.000 AM|\n# |  power off|   Amabelle De Haven|Footbot Air Quali...|12:11:59.000 PM|\n# |  power off|     Benton Redbourn|Nest T3021US Ther...| 3:57:39.000 AM|\n# |low battery|        Asher Potten| August Doorbell Cam| 1:34:44.000 AM|\n# |low battery|    Lorianne Hullyer| August Doorbell Cam| 7:26:42.000 PM|\n# |  power off|     Ruperto Aldcorn|Footbot Air Quali...| 3:54:49.000 AM|\n# |   power on|   Agatha Di Giacomo|Footbot Air Quali...| 7:15:20.000 AM|\n# |   power on|    Eunice Penwright|             ecobee4|11:14:14.000 PM|\n# +-----------+--------------------+--------------------+---------------+\n\ninput.printSchema()\n\n# root\n#  |-- action: string (nullable = true)\n#  |-- customer: string (nullable = true)\n#  |-- device: string (nullable = true)\n#  |-- time: string (nullable = true)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#change-the-schema","title":"change the schema","text":"<p>Set time as StringType </p><pre><code>json_schema = StructType([\n    StructField(\"time\", StringType(), True),\n    StructField(\"customer\", StringType(), True),\n    StructField(\"action\", StringType(), True),\n    StructField(\"device\", StringType(), True)\n])\n\n\ninput = spark.read.schema(json_schema).json(\"file:///path/pyspark_test_data\", multiLine=True)\n\ninput.show()\n\n# +---------------+--------------------+-----------+--------------------+\n# |           time|            customer|     action|              device|\n# +---------------+--------------------+-----------+--------------------+\n# | 3:35:09.000 AM|     Raynor Blaskett|   power on|Nest T3021US Ther...|\n# |10:59:46.000 AM|Stafford Blakebrough|   power on|  GreenIQ Controller|\n# | 6:26:36.000 PM|      Alex Woolcocks|   power on|Nest T3021US Ther...|\n# | 4:46:28.000 AM|      Clarice Nayshe|   power on|Footbot Air Quali...|\n# | 8:58:43.000 AM|      Killie Pirozzi|  power off|Footbot Air Quali...|\n# | 4:20:49.000 PM|    Lynne Dymidowicz|   power on|Footbot Air Quali...|\n# | 3:41:33.000 AM|       Shaina Dowyer|   power on|             ecobee4|\n# |10:40:24.000 PM|       Barbee Melato|low battery| August Doorbell Cam|\n# |11:13:38.000 PM|        Clem Westcot|  power off|Nest T3021US Ther...|\n# |10:12:15.000 PM|       Kerri Galfour|  power off|         Amazon Echo|\n# |11:04:41.000 AM|        Trev Ashmore|low battery|  GreenIQ Controller|\n# | 3:06:31.000 AM|      Coral Jahnisch|   power on| August Doorbell Cam|\n# | 2:49:02.000 AM|      Feliza Cowdrey|   power on|Nest T3021US Ther...|\n# |12:11:59.000 PM|   Amabelle De Haven|  power off|Footbot Air Quali...|\n# | 3:57:39.000 AM|     Benton Redbourn|  power off|Nest T3021US Ther...|\n# | 1:34:44.000 AM|        Asher Potten|low battery| August Doorbell Cam|\n# | 7:26:42.000 PM|    Lorianne Hullyer|low battery| August Doorbell Cam|\n# | 3:54:49.000 AM|     Ruperto Aldcorn|  power off|Footbot Air Quali...|\n# | 7:15:20.000 AM|   Agatha Di Giacomo|   power on|Footbot Air Quali...|\n# |11:14:14.000 PM|    Eunice Penwright|   power on|             ecobee4|\n# +---------------+--------------------+-----------+--------------------+\n</code></pre> Pyspark can load json files successfully without TimestampType. However, how to handle timestamp issue in this job?","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#timestamptype","title":"TimestampType","text":"<p>In offical document, the class pyspark.sql.DataFrameReader has one parameter - timestampFormat </p> <p>sets the string that indicates a timestamp format. </p> <p>Custom date formats follow the formats at java.text.SimpleDateFormat. </p> <p>This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HHss.SSSXXX.</p> <pre><code>input = spark.read.schema(schema).option(\"multiLine\", True).json(\"file:///path/pyspark_test_data\", timestampFormat=\"h:mm:ss.SSS aa\")\n\ninput.show()\n# +-------------------+--------------------+-----------+--------------------+\n# |               time|            customer|     action|              device|\n# +-------------------+--------------------+-----------+--------------------+\n# |1970-01-01 03:35:09|     Raynor Blaskett|   power on|Nest T3021US Ther...|\n# |1970-01-01 10:59:46|Stafford Blakebrough|   power on|  GreenIQ Controller|\n# |1970-01-01 18:26:36|      Alex Woolcocks|   power on|Nest T3021US Ther...|\n# |1970-01-01 04:46:28|      Clarice Nayshe|   power on|Footbot Air Quali...|\n# |1970-01-01 08:58:43|      Killie Pirozzi|  power off|Footbot Air Quali...|\n# |1970-01-01 16:20:49|    Lynne Dymidowicz|   power on|Footbot Air Quali...|\n# |1970-01-01 03:41:33|       Shaina Dowyer|   power on|             ecobee4|\n# |1970-01-01 22:40:24|       Barbee Melato|low battery| August Doorbell Cam|\n# |1970-01-01 23:13:38|        Clem Westcot|  power off|Nest T3021US Ther...|\n# |1970-01-01 22:12:15|       Kerri Galfour|  power off|         Amazon Echo|\n# |1970-01-01 11:04:41|        Trev Ashmore|low battery|  GreenIQ Controller|\n# |1970-01-01 03:06:31|      Coral Jahnisch|   power on| August Doorbell Cam|\n# |1970-01-01 02:49:02|      Feliza Cowdrey|   power on|Nest T3021US Ther...|\n# |1970-01-01 12:11:59|   Amabelle De Haven|  power off|Footbot Air Quali...|\n# |1970-01-01 03:57:39|     Benton Redbourn|  power off|Nest T3021US Ther...|\n# |1970-01-01 01:34:44|        Asher Potten|low battery| August Doorbell Cam|\n# |1970-01-01 19:26:42|    Lorianne Hullyer|low battery| August Doorbell Cam|\n# |1970-01-01 03:54:49|     Ruperto Aldcorn|  power off|Footbot Air Quali...|\n# |1970-01-01 07:15:20|   Agatha Di Giacomo|   power on|Footbot Air Quali...|\n# |1970-01-01 23:14:14|    Eunice Penwright|   power on|             ecobee4|\n# +-------------------+--------------------+-----------+--------------------+\n</code></pre> <p>All yyyy-MM-dd are 1970-01-01 because source file only hh-mm-ss.  These source files are in wrong format in Windows.</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#streaming-our-data","title":"Streaming Our Data","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField\n\n\nspark = SparkSession.builder.appName(\"Test Streaming\").enableHiveSupport().getOrCreate()\n\njson_schema = StructType([\n    StructField(\"time\", StringType(), True),\n    StructField(\"customer\", StringType(), True),\n    StructField(\"action\", StringType(), True),\n    StructField(\"device\", StringType(), True)\n])\n\nstreamingDF = spark.readStream.schema(json_schema) \\\n              .option(\"maxFilesPerTrigger\", 1) \\\n              .option(\"multiLine\", True) \\\n              .json(\"file:///path/pyspark_test_data\")\n\nstreamingActionCountsDF = streamingDF.groupBy('action').count()\n# streamingActionCountsDF.isStreaming\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n\n\n# View stream in real-time\n# query = streamingActionCountsDF.writeStream \\\n#         .format(\"memory\").queryName(\"counts\").outputMode(\"complete\").start()\n\n# format choice:\n# parquet\n# kafka\n# console\n# memory\n\n# query = streamingActionCountsDF.writeStream \\\n#         .format(\"console\").queryName(\"counts\").outputMode(\"complete\").start()\n\nquery = streamingActionCountsDF.writeStream.format(\"console\") \\\n        .queryName(\"counts\").outputMode(\"complete\").start().awaitTermination(timeout=10)\n# Output Mode choice:\n# append\n# complete\n# update\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/whitening-transformation/","title":"Whitening Transformation","text":""},{"location":"2012/10/repo-list/","title":"Repo List","text":""},{"location":"2012/10/repo-list/#repos","title":"Repos","text":"Repo List language link"},{"location":"2020/02/spark-sql/","title":"Spark SQL","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-sql/#spark-submit-options","title":"Spark Submit options","text":"<pre><code>--master MASTER_URL --&gt; \u8fd0\u884c\u6a21\u5f0f\n    \u4f8b\uff1aspark://host:port, mesos://host:port, yarn, or local.\n\n--deploy-mode DEPLOY_MODE \n    Whether to launch the driver program locally (\"client\") or\n    on one of the worker machines inside the cluster (\"cluster\")\n    (Default: client).\n\n--class CLASS_NAME  --&gt; \u8fd0\u884c\u7a0b\u5e8f\u7684main_class \n    \u4f8b\uff1a com.jhon.hy.main.Test\n\n--name NAME --&gt;application \u7684\u540d\u5b57\n    \u4f8b\uff1amy_application\n\n--jars JARS  --&gt;\u9017\u53f7\u5206\u9694\u7684\u672c\u5730jar\u5305\uff0c\u5305\u542b\u5728driver\u548cexecutor\u7684classpath\u4e0b\n    \u4f8b\uff1a/home/app/python_app/python_v2/jars/datanucleus-api-jdo-3.2.6.jar,/home/app/python_app/python_v2/jars/datanucleus-core-3.2.10.jar,/home/app/python_app/python_v2/jars/datanucleus-rdbms-3.2.9.jar,/home/app/python_app/python_v2/jars/mysql-connector-java-5.1.37-bin.jar,/home/app/python_app/python_v2/jars/hive-bonc-plugin-2.0.0.jar\\\n\n--exclude-packages --&gt;\u7528\u9017\u53f7\u5206\u9694\u7684\u201dgroupId:artifactId\u201d\u5217\u8868\n\n--repositories  --&gt;\u9017\u53f7\u5206\u9694\u7684\u8fdc\u7a0b\u4ed3\u5e93\n\n--py-files PY_FILES  --&gt;\u9017\u53f7\u5206\u9694\u7684\u201d.zip\u201d,\u201d.egg\u201d\u6216\u8005\u201c.py\u201d\u6587\u4ef6\uff0c\u8fd9\u4e9b\u6587\u4ef6\u653e\u5728python app\u7684\n\n--files FILES    --&gt;\u9017\u53f7\u5206\u9694\u7684\u6587\u4ef6\uff0c\u8fd9\u4e9b\u6587\u4ef6\u653e\u5728\u6bcf\u4e2aexecutor\u7684\u5de5\u4f5c\u76ee\u5f55\u4e0b\u9762\uff0c\u6d89\u53ca\u5230\u7684k-vge\u683c\u5f0f\u7684\u53c2\u6570\uff0c\u7528 \u2018#\u2019 \u8fde\u63a5,\u5982\u679c\u6709\u81ea\u5b9a\u4e49\u7684log4j \u914d\u7f6e\uff0c\u4e5f\u653e\u5728\u6b64\u914d\u7f6e\u4e0b\u9762\n    \u4f8b\uff1a/home/app/python_app/python_v2/jars/kafka_producer.jar,/home/app/python_app/python_v2/resources/....cn.keytab#.....keytab,/home/app/python_app/python_v2/resources/app.conf#app.conf,/home/app/python_app/python_v2/resources/hive-site.xml,/home/app/python_app/python_v2/resources/kafka_client_jaas.conf#kafka_client_jaas.conf\n\n--properties-file FILE   --&gt; \u9ed8\u8ba4\u7684spark\u914d\u7f6e\u9879\uff0c\u9ed8\u8ba4\u8def\u5f84 conf/spark-defaults.conf\n\n--conf PROP=VALUE  --&gt;  \u4efb\u610f\u7684spark\u914d\u7f6e\u9879\n    \u4f8b\uff1a --conf \"spark.driver.maxResultSize=4g\" \\\n        --conf spark.sql.shuffle.partitions=2600 \\\n        --conf spark.default.parallelism=300 \\\n\n--driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n--driver-java-options       Extra Java options to pass to the driver.\n--driver-library-path       Extra library path entries to pass to the driver.\n--driver-class-path         Extra class path entries to pass to the driver. Note that\n                            jars added with --jars are automatically included in the\n                            classpath.\n\n--executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n\n--proxy-user NAME           User to impersonate when submitting the application.\n                            This argument does not work with --principal / --keytab.\n\n--help, -h                  Show this help message and exit.\n--verbose, -v               Print additional debug output.\n--version,                  Print the version of current Spark.\n\n Spark standalone with cluster deploy mode only:\n  --driver-cores NUM          Cores for driver (Default: 1).\n\n Spark standalone or Mesos with cluster deploy mode only:\n  --supervise                 If given, restarts the driver on failure.\n  --kill SUBMISSION_ID        If given, kills the driver specified.\n  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n\n Spark standalone and Mesos only:\n  --total-executor-cores NUM  Total cores for all executors.\n\n Spark standalone and YARN only:\n  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n                              or all available cores on the worker in standalone mode)\n\n YARN-only:\n  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                              (Default: 1).\n  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n  --num-executors NUM         Number of executors to launch (Default: 2).\n                              If dynamic allocation is enabled, the initial number of\n                              executors will be at least NUM.\n  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n                              working directory of each executor.\n  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n                              secure HDFS.\n  --keytab KEYTAB             The full path to the file that contains the keytab for the\n                              principal specified above. This keytab will be copied to\n                              the node running the Application Master via the Secure\n                              Distributed Cache, for renewing the login tickets and the\n                              delegation tokens periodically.\n\n\n\n--num-executors 30 \\    \u542f\u52a8\u7684executor\u6570\u91cf\u3002\u9ed8\u8ba4\u4e3a2\u3002\u5728yarn\u4e0b\u4f7f\u7528\n--executor-cores 4 \\    \u6bcf\u4e2aexecutor\u7684\u6838\u6570\u3002\u5728yarn\u6216\u8005standalone\u4e0b\u4f7f\u7528\n--driver-memory 8g \\    Driver\u5185\u5b58\uff0c\u9ed8\u8ba41G\n--executor-memory 16g   \u6bcf\u4e2aexecutor\u7684\u5185\u5b58\uff0c\u9ed8\u8ba4\u662f1G\n\n`\u901a\u5e38\u6211\u4eec\u8bb2\u7528\u4e86\u591a\u5c11\u8d44\u6e90\u662f\u6307: num-executor * executor-cores \u6838\u5fc3\u6570\uff0c--num-executors*--executor-memory \u5185\u5b58`\n\n`\u56e0\u73b0\u5728\u6240\u5728\u516c\u53f8\u7528\u7684\u662fspark on yarn \u6a21\u5f0f\uff0c\u4ee5\u4e0b\u6d89\u53ca\u8c03\u4f18\u4e3b\u8981\u9488\u5bf9\u76ee\u524d\u6240\u7528`\n\n--num-executors \u8fd9\u4e2a\u53c2\u6570\u51b3\u5b9a\u4e86\u4f60\u7684\u7a0b\u5e8f\u4f1a\u542f\u52a8\u591a\u5c11\u4e2aExecutor\u8fdb\u7a0b\u6765\u6267\u884c\uff0cYARN\u96c6\u7fa4\u7ba1\u7406\n                \u5668\u4f1a\u5c3d\u53ef\u80fd\u6309\u7167\u4f60\u7684\u8bbe\u7f6e\u6765\u5728\u96c6\u7fa4\u7684\u5404\u4e2a\u5de5\u4f5c\u8282\u70b9\u4e0a\uff0c\u542f\u52a8\u76f8\u5e94\u6570\u91cf\u7684Executor\n                \u8fdb\u7a0b\u3002\u5982\u679c\u5fd8\u8bb0\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u542f\u52a8\u4e24\u4e2a\uff0c\u8fd9\u6837\u4f60\u540e\u9762\u7533\u8bf7\u7684\u8d44\u6e90\u518d\u591a\uff0c\u4f60\u7684Spark\u7a0b\n                \u5e8f\u6267\u884c\u901f\u5ea6\u4e5f\u662f\u5f88\u6162\u7684\u3002\n    \u8c03\u4f18\u5efa\u8bae\uff1a \u8fd9\u4e2a\u8981\u6839\u636e\u4f60\u7a0b\u5e8f\u8fd0\u884c\u60c5\u51b5\uff0c\u4ee5\u53ca\u591a\u6b21\u6267\u884c\u7684\u7ed3\u8bba\u8fdb\u884c\u8c03\u4f18\uff0c\u592a\u591a\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8d44\n             \u6e90\uff0c\u592a\u5c11\uff0c\u5219\u4fdd\u8bc1\u4e0d\u4e86\u6548\u7387\u3002\n\n--executor-memory   \u8fd9\u4e2a\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u7684\u5185\u5b58\uff0cExecutor\u7684\u5185\u5b58\u5f88\u591a\u65f6\u5019\u51b3\n                    \u5b9a\u4e86Spark\u4f5c\u4e1a\u7684\u6027\u80fd\uff0c\u800c\u4e14\u8ddf\u5e38\u89c1\u7684JVM OOM\u4e5f\u6709\u76f4\u63a5\u8054\u7cfb\n    \u8c03\u4f18\u5efa\u8bae\uff1a\u53c2\u8003\u503c --&gt; 4~8G,\u907f\u514d\u7a0b\u5e8f\u5c06\u6574\u4e2a\u96c6\u7fa4\u7684\u8d44\u6e90\u5168\u90e8\u5360\u7528,\u9700\u8981\u5148\u770b\u4e00\u4e0b\u4f60\u961f\u5217\u7684\u6700\u5927\n            \u5185\u5b58\u9650\u5236\u662f\u591a\u5c11\uff0c\u5982\u679c\u662f\u516c\u7528\u4e00\u4e2a\u961f\u5217\uff0c\u4f60\u7684num-executors * executor-memory\n            \u6700\u597d\u4e0d\u8981\u8d85\u8fc7\u961f\u5217\u76841/3 ~ 1/2\n-- executor-cores\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u7684CPU core\u6570\u91cf\u3002\u8fd9\u4e2a\u53c2\u6570\u51b3\u5b9a\u4e86\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u5e76\u884c\u6267\u884ctask\u7ebf\u7a0b\u7684\u80fd\u529b\u3002\u56e0\u4e3a\u6bcf\u4e2aCPU core\u540c\u4e00\u65f6\u95f4\u53ea\u80fd\u6267\u884c\u4e00\u4e2a\n    task\u7ebf\u7a0b\uff0c\u56e0\u6b64\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u7684CPU core\u6570\u91cf\u8d8a\u591a\uff0c\u8d8a\u80fd\u591f\u5feb\u901f\u5730\u6267\u884c\u5b8c\u5206\u914d\u7ed9\u81ea\u5df1\u7684\u6240\u6709task\u7ebf\u7a0b\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    Executor\u7684CPU core\u6570\u91cf\u8bbe\u7f6e\u4e3a2~4\u4e2a\u8f83\u4e3a\u5408\u9002\u3002\u540c\u6837\u5f97\u6839\u636e\u4e0d\u540c\u90e8\u95e8\u7684\u8d44\u6e90\u961f\u5217\u6765\u5b9a\uff0c\u53ef\u4ee5\u770b\u770b\u81ea\u5df1\u7684\u8d44\u6e90\u961f\u5217\u7684\u6700\u5927CPU core\u9650\u5236\u662f\u591a\u5c11\uff0c\u518d\u4f9d\u636e\u8bbe\u7f6e\u7684\n    Executor\u6570\u91cf\uff0c\u6765\u51b3\u5b9a\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u53ef\u4ee5\u5206\u914d\u5230\u51e0\u4e2aCPU core\u3002\u540c\u6837\u5efa\u8bae\uff0c\u5982\u679c\u662f\u8ddf\u4ed6\u4eba\u5171\u4eab\u8fd9\u4e2a\u961f\u5217\uff0c\u90a3\u4e48num-executors * executor-cores\u4e0d\u8981\u8d85\u8fc7\n    \u961f\u5217\u603bCPU core\u76841/3~1/2\u5de6\u53f3\u6bd4\u8f83\u5408\u9002\uff0c\u4e5f\u662f\u907f\u514d\u5f71\u54cd\u5176\u4ed6\u540c\u5b66\u7684\u4f5c\u4e1a\u8fd0\u884c\u3002\n\n--driver-memory\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6eDriver\u8fdb\u7a0b\u7684\u5185\u5b58\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    Driver\u7684\u5185\u5b58\u901a\u5e38\u6765\u8bf4\u4e0d\u8bbe\u7f6e\uff0c\u6216\u8005\u8bbe\u7f6e1G\u5de6\u53f3\u5e94\u8be5\u5c31\u591f\u4e86\u3002\u552f\u4e00\u9700\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u662f\uff0c\u5982\u679c\u9700\u8981\u4f7f\u7528collect\u7b97\u5b50\u5c06RDD\u7684\u6570\u636e\u5168\u90e8\u62c9\u53d6\u5230Driver\u4e0a\u8fdb\u884c\u5904\u7406\uff0c\n    \u90a3\u4e48\u5fc5\u987b\u786e\u4fddDriver\u7684\u5185\u5b58\u8db3\u591f\u5927\uff0c\u5426\u5219\u4f1a\u51fa\u73b0OOM\u5185\u5b58\u6ea2\u51fa\u7684\u95ee\u9898\u3002\n\n--spark.default.parallelism\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u6bcf\u4e2astage\u7684\u9ed8\u8ba4task\u6570\u91cf\u3002\u8fd9\u4e2a\u53c2\u6570\u6781\u4e3a\u91cd\u8981\uff0c\u5982\u679c\u4e0d\u8bbe\u7f6e\u53ef\u80fd\u4f1a\u76f4\u63a5\u5f71\u54cd\u4f60\u7684Spark\u4f5c\u4e1a\u6027\u80fd\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    Spark\u4f5c\u4e1a\u7684\u9ed8\u8ba4task\u6570\u91cf\u4e3a500~1000\u4e2a\u8f83\u4e3a\u5408\u9002\u3002\u5f88\u591a\u540c\u5b66\u5e38\u72af\u7684\u4e00\u4e2a\u9519\u8bef\u5c31\u662f\u4e0d\u53bb\u8bbe\u7f6e\u8fd9\u4e2a\u53c2\u6570\uff0c\u90a3\u4e48\u6b64\u65f6\u5c31\u4f1a\u5bfc\u81f4Spark\u81ea\u5df1\u6839\u636e\u5e95\u5c42HDFS\u7684block\u6570\u91cf\n    \u6765\u8bbe\u7f6etask\u7684\u6570\u91cf\uff0c\u9ed8\u8ba4\u662f\u4e00\u4e2aHDFS block\u5bf9\u5e94\u4e00\u4e2atask\u3002\u901a\u5e38\u6765\u8bf4\uff0cSpark\u9ed8\u8ba4\u8bbe\u7f6e\u7684\u6570\u91cf\u662f\u504f\u5c11\u7684\uff08\u6bd4\u5982\u5c31\u51e0\u5341\u4e2atask\uff09\uff0c\u5982\u679ctask\u6570\u91cf\u504f\u5c11\u7684\u8bdd\uff0c\u5c31\u4f1a\n    \u5bfc\u81f4\u4f60\u524d\u9762\u8bbe\u7f6e\u597d\u7684Executor\u7684\u53c2\u6570\u90fd\u524d\u529f\u5c3d\u5f03\u3002\u8bd5\u60f3\u4e00\u4e0b\uff0c\u65e0\u8bba\u4f60\u7684Executor\u8fdb\u7a0b\u6709\u591a\u5c11\u4e2a\uff0c\u5185\u5b58\u548cCPU\u6709\u591a\u5927\uff0c\u4f46\u662ftask\u53ea\u67091\u4e2a\u6216\u800510\u4e2a\uff0c\u90a3\u4e4890%\u7684\n    Executor\u8fdb\u7a0b\u53ef\u80fd\u6839\u672c\u5c31\u6ca1\u6709task\u6267\u884c\uff0c\u4e5f\u5c31\u662f\u767d\u767d\u6d6a\u8d39\u4e86\u8d44\u6e90\uff01\u56e0\u6b64Spark\u5b98\u7f51\u5efa\u8bae\u7684\u8bbe\u7f6e\u539f\u5219\u662f\uff0c\u8bbe\u7f6e\u8be5\u53c2\u6570\u4e3anum-executors * executor-cores\u76842~3\u500d\n    \u8f83\u4e3a\u5408\u9002\uff0c\u6bd4\u5982Executor\u7684\u603bCPU core\u6570\u91cf\u4e3a300\u4e2a\uff0c\u90a3\u4e48\u8bbe\u7f6e1000\u4e2atask\u662f\u53ef\u4ee5\u7684\uff0c\u6b64\u65f6\u53ef\u4ee5\u5145\u5206\u5730\u5229\u7528Spark\u96c6\u7fa4\u7684\u8d44\u6e90\u3002\n\n--spark.storage.memoryFraction\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6eRDD\u6301\u4e45\u5316\u6570\u636e\u5728Executor\u5185\u5b58\u4e2d\u80fd\u5360\u7684\u6bd4\u4f8b\uff0c\u9ed8\u8ba4\u662f0.6\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u9ed8\u8ba4Executor 60%\u7684\u5185\u5b58\uff0c\u53ef\u4ee5\u7528\u6765\u4fdd\u5b58\u6301\u4e45\u5316\u7684RDD\u6570\u636e\u3002\u6839\u636e\u4f60\u9009\u62e9\n    \u7684\u4e0d\u540c\u7684\u6301\u4e45\u5316\u7b56\u7565\uff0c\u5982\u679c\u5185\u5b58\u4e0d\u591f\u65f6\uff0c\u53ef\u80fd\u6570\u636e\u5c31\u4e0d\u4f1a\u6301\u4e45\u5316\uff0c\u6216\u8005\u6570\u636e\u4f1a\u5199\u5165\u78c1\u76d8\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    \u5982\u679cSpark\u4f5c\u4e1a\u4e2d\uff0c\u6709\u8f83\u591a\u7684RDD\u6301\u4e45\u5316\u64cd\u4f5c\uff0c\u8be5\u53c2\u6570\u7684\u503c\u53ef\u4ee5\u9002\u5f53\u63d0\u9ad8\u4e00\u4e9b\uff0c\u4fdd\u8bc1\u6301\u4e45\u5316\u7684\u6570\u636e\u80fd\u591f\u5bb9\u7eb3\u5728\u5185\u5b58\u4e2d\u3002\u907f\u514d\u5185\u5b58\u4e0d\u591f\u7f13\u5b58\u6240\u6709\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u6570\u636e\u53ea\n    \u80fd\u5199\u5165\u78c1\u76d8\u4e2d\uff0c\u964d\u4f4e\u4e86\u6027\u80fd\u3002\u4f46\u662f\u5982\u679cSpark\u4f5c\u4e1a\u4e2d\u7684shuffle\u7c7b\u64cd\u4f5c\u6bd4\u8f83\u591a\uff0c\u800c\u6301\u4e45\u5316\u64cd\u4f5c\u6bd4\u8f83\u5c11\uff0c\u90a3\u4e48\u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u9002\u5f53\u964d\u4f4e\u4e00\u4e9b\u6bd4\u8f83\u5408\u9002\u3002\u6b64\u5916\uff0c\u5982\u679c\u53d1\u73b0\n    \u4f5c\u4e1a\u7531\u4e8e\u9891\u7e41\u7684gc\u5bfc\u81f4\u8fd0\u884c\u7f13\u6162\uff08\u901a\u8fc7spark web ui\u53ef\u4ee5\u89c2\u5bdf\u5230\u4f5c\u4e1a\u7684gc\u8017\u65f6\uff09\uff0c\u610f\u5473\u7740task\u6267\u884c\u7528\u6237\u4ee3\u7801\u7684\u5185\u5b58\u4e0d\u591f\u7528\uff0c\u90a3\u4e48\u540c\u6837\u5efa\u8bae\u8c03\u4f4e\u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u3002\n\n--spark.shuffle.memoryFraction\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6eshuffle\u8fc7\u7a0b\u4e2d\u4e00\u4e2atask\u62c9\u53d6\u5230\u4e0a\u4e2astage\u7684task\u7684\u8f93\u51fa\u540e\uff0c\u8fdb\u884c\u805a\u5408\u64cd\u4f5c\u65f6\u80fd\u591f\u4f7f\u7528\u7684Executor\u5185\u5b58\u7684\u6bd4\u4f8b\uff0c\u9ed8\u8ba4\u662f0.2\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cExecutor\n    \u9ed8\u8ba4\u53ea\u670920%\u7684\u5185\u5b58\u7528\u6765\u8fdb\u884c\u8be5\u64cd\u4f5c\u3002shuffle\u64cd\u4f5c\u5728\u8fdb\u884c\u805a\u5408\u65f6\uff0c\u5982\u679c\u53d1\u73b0\u4f7f\u7528\u7684\u5185\u5b58\u8d85\u51fa\u4e86\u8fd9\u4e2a20%\u7684\u9650\u5236\uff0c\u90a3\u4e48\u591a\u4f59\u7684\u6570\u636e\u5c31\u4f1a\u6ea2\u5199\u5230\u78c1\u76d8\u6587\u4ef6\u4e2d\u53bb\uff0c\u6b64\u65f6\n    \u5c31\u4f1a\u6781\u5927\u5730\u964d\u4f4e\u6027\u80fd\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    \u5982\u679cSpark\u4f5c\u4e1a\u4e2d\u7684RDD\u6301\u4e45\u5316\u64cd\u4f5c\u8f83\u5c11\uff0cshuffle\u64cd\u4f5c\u8f83\u591a\u65f6\uff0c\u5efa\u8bae\u964d\u4f4e\u6301\u4e45\u5316\u64cd\u4f5c\u7684\u5185\u5b58\u5360\u6bd4\uff0c\u63d0\u9ad8shuffle\u64cd\u4f5c\u7684\u5185\u5b58\u5360\u6bd4\u6bd4\u4f8b\uff0c\u907f\u514dshuffle\u8fc7\u7a0b\u4e2d\u6570\u636e\u8fc7\u591a\n    \u65f6\u5185\u5b58\u4e0d\u591f\u7528\uff0c\u5fc5\u987b\u6ea2\u5199\u5230\u78c1\u76d8\u4e0a\uff0c\u964d\u4f4e\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5982\u679c\u53d1\u73b0\u4f5c\u4e1a\u7531\u4e8e\u9891\u7e41\u7684gc\u5bfc\u81f4\u8fd0\u884c\u7f13\u6162\uff0c\u610f\u5473\u7740task\u6267\u884c\u7528\u6237\u4ee3\u7801\u7684\u5185\u5b58\u4e0d\u591f\u7528\uff0c\u90a3\u4e48\u540c\u6837\u5efa\u8bae\u8c03\u4f4e\n    \u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u3002\n\u8d44\u6e90\u53c2\u6570\u7684\u8c03\u4f18\uff0c\u6ca1\u6709\u4e00\u4e2a\u56fa\u5b9a\u7684\u503c\uff0c\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u5b9e\u9645\u60c5\u51b5\uff08\u5305\u62ecSpark\u4f5c\u4e1a\u4e2d\u7684shuffle\u64cd\u4f5c\u6570\u91cf\u3001RDD\u6301\u4e45\u5316\u64cd\u4f5c\u6570\u91cf\u4ee5\u53caspark web ui\u4e2d\u663e\u793a\u7684\u4f5c\u4e1agc\u60c5\u51b5\uff09\uff0c\n\u5408\u7406\u5730\u8bbe\u7f6e\u4e0a\u8ff0\u53c2                \n</code></pre>","tags":["Data Engineer"]},{"location":"2024/09/autogen-httpclient/","title":"AutoGen HttpClient","text":""},{"location":"2024/09/autogen-httpclient/#httpclient","title":"HttpClient","text":"my_client.py<pre><code>import httpx\n\nclass MyHttpClient(httpx.Client):\n    def __deepcopy__(self, dummy):\n        return self\n</code></pre>"},{"location":"2024/09/autogen-httpclient/#autogen-llm-config","title":"AutoGen LLM Config","text":"<p>Now we can add our proxy config in our client, which will resolve connection issue. </p><pre><code>\"http_client\": MyHttpClient(verify=False, proxy=\"http://&lt;&gt;\")\n</code></pre>"},{"location":"2024/10/reflex-learning/","title":"Reflex Learning","text":""},{"location":"2024/10/reflex-learning/#reflex-pynecone","title":"Reflex (pynecone)","text":"<p>Reflex is a library to build full-stack web apps in pure Python.</p> <ul> <li>Repo</li> <li>Video  </li> </ul>"},{"location":"2024/10/zio/","title":"ZIO","text":""},{"location":"2024/10/zio/#scala-full-stack","title":"Scala Full Stack","text":"<p>Recently I read a Blog that introducing how to use scala framework ZIO to build an application.</p> <p>This video is helpful to understand it. </p>"},{"location":"2024/10/python-decorator/","title":"Python Decorator","text":""},{"location":"2024/10/python-decorator/#python-decorators","title":"Python decorators","text":"<p> Why we need decorator</p> <p> It will extend your function behaviors during runtime.</p> <p>For example, you already have a function <code>say_hi</code></p> <pre><code>def say_hi(name: str):\n    return f\"Hi! {name}\"\n</code></pre> <ul> <li>function name <code>say_hi</code></li> <li>parameter <code>name</code> and type <code>str</code></li> <li>one output <code>say_hi('Apple') =&gt; Hi! Apple</code></li> </ul> <p>Next we plan to add one introduction to the output, such as <code>Hi! Apple. I'm Bob</code>.</p> <ol> <li> <p>Modify your function </p><pre><code>def say_hi(name: str, my_name:str):\n    return f\"Hi! {name}. I'm {my_name}\"\n</code></pre> If this is only used once in our project, it\u2019s manageable.  However, modifying the function signature means that every instance of its use throughout the project must be updated, which is both tedious and time-consuming. </li> <li> <p>Use decorator </p><pre><code>def add_intro(my_name):\n    def dec(func):\n        def wrapper(name):\n            return func(name) + f\". I'm {my_name}\"\n        return wrapper\n    return dec\n\n@add_intro(\"Bob\")\ndef say_hi(name: str):\n    return f\"Hi! {name}\"\n</code></pre> <code>Function signature is not changed and function behavior is enriched</code> </li> </ol>"},{"location":"2024/10/python-decorator/#how-to-create-decorator","title":"How to create decorator","text":""},{"location":"2024/10/python-decorator/#decorator-function","title":"Decorator Function","text":"<p>Before starting decorator, we have to understand</p> <ul> <li>original function<ul> <li>function name</li> <li>function parameters and types</li> </ul> </li> <li>decorator function<ul> <li>extra parameter</li> <li>new features</li> </ul> </li> </ul> original function<pre><code>def hello(name:str) -&gt; str:\n    return f\"hello, {name}\"\n</code></pre> <p>We have an original function</p> <ul> <li>function name: <code>hello</code></li> <li>parameters: <code>name</code></li> <li>types: <code>str</code></li> </ul> <p>Now we can use these to build decorator function <code>my_dec</code> </p>my_dec<pre><code>def my_dec(func):\n    def wrapper(name:str):\n        return func(name)\n    return wrapper\n</code></pre> <p>Explanation:</p> <ul> <li><code>line 1</code><ul> <li>decorator name : <code>my_dec</code></li> <li>decorator parameter : <code>func</code> ( we did not use type hint here)</li> <li>it mean <code>my_dec</code> will decorate function <code>func</code></li> </ul> </li> <li><code>line 2</code><ul> <li>inner function <code>wrapper</code> ( any function names)</li> <li>inner function signature. It MUST be a superset of your original signature <p>e.g. only 1 parameter <code>name</code> at <code>hello</code> the wrapper function should include <code>name</code> at least it could be <code>wrapper(name)</code>, <code>wrapper(name, name1=None)</code>, <code>wrapper(name, *args, **kwargs)</code> <code>wrapper(*args, **kwargs)</code> etc.</p> </li> </ul> </li> <li><code>line 3</code><ul> <li><code>return value</code></li> </ul> </li> <li><code>line 4</code><ul> <li> return a function name <code>wrapper</code></li> </ul> </li> </ul> <p>Now the decorator is working as <code>func = my_dec(func)</code></p> <ul> <li>function IN : <code>func</code></li> <li>function OUT: <code>wrapper</code> and reassigned <code>wrapper</code> to <code>func</code></li> </ul>"},{"location":"2024/10/python-decorator/#decorator-class","title":"Decorator Class","text":"Decorator Class<pre><code>class DecoratorClass:\n    def __init__(self, decorator_param: str):\n        self.decorator_param = decorator_param\n\n    def __call__(self, func):\n        def wrapper(original_param):\n            \"\"\"wrapper doc\"\"\"\n            return func(original_param) + self.decorator_param\n        return wrapper\n\n@DecoratorClass(decorator_param=\"!!!\")\ndef hello_again(name: str):\n    \"\"\"original docStr\"\"\"\n    return f\"Hello {name}\"\n</code></pre> <p>it's obviously to understand how to setup decorator's parameters.</p>"},{"location":"2024/10/python-decorator/#built-in-python-decorator","title":"built-in python decorator","text":"<p>Each function in <code>python</code> has metadata</p> <ul> <li><code>__name__</code></li> <li><code>__doc__</code></li> </ul> <p>Either <code>function</code> or <code>class</code> cannot update decorator function metadata.</p> <pre><code>print(hello_again.__name__) # wrapper\n\nprint(hello_again.__doc__) # Wrapper Doc\n</code></pre> <p> How to update function metadata</p> <ul> <li>manually update     <pre><code>class DecoratorClass1:\ndef __init__(self, decorator_param: str):\n    self.decorator_param = decorator_param\n\ndef __call__(self, func):\n    def wrapper(original_param):\n        \"\"\"Wrapper Doc\"\"\"\n        return func(original_param) + self.decorator_param\n\n    # Manually update metadata\n    wrapper.__doc__ = func.__doc__\n    wrapper.__name__ = func.__name__\n\n    return wrapper\n</code></pre></li> <li>use python built-in <code>wrapper</code></li> </ul>"},{"location":"2024/11/databricks-wheel-job/","title":"Databricks Wheel Job","text":""},{"location":"2024/11/databricks-wheel-job/#databricks-jobs","title":"Databricks Jobs","text":"<p>Recently I successfully deploy my python wheel to Databricks Cluster. Here are some tips if you plan to deploy <code>pyspark</code>.</p> <ul> <li><code>pyspark</code> project</li> <li><code>pytest</code></li> </ul>"},{"location":"2024/11/databricks-wheel-job/#pyspark-project","title":"<code>pyspark</code> project","text":"<p>My previous spark project is <code>scala</code> based and I use <code>IDEA</code> to <code>compile</code> and <code>test</code> conveniently.</p> <p></p> <p><code>Databricks</code> Job nice UI save your time to create <code>JAR</code> job.</p> <p>This is official guide: Databricks Wheel Job</p> <p>What I did:</p> <ol> <li> <p>Initialize a python project     </p><pre><code># create python virtual environment\npython -m venv pyspark_venv\n\n# active your venv\nsource pyspark_venv/bin/activate\n\n# check your current python\nwhich python\n\n# install python lib\npip install uv ruff pyspark pytest wheel\n\n## if pip failed at proxy error\n## adding your proxy \n## --proxy http://proxy:port\n\n# create your project\nuv init --package &lt;your package name&gt;\n</code></pre> <p>After <code>uv</code> command complete, a nice python project is created. </p><pre><code>pyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 pyspark_app\n        \u2514\u2500\u2500 __init__.py\n</code></pre> </li> <li> <p> pyspark <code>entry point</code></p> <ul> <li>add one file <code>__main__.py</code> in pyspark_app</li> <li>modify <code>[project.scripts]</code> in <code>pyproject.toml</code> and this is <code>entry point</code> of Databricks job</li> </ul> <p>Now the project is </p><pre><code>pyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 pyspark_app\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 __main__.py\n</code></pre> </li> </ol>"},{"location":"2024/11/databricks-wheel-job/#pytest","title":"<code>pytest</code>","text":"<p>Please check your <code>pytest</code> installed. Let create a new package <code>test</code></p> <pre><code>pyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 pyspark_app\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 __main__.py\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 conftest.py\n            \u2514\u2500\u2500 test_spark.py\n</code></pre> test_spark<pre><code>def test_spark(init_spark):\n    spark = init_spark\n    df = spark.range(10)\n    df.show()\n\n\"\"\" output\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/01 20:59:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nPASSED                                         [100%]+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\"\"\"\n</code></pre> <p>Now you can work on your spark application with test</p>"},{"location":"2024/11/databricks-wheel-job/#wheel-file","title":"wheel file","text":"<p>Final step is building <code>wheel</code> file</p> <p></p><pre><code># 1. change your work directory to pyproject.toml\n# 2. run below command\npython -m build --wheel\n\n# project is now changing to\n\npyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 build\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bdist.macosx-12.0-x86_64\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lib\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pyspark_app\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __main__.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 test\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 conftest.py\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 test_spark.py\n\u251c\u2500\u2500 dist\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pyspark_app-0.1.0-py3-none-any.whl\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 pyspark_app\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __main__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 test\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 conftest.py\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 test_spark.py\n    \u2514\u2500\u2500 pyspark_app.egg-info\n        \u251c\u2500\u2500 PKG-INFO\n        \u251c\u2500\u2500 SOURCES.txt\n        \u251c\u2500\u2500 dependency_links.txt\n        \u251c\u2500\u2500 entry_points.txt\n        \u2514\u2500\u2500 top_level.txt\n</code></pre> Your wheel file is at line <code>20</code> <p>Go to view all at  Project template</p>"},{"location":"2024/11/pyspark-dataframe-transformation/","title":"PySpark Dataframe Transformation","text":""},{"location":"2024/11/pyspark-dataframe-transformation/#migration-from-scala-to-python","title":"Migration from <code>Scala</code> to <code>Python</code>","text":"<p>Migrating a history <code>Scala</code> project to <code>Python</code>, I find some tips that can help me forget the <code>type</code> system in <code>scala</code>. Feel good!!! </p>"},{"location":"2024/11/pyspark-dataframe-transformation/#dataclass-vs-case-class","title":"<code>dataclass</code> vs <code>case class</code>","text":"<p>You have to create a <code>case class</code> for each data model in Scala,  while<code>dataclass</code> is your alternative in python</p> <pre><code>@dataclass()\nclass Event:\n    event_id: int\n    event_name: str\n</code></pre>"},{"location":"2024/11/pyspark-dataframe-transformation/#create-dataframe-from-dataclass","title":"Create Dataframe from <code>dataclass</code>","text":"<pre><code>spark = (\n    SparkSession.builder.master(\"local[*]\")\n    .appName(\"test\")\n    .getOrCreate()\n)\nd = [\n    Event(1, \"abc\"),\n    Event(2, \"ddd\"),\n]\n\n# Row object\ndf = spark.createDataFrame(Row(**e.__dict__) for e in d)\ndf.show()\n# +--------+----------+\n# |event_id|event_name|\n# +--------+----------+\n# |       1|       abc|\n# |       2|       ddd|\n# +--------+----------+\n</code></pre>"},{"location":"2024/12/gradio-with-ollama/","title":"Gradio with Ollama","text":""},{"location":"2024/12/gradio-with-ollama/#simple-unstructured-file-processing","title":"Simple Unstructured file processing","text":"<p>We have a lot of pdf files that contain import information, however, the information are unstructured (text, table, image, etc...). To extract and utilize them in our downstream job, an open source unstructured is helpful to implement what we want</p> <p></p>"},{"location":"2024/12/gradio-with-ollama/#demo-app","title":"Demo App","text":"<pre><code>\n%%{init: { 'look':'handDrawn' } }%%\n\nflowchart LR\n  A[Gradio UI] --&gt; B(Ollama Server)\n  B --&gt; C[\"\n  #bull; Gemma2\n  #bull; Llama3\n  #bull; Phi3\n  #bull; Mistral\n  \"]\n  style C color:#FFFFFF,text-align:left,fill:#D2691E\n  style B fill:#FFE4C4\n</code></pre>"},{"location":"2025/02/langgraph-vs-autogen/","title":"LangGraph VS AutoGen","text":""},{"location":"2025/02/langgraph-vs-autogen/#langgraph-vs-autogen","title":"LangGraph VS AutoGen","text":"Feature LangGraph AutoGen Core Concept Graph-based workflow for LLM chaining Multi-agent system with customizable agents Architecture Node-based computation graph Message-passing system between agents Ease of Use Requires defining workflows explicitly as graphs Provides high-level agents for easy configuration Flexibility High (can create complex workflows, DAGs) High (supports various agent types and interactions) Concurrency Supports async execution for parallel nodes Supports multi-agent asynchronous interactions Customization Fully customizable workflows and control flow Customizable agents and message routing strategies LLM Integration Supports OpenAI, Anthropic, and other providers via LangChain Primarily supports OpenAI but extensible State Management Built-in graph state tracking Agent state managed via messages Error Handling Easier to debug with structured DAG execution Debugging can be complex due to emergent agent behavior Use Cases Workflow automation, decision trees, RAG pipelines Autonomous multi-agent collaboration, code execution, RAG Complexity Handling High control over execution paths More emergent behavior, less structured execution Multi-Agent Support Limited (single LLM per node, multi-step workflows) Strong support for multiple interacting agents Pros Fine-grained control over execution paths and state management Easily integrates with LangChain\u2019s ecosystem (retrievers, tools, memory) Supports parallel execution and dependency-based workflows Better for structured workflows like data pipelines, RAG, and decision trees Designed for multi-agent  collaboration, making it ideal for autonomous  agents Easier to  set up  for  conversational  AI, coding  assistants,  and  team-based LLM  interactions Includes  specialized  agents  like CodeExecutorAgent  and SocietyOfMindAgent Strong asynchronous processing capabilities for real-time interactions Cons Requires explicit graph  definition, which can be verbose Less emergent behavior compared to agent-based approaches Multi-agent interactions are not as native as in AutoGen State  management is more  implicit via messages  rather than a  structured graph More opinionated, requiring adaptation to its agent-based paradigm <p>Tips </p> Use Case Recommended Framework Workflow automation (DAGs, logic flows) LangGraph Multi-agent collaboration (AI teams, autonomous systems) AutoGen RAG pipeline with structured retrieval and ranking LangGraph Conversational AI with multiple agents AutoGen Decision trees or conditional logic workflows LangGraph Autonomous coding assistants (e.g., pair programming) AutoGen Parallel execution of tasks LangGraph Emergent multi-agent reasoning AutoGen"},{"location":"2025/02/crawling-the-web-with-llm/","title":"Crawling the Web with LLM","text":""},{"location":"2025/02/crawling-the-web-with-llm/#crawling-the-web-with-large-language-models-llms","title":"Crawling the Web with Large Language Models (LLMs)","text":"<p>Frameworks</p> <ul> <li>Skyvern</li> <li>ScrapegraphAI</li> <li>Crawl4AI</li> <li>Reader</li> <li>Firecrawl</li> <li>Markdowner</li> </ul> <p>Code Examples</p> <pre><code>import json\n\nfrom scrapegraphai.graphs import SmartScraperGraph\n\ngraph_config = {\n    \"llm\": {\n        \"model\": \"ollama/phi4\",\n        \"temperature\": 0,\n        \"format\": \"json\",  # Ollama needs the format to be specified explicitly\n        \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n    },\n    \"embeddings\": {\n        \"model\": \"ollama/jina/jina-embeddings-v2-base-en\",\n        \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n    }\n}\n\nsmart_scraper_graph = SmartScraperGraph(\n    prompt=\"Extract useful information from the webpage, including a description of what the company does, founders and social media links\",\n    source=\"https://scrapegraphai.com/\",\n    config=graph_config\n)\n\nresult = smart_scraper_graph.run()\nprint(json.dumps(result, indent=4))\n</code></pre> <p>Output: </p><pre><code>{\n    \"title\": \"ScrapeGraphAI: AI-Powered Web Scraping Made Easy\",\n    \"description\": \"ScrapeGraphAI is a cutting-edge tool designed to simplify web scraping by leveraging Large Language Models (LLMs). It offers an easy-to-use API that allows developers and AI agents to extract structured data from various types of websites without the need for complex coding. With features like handling website changes, scalability, and integration with popular programming languages such as Python, JavaScript, and TypeScript, ScrapeGraphAI is perfect for anyone looking to harness web data efficiently.\",\n    \"features\": {\n        \"ease_of_use\": \"ScrapeGraphAI provides a user-friendly API that requires minimal coding effort, making it accessible even for those without extensive technical expertise.\",\n        \"integration\": \"The tool seamlessly integrates with Python, JavaScript, and TypeScript, allowing developers to incorporate web scraping into their existing projects effortlessly.\",\n        \"website_handling\": \"Capable of handling diverse websites including e-commerce platforms, social media sites, blogs, forums, and more. It efficiently manages dynamic content loaded via JavaScript or AJAX.\",\n        \"adaptability\": \"ScrapeGraphAI is designed to adapt to website changes automatically, reducing the need for manual maintenance and ensuring consistent data extraction.\",\n        \"performance_and_reliability\": \"The platform offers high performance and reliability, with features like rate limiting, IP rotation, and CAPTCHA solving to ensure smooth operation even under challenging conditions.\"\n    },\n    \"pricing\": {\n        \"free_plan\": \"Includes 1000 requests per month, suitable for small-scale projects or testing purposes.\",\n        \"paid_plans\": [\n            {\n                \"starter\": \"$9.99/month\",\n                \"requests\": \"10,000\"\n            },\n            {\n                \"pro\": \"$49.99/month\",\n                \"requests\": \"50,000\"\n            },\n            {\n                \"enterprise\": \"Custom pricing\",\n                \"requests\": \"Unlimited\"\n            }\n        ]\n    },\n    \"founders\": {\n        \"marco_perini\": \"Founder &amp; CTO - [LinkedIn](https://www.linkedin.com/in/perinim/)\",\n        \"marco_vinciguerra\": \"Founder &amp; Software Engineer - [LinkedIn](https://www.linkedin.com/in/marco-vinciguerra-7ba365242/)\",\n        \"lorenzo_padoan\": \"Founder &amp; CEO - [LinkedIn](https://www.linkedin.com/in/lorenzo-padoan-4521a2154/)\"\n    },\n    \"contact_info\": {\n        \"email\": \"contact@scrapegraphai.com\",\n        \"social_media\": {\n            \"linkedin\": \"[ScrapeGraphAI LinkedIn](https://www.linkedin.com/company/101881123)\",\n            \"twitter\": \"[Twitter](https://x.com/scrapegraphai)\",\n            \"github\": \"[GitHub](https://github.com/ScrapeGraphAI/Scrapegraph-ai)\",\n            \"discord\": \"[Discord](https://discord.gg/uJN7TYcpNa)\"\n        }\n    },\n    \"legal_info\": {\n        \"privacy_policy\": \"[Privacy Policy](/privacy)\",\n        \"terms_of_service\": \"[Terms of Service](/terms)\",\n        \"api_status\": \"[API Status](https://scrapegraphapi.openstatus.dev)\"\n    },\n    \"footer\": \"\\u00a9 2025 - ScrapeGraphAI, Inc\"\n}\n</code></pre>"},{"location":"2025/03/genai-projects/","title":"GenAI Projects","text":"<p>Learning never exhausts the mind   \u00a0 \u00a0 \u00a0 \u00a0 \u2015 Leonardo da Vinci</p>"},{"location":"2025/03/genai-projects/#collections","title":"Collections","text":""},{"location":"2025/03/genai-projects/#blogs","title":"Blogs","text":"Name URL LLM terminology Link A Critical Look at MCP Link Ilya Rice: How I Won the Enterprise RAG Challenge Link"},{"location":"2025/03/genai-projects/#papers","title":"Papers","text":"Paper Link Preview A Comprehensive Overview of Large Language Models Click ref KBLaM: Knowledge Base augmented Language Model Click Retrieval-Augmented Generation for Large Language Models: A Survey Click ref Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition Click Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case Study with Locally Deployed Ollama Models Click Google Prompt Engineering whitepaper ref Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time Click LLM Post-Training: A Deep Dive into Reasoning Large Language Models ref Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder Click What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? Click"},{"location":"2025/03/genai-projects/#modelrepository","title":"Model/Repository","text":"Model/Repository Link ds4sd/SmolDocling-256M-preview Hugging Face qlib GitHub ByteDance/Dolphin Hugging Face"},{"location":"2025/03/genai-projects/#agent-frameworks","title":"Agent Frameworks","text":"Agentic Framework Name Github Link LangChain GitHub llama_index GitHub Autogen GitHub Haystack GitHub CrewAI (flow) GitHub langflow GitHub smolagents GitHub Pydantic AI GitHub pyspur GitHub agno (phiData) Github instructor Github DSpy Github JS Only --- n8n GitHub"},{"location":"2025/03/genai-projects/#my-llm-working-projects","title":"My LLM Working Projects","text":""},{"location":"2025/03/genai-projects/#schema-mapping-completed","title":"Schema Mapping (Completed)","text":"<p>LLM can be used to map schema from one format to another. This is useful for data migration and integration.</p> Resources Link blog (inspired by this blog) Blog paper (research paper on schema mapping) ref"},{"location":"2025/03/genai-projects/#feature-engineering-ongoing","title":"Feature Engineering (Ongoing)","text":"<p>LLM can be used to generate features for machine learning models. This can save time and effort in the feature engineering process.</p> Resources Link paper (research paper on schema mapping) ref paper (research paper on schema mapping) ref"},{"location":"2025/03/genai-projects/#rag-pdf-converting-completed","title":"RAG: PDF Converting (Completed)","text":"<p>LLM can be used to convert PDFs into structured data. This is useful for extracting information from unstructured documents.</p>"},{"location":"2025/03/genai-projects/#llm-fine-tuning-ongoing","title":"LLM: Fine-Tuning (Ongoing)","text":""},{"location":"2025/04/ollama-import-gguf-models/","title":"Ollama Import GGUF Models","text":""},{"location":"2025/04/ollama-import-gguf-models/#ollama-models","title":"Ollama Models","text":"<p>If you're looking to experiment with various models easily, importing GGUF might be your go-to method.  Here's how it works: </p> <ul> <li>You start by creating a <code>Modelfile</code>, which acts as a key to unlock any GGUF model you want to use.</li> </ul> Modelfile<pre><code>FROM /agentica-org_DeepCoder-14B-Preview-Q8_0.gguf\nFROM /agentica-org_DeepCoder-14B-Preview.imatrix\n\nTEMPLATE \"\"\"\n{{- if .System }}{{ .System }}{{ end }}\n{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1}}\n{{- if eq .Role \"user\" }}&lt;\uff5cUser\uff5c&gt;{{ .Content }}\n{{- else if eq .Role \"assistant\" }}&lt;\uff5cAssistant\uff5c&gt;{{ .Content }}{{- if not $last }}&lt;\uff5cend\u2581of\u2581sentence\uff5c&gt;{{- end }}\n{{- end }}\n{{- if and $last (ne .Role \"assistant\") }}&lt;\uff5cAssistant\uff5c&gt;{{- end }}\n{{- end }}\n\"\"\"\n</code></pre> <ul> <li>Once that file is ready, all you need to do is run a simple command, and voil\u00e0\u2014your model is imported and ready for action!</li> </ul> <pre><code>ollama create deepcoder -f ./Modelfile\n\n# gathering model components \n# copying file sha256:8add5abcbf3c0413496f039275119f6d74555a16e410c32ada75d69815d904cb 100% \n# parsing GGUF \n# using existing layer sha256:8add5abcbf3c0413496f039275119f6d74555a16e410c32ada75d69815d904cb \n# creating new layer sha256:6b62346ff5a355054f0cf583cb76c4cb0841c729114aae34d40e75c7170a8c59 \n# writing manifest \n# success \n</code></pre>"},{"location":"2025/04/rag-reranking/","title":"RAG-Reranking","text":""},{"location":"2025/04/rag-reranking/#reranking-in-retrieval-augmented-generation-rag","title":"Reranking in Retrieval-Augmented Generation (RAG)","text":"<p>Retrieval-Augmented Generation (RAG) is a powerful approach that combines retrieval and generation to produce high-quality responses.  However, the quality of the final response can be significantly influenced by the effectiveness of the retrieval process.</p> <p><code>Reranking</code> can improve the quality of the final response by reordering the retrieved documents based on their relevance to the query.  In this blog, we will discuss how reranking can be integrated into RAG and its benefits.</p> <p>This Blog is good for you to choose correct <code>reranker</code> model.</p>"},{"location":"2025/04/rag-reranking/#langchain-implementation","title":"<code>langchain</code> implementation","text":"reranking.py<pre><code>from langchain_community.document_loaders import TextLoader\nfrom langchain_postgres import PGVector\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import CrossEncoderReranker\nfrom langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\n\ndef pretty_print_docs(docs):\n    print(\n        f\"\\n{'-' * 100}\\n\".join(\n            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n        )\n    )\n\n# Text split\n# chunking\ndocuments = TextLoader(\"state_of_the_union.txt\").load()\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n\nembeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2/\")\nconnection = \"postgresql+psycopg://\"  # Uses psycopg3!\ncollection_name = \"reranking_test\"\n\n\n# indexing\nvector_store = PGVector(\n    embeddings=embeddings,\n    collection_name=collection_name,\n    connection=connection,\n    use_jsonb=True,\n)\n\nvector_store.add_documents(texts, ids=[i for i, _ in enumerate(texts, start=1)])\n\n# regular retrieval\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 20})\n\nquery = \"What is the plan for the economy?\"\ndocs = retriever.invoke(query)\nprint(\"\\nRetrieved Documents:\\n\")\npretty_print_docs(docs)\n\n# reranking with CrossEncoder\nmodel = HuggingFaceCrossEncoder(model_name=\"/Users/binzhang/models/BAAI/bge-reranker-v2-m3\")\ncompressor = CrossEncoderReranker(model=model, top_n=3)\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor, base_retriever=retriever #reordering after retrieval\n)\ncompressed_docs = compression_retriever.invoke(query)\nprint(\"\\nReranked Documents:\\n\")\npretty_print_docs(compressed_docs)\n</code></pre>"},{"location":"2025/04/rag-reranking/#calculate-score-of-reranking-pairs","title":"Calculate Score of reranking pairs","text":"calculate_score.py<pre><code>from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n\n# Initialize the cross encoder\ncross_encoder = HuggingFaceCrossEncoder(\n    model_name=\"BAAI/bge-reranker-v2-m3\",\n    model_kwargs={'device': 'cpu'}\n)\n\n# Create text pairs to score\ntext_pairs = [\n    (\"How do I bake bread?\", \"This is a recipe for sourdough bread\"),\n    (\"How do I bake bread?\", \"The weather is nice today\")\n]\n\n# Get similarity scores\nscores = cross_encoder.score(text_pairs)\n\nprint(scores)\n</code></pre>"},{"location":"2025/04/mcp-server--client-sse/","title":"MCP Server & Client (SSE)","text":""},{"location":"2025/04/mcp-server--client-sse/#guide-to-setting-up-a-mcp-serverclient-sse-protocol","title":"Guide to Setting Up a MCP Server/Client (SSE Protocol)","text":"<p>This guide is inspired by </p> <ul> <li>Step-by-Step Guide: Building an MCP Server using Python-SDK, AlphaVantage &amp; Claude AI</li> <li>Model Context Protocol (MCP) lab</li> </ul> <p>The blog is using <code>stdio</code> for communication, but we will use <code>Server Sent Events (SSE)</code> for remote communication. </p>"},{"location":"2025/04/mcp-server--client-sse/#requirements","title":"Requirements","text":"Requirement Description Docker Containerization platform Python &gt;=3.11 Ollama LLM inference service Model Name qwq:32b-q8_0"},{"location":"2025/04/mcp-server--client-sse/#mcp-sse-server-connection","title":"MCP SSE Server Connection","text":"<ul> <li> <p>Extension: <code>cline</code> or <code>continue</code></p> </li> <li> <p>Docker host </p><pre><code>docker pull binzhango/mcp_server_sse:latest\n</code></pre> </li> </ul>"},{"location":"2025/04/mcp-server--client-sse/#cline-configuration","title":"<code>cline</code> configuration","text":"<ul> <li>Add MCP server in <code>cline</code></li> </ul> <ul> <li>Verify MCP tools</li> </ul> <ul> <li>submit one question and <code>cline</code> will ask for the approval of calling tool</li> </ul> <ul> <li>After approval, <code>cline</code> will call the MCP server and get the response</li> </ul>"},{"location":"2025/04/mcp-server--client-sse/#mcp-client","title":"MCP client","text":"<p>Beside the vscode extension hosts like (<code>cline</code>, <code>continue</code>), you can write your own client to connect to the MCP server. The client should be able to send and receive messages using Server Sent Events (SSE). Here is a simple example in Python:</p>"},{"location":"2025/04/mcp-server--client-sse/#example-of-python-client","title":"Example of python client","text":"<ul> <li> <p><code>run</code> below command to start the client </p><pre><code> python mcp_sse_client.py http://localhost:8000/sse \n\nInitialized SSE client...\nListing tools...\n\nConnected to server with tools:\n['calculate_moving_average', 'calculate_rsi', 'trade_recommendation']\n\nMCP Client Started!\nType your queries or 'quit' to exit.\n\n\n# Input your query here, for example:\nQuery: analyze google stock\n</code></pre> </li> <li> <p>After inputting the query, you will get the response from the server. The response will be in JSON format. </p><pre><code>#################################################################\n# This is the first response from the server,\n# which evaluates the query and determines which tools to use.\n#################################################################\nTool trade_recommendation return: {\"symbol\": \"GOOG\", \"recommendation\": \"SELL\", \"risk_level\": \"MEDIUM\", \"signal_strength\": -1, \"ma_signal\": \"BEARISH (Short MA below Long MA)\", \n\"rsi_signal\": \"NEUTRAL\", \"current_price\": 88.31172180175781, \"analysis\": \"# Trading Recommendation for GOOG\\n\\n    ## Summary\\n    Recommendation: SELL\\n    Risk Level: MEDIUM\\n   \nSignal Strength: -1.0 / 4.5\\n\\n    ## Technical Indicators\\n    Moving Averages: BEARISH (Short MA below Long MA)\\n    Recent Crossover: No\\n    RSI (14): 39.25 - NEUTRAL\\n\\n    ##\nReasoning\\n    This recommendation is based on a combination of Moving Average analysis and RSI indicators.\\n    \\n\\n    ## Action Plan\\n    Start reducing position on strength or \nset trailing stop losses.\\n    \"}\nmessages\n[\n    {'role': 'user', 'content': 'analyze google stock'},\n    {\n        'role': 'assistant',\n        'content': None,\n        'tool_calls': [ChatCompletionMessageToolCall(id='call_0xaboll8', function=Function(arguments='{\"symbol\":\"GOOG\"}', name='trade_recommendation'), type='function', index=0)]\n    },\n    {\n        'role': 'tool',\n        'tool_call_id': 'call_0xaboll8',\n        'content': '{\"symbol\": \"GOOG\", \"recommendation\": \"SELL\", \"risk_level\": \"MEDIUM\", \"signal_strength\": -1, \"ma_signal\": \"BEARISH (Short MA below Long MA)\", \"rsi_signal\": \n\"NEUTRAL\", \"current_price\": 88.31172180175781, \"analysis\": \"# Trading Recommendation for GOOG\\\\n\\\\n    ## Summary\\\\n    Recommendation: SELL\\\\n    Risk Level: MEDIUM\\\\n    Signal \nStrength: -1.0 / 4.5\\\\n\\\\n    ## Technical Indicators\\\\n    Moving Averages: BEARISH (Short MA below Long MA)\\\\n    Recent Crossover: No\\\\n    RSI (14): 39.25 - NEUTRAL\\\\n\\\\n    ##\nReasoning\\\\n    This recommendation is based on a combination of Moving Average analysis and RSI indicators.\\\\n    \\\\n\\\\n    ## Action Plan\\\\n    Start reducing position on \nstrength or set trailing stop losses.\\\\n    \"}'\n    }\n]\n\n#################################################################\n# This is the seconde response from the server,\n# which give your final response after tools execution.\n#################################################################\n\n\n[Calling tool trade_recommendation with args {'symbol': 'GOOG'}]\n&lt;think&gt;\nOkay, so the user asked me to analyze Google's stock, which is symbol GOOG. I used the trade_recommendation tool with the symbol GOOG, and got back the response. Let me break this \ndown.\n\nFirst, the recommendation is to SELL with a MEDIUM risk level. The signal strength is -1, which is on the bearish side. The MA signal is BEARISH because the short-term moving \naverage is below the long-term one. That usually means the trend is downwards. The RSI is 39.25, which is below 50, so it's neutral but leaning bearish. The current price is around\n$88.31.\n\nHmm, I need to explain this in a user-friendly way. The user might be an investor looking to make a decision. They might want to know why the SELL recommendation, so I should \nexplain the MA crossover and RSI. Since the MA is bearish, that's a key point. RSI being neutral but in the lower range might indicate oversold, but not yet? Wait, RSI below 30 is \noversold, but 39 is still below 50, which is neutral. Maybe the user isn't familiar with these terms, so I should define them briefly.\n\nAlso, the action plan suggests reducing positions or setting trailing stops. I should mention that as a strategy. The risk level is medium, so maybe there's some caution here. The \nsignal strength is -1 out of 4.5, but I need to clarify what that scale means. Maybe the tool's documentation says that -4.5 to +4.5 is the range, with negative being bearish. The \nuser might need to know that context.\n\nI should structure the answer clearly: summary, indicators explanation, reasoning, and action steps. Make sure it's easy to follow. Also, note any assumptions, like that the \ntechnical indicators are based on current data. Maybe mention that past performance isn't indicative of future results to cover bases.\n\nWait, the RSI is 39.25, which is actually below 40. Some strategies might see that as oversold, but the tool categorized it as neutral. Maybe the tool's thresholds are different. I\nshould stick to the tool's signals as provided. The user might be confused if I add my own interpretation, so better to relay the tool's analysis.\n\nCheck if there's any missing info, like time frames for the moving averages or RSI period (which is 14 as per the data). Including that adds clarity. Also, the current price is \nprecise, so I can present that clearly.\n\nMake sure the explanation links the indicators to the recommendation. Since MA is bearish and RSI neutral, the recommendation is based on the MA trend. The action plan aligns with \na cautious approach in a bearish trend. Highlight the key points in bold or bullet points for readability. Keep it concise but informative.\n&lt;/think&gt;\n\nHere's the analysis of **GOOG (Alphabet Inc.)** based on the provided technical indicators:\n\n---\n\n### **Trading Recommendation Summary**\n- **Symbol**: GOOG  \n- **Recommendation**: **SELL**  \n- **Risk Level**: **MEDIUM**  \n- **Signal Strength**: **-1.0/4.5** (Bearish)  \n- **Current Price**: $88.31  \n\n---\n\n### **Key Technical Indicators**\n1. **Moving Averages (MA)**  \n   - **Signal**: **BEARISH**  \n   - **Reason**: Short-term MA (e.g., 50-day) is **below** the long-term MA (e.g., 200-day). This suggests a weakening upward trend or a potential downtrend.  \n   - **Recent Crossover**: No recent crossover detected (e.g., \"death cross\" or \"golden cross\").\n\n2. **RSI (14-period)**  \n   - **Value**: 39.25  \n   - **Signal**: **NEUTRAL**  \n   - **Interpretation**: The RSI is below 50, indicating bearish momentum, but not yet in oversold territory (typically &lt; 30). This suggests consolidation rather than extreme \nundervaluation.  \n\n---\n\n### **Reasoning Behind the Recommendation**\n- The **MA bearish signal** is the primary driver for the \"SELL\" recommendation. A sustained short-term MA below the long-term MA often signals a shift to a bearish trend.  \n- While the **RSI is neutral**, it supports the bearish sentiment by showing downward momentum but not overextended selling.  \n- The **signal strength of -1.0/4.5** reflects moderate bearishness, combining both MA and RSI inputs.  \n\n---\n\n### **Action Plan**  \n- **Short-Term Traders**: Consider reducing positions or booking profits if prices rebound.  \n- **Risk Management**: Set **trailing stop-losses** (e.g., 5-10% below recent support levels) to protect gains.  \n- **Monitor**: Watch for a reversal if the short-term MA crosses **above** the long-term MA or if RSI moves into oversold territory (&lt; 30).  \n\n---\n\n### **Risk Considerations**  \n- **Medium Risk**: The bearish MA crossover suggests a cautious stance, but the RSI not being oversold implies potential further downside.  \n- **Uncertainties**: External factors (e.g., earnings reports, macroeconomic trends, tech-sector news) could influence the trend unexpectedly.  \n\n---\n\n### **Next Steps**  \n- Reassess in 1\u20132 weeks for potential crossover signals or changes in momentum.  \n- Track support levels (e.g., $85\u2013$87) for possible bounce opportunities.  \n\nLet me know if you want further details or a deeper dive into specific indicators! \ud83d\udcc8\n</code></pre> </li> </ul>"},{"location":"2025/05/text-to-sql-smolagents/","title":"Text to SQL (Smolagents)","text":""},{"location":"2025/05/text-to-sql-smolagents/#guide-to-setting-up-a-text-to-sql-agent-with-smolagents","title":"Guide to Setting Up a Text to SQL Agent with <code>smolagents</code>","text":"<p>This is implemented from the guide Code</p>"},{"location":"2025/05/text-to-sql-smolagents/#output","title":"Output","text":""},{"location":"2025/05/text-to-sql-smolagents/#question-1-can-you-give-me-the-name-of-the-client-who-got-the-most-expensive-receipt","title":"Question 1: <code>Can you give me the name of the client who got the most expensive receipt?</code>","text":"<pre><code>\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 1 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n \u2500 Executing parsed code: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  query = \"SELECT customer_name FROM receipts ORDER BY (price + tip) DESC       \n  LIMIT 1;\"                                                                     \n  result = sql_engine(query)                                                    \n  # Split the result to get the customer name from the second line              \n  lines = result.split('\\n')                                                    \n  customer = lines[1].strip()                                                   \n  final_answer(customer)                                                        \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \nOut - Final answer: ('Woodrow Wilson',)\n[Step 1: Duration 86.91 seconds| Input tokens: 2,099 | Output tokens: 906]\n</code></pre>"},{"location":"2025/05/text-to-sql-smolagents/#question-2-which-waiter-got-more-total-money-from-tips","title":"Question 2: <code>Which waiter got more total money from tips?</code>","text":"<pre><code> \u2500 Executing parsed code: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  query = \"\"\"                                                                   \n  SELECT waiter_name, SUM(tip) AS total_tip                                     \n  FROM waiters                                                                  \n  JOIN receipts ON waiters.receipt_id = receipts.receipt_id                     \n  GROUP BY waiter_name                                                          \n  ORDER BY total_tip DESC;                                                      \n  \"\"\"                                                                           \n  result = sql_engine(query)                                                    \n  print(\"SQL Result:\", result)                                                  \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \nExecution logs:\nSQL Result: \n('Michael Watts', 5.669999822974205)\n('Corey Johnson', 1.2000000476837158)\n('Margaret James', 1.0)\n\nOut: None\n[Step 1: Duration 146.87 seconds| Input tokens: 2,113 | Output tokens: 923]\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Step 2 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n \u2500 Executing parsed code: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n  final_answer(\"Michael Watts\")                                                 \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \nOut - Final answer: Michael Watts\n[Step 2: Duration 21.90 seconds| Input tokens: 5,333 | Output tokens: 1,145]\n</code></pre>"},{"location":"2025/05/text-to-sql-smolagents/#tracing","title":"Tracing","text":"<p>[!NOTE]</p> <p>We will add this lightweight agent into our existing LLM job to interact with SQL databases and other tools. This will allow us to trace the execution of SQL queries and other operations, providing insights into the performance and behavior of our models.</p>"},{"location":"2025/06/mcp-transports/","title":"MCP Transports","text":""},{"location":"2025/06/mcp-transports/#mcp-transports-overview","title":"MCP Transports Overview","text":"Feature <code>stdio</code> <code>sse</code> (Server-Sent Events) <code>streamable-http</code> Bidirectional \u274c No \u274c No \u2705 Yes Protocol OS-level <code>stdin/stdout</code> over TCP HTTP (one-way stream over HTTP/1.1) HTTP (usually chunked transfer or HTTP/\u2154) Data Format Text (line-delimited), JSON JSON/Text (EventStream) Arbitrary bytes, JSON, binary, I/O streams Latency \ud83d\udfe2 Low (no network) \ud83d\udfe1 Medium (HTTP overhead, server push model) \ud83d\udfe2 Low-to-medium (depends on server impl) Streaming Support \u26a0\ufe0f Simulated (by polling stdout) \u2705 Yes (server pushes tokens as events) \u2705 Yes (true duplex stream) Transport Overhead \ud83d\udfe2 Minimal \ud83d\udfe1 Medium (HTTP headers, reconnection) \ud83d\udfe1 Higher (duplex control, buffers) Concurrency \u274c Usually single process/thread \u2705 Supported with multiple HTTP connections \u2705 Natively concurrent Error Handling \u274c Basic (process exit codes) \ud83d\udfe1 Limited (need to parse events) \u2705 Custom error/status codes possible Tooling Complexity \ud83d\udfe2 Simple subprocess \ud83d\udfe1 Moderate (SSE client lib required) \ud83d\udd34 High (custom protocol or server code needed) Server Implementation CLI app or local executable Web server with SSE support Custom backend (e.g., FastAPI with async streams) Ideal Use Case Local models or CLI tools Hosted LLMs with streaming (e.g., OpenAI) Agent/toolchain with rich, stateful interaction Transport Type IPC/OS process-based Unidirectional HTTP stream Bidirectional over HTTP Compression Support \u274c None \u26a0\ufe0f Limited (gzip encoding possible) \u2705 Full control over compression Backpressure Handling \u274c Minimal (buffer overflow risk) \u26a0\ufe0f Poor (SSE lacks flow control) \u2705 Good (can implement windowing/chunking)"},{"location":"2025/06/mcp-transports/#server-code-snippet","title":"Server Code Snippet","text":"<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Server1\")\n\n# start the server with default stdio transport\nmcp.run(transport=\"stdio\")\n\n# start the server with streamable HTTP transport\nmcp.run(transport=\"streamable-http\")\n\n# start the server with Server-Sent Events (SSE) transport\nmcp.run(transport=\"sse\") # http://127.0.0.1:8000/sse\n</code></pre>"},{"location":"2025/06/mcp-transports/#client-code-snippet-langchain-adapter","title":"Client Code Snippet (langchain adapter)","text":"<pre><code>from langchain_mcp_adapters.client import MultiServerMCPClient\n\nclient = MultiServerMCPClient(\n    {\n        \"server1\": {\n            \"command\": \"python\",\n            \"args\": [\"server1.py\"],\n            \"transport\": \"stdio\",\n        },\n        \"server2\": {\n            \"url\": \"http://localhost:8000/mcp\",\n            \"transprot\": \"streamable_http\"\n        },\n        \"server3\":{\n            \"url\": \"http://localhost:8000/sse\",\n            \"transport\": \"sse\"\n        }\n    }\n)\n</code></pre>"},{"location":"2025/07/langchain-retry-logic/","title":"LangChain Retry Logic","text":""},{"location":"2025/07/langchain-retry-logic/#langchain-invoke-retry-logic","title":"LangChain Invoke Retry Logic","text":"<p>LLM call is not stable and may fail due to network issues or other reasons, therefore, retry logic is necessary. </p> <p>Below is an example of how to implement retry logic in LangChain.</p> <p>Before implementing the retry logic, you need to install <code>tenacity</code> package which provides a flexible retry mechanism.</p> <ul> <li>tenacity</li> </ul>"},{"location":"2025/07/langchain-retry-logic/#httpx-retry-logic","title":"<code>httpx</code> Retry Logic","text":"<pre><code>from httpx import ConnectTimeout\nfrom tenacity import retry, stop_after_attempt, retry_if_exception_type\n\n@retry(retry=retry_if_exception_type(ConnectTimeout), stop=stop_after_attempt(3))\nasync def send_address_match_request(requests_client, payload):\n    response = await requests_client.post(\n        url=f\"&lt;endpoint&gt;\",\n        data=payload,\n    )\n    response.raise_for_status()\n    resp_data = response.json()\n    return resp_data \n</code></pre> <p>Note: The above code snippet is an basic retry logic implementation.</p> <ul> <li><code>stop_after_attempt(3)</code> : will retry 3 times</li> <li><code>retry_if_exception_type(ConnectTimeout)</code> : only <code>ConnectTimeout</code> exception will trigger the retry</li> </ul>"},{"location":"2025/07/langchain-retry-logic/#one-advanced-retry-logic","title":"One Advanced Retry Logic","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\n@tenacity.retry(\n    reraise=True,\n    stop=tenacity.stop_after_attempt(3),\n    wait=tenacity.wait_random(\n        min=15, max=45\n    ),  # this is mainly included to allow calming RateLimitErrors\n    before_sleep=before_sleep_log(logger, logging.INFO),\n    after=after_log(logger, logging.INFO),\n    retry=(\n        tenacity.retry_if_exception_type(APITimeoutError)\n        | tenacity.retry_if_exception_type(RateLimitError)\n        | tenacity.retry_if_exception_type(InternalServerError)\n    ),\n)\nasync def my_async_function():\n  pass\n</code></pre> <p>Note: The above code snippet is an another retry logic implementation.</p> <ul> <li><code>stop_after_attempt(3)</code> : will retry 3 times</li> <li><code>tenacity.wait_random(min=15, max=45)</code> : wait for a random time between 15 and 45 seconds before retrying</li> <li><code>before_sleep_log(logger, logging.INFO)</code> : log before retrying</li> <li><code>after_log(logger, logging.INFO)</code> : log after retrying</li> <li><code>retry()</code> : only the specified exceptions will trigger the retry</li> </ul>"},{"location":"2025/07/langchain-retry-logic/#langchain-retry","title":"LangChain Retry","text":"<pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.utils import RetryConfig\nfrom tenacity import wait_fixed, stop_after_attempt, wait_random_exponential\n\nmodel = ChatOpenAI()\n\n# Custom retry configuration\nretry_config = RetryConfig(\n    stop=stop_after_attempt(3),  # Retry up to 3 times\n    wait=wait_fixed(2),          # Wait 2 seconds between attempts\n)\n\n# Or a more complex configuration with exponential backoff\nretry_config = RetryConfig(\n    stop=stop_after_attempt(5),\n    wait=wait_random_exponential(min=1, max=10),  # Waits 1\u201310s, increasing randomly\n)\n\n\n# Apply retry config\nmodel_with_retry = model.with_retry(retry_config=retry_config)\n\n# OR: You can now use this in your chain\ntemplate = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\nchain = template | model.with_retry(retry_config=retry_config)\nresult = chain.invoke({\"topic\": \"AI\"})\n</code></pre>"},{"location":"2025/07/how-llm-tools-work/","title":"How LLM Tools work","text":""},{"location":"2025/07/how-llm-tools-work/#tools-in-large-language-models-llms","title":"Tools in Large Language Models (LLMs)","text":"<p>Tools enable large language models (LLMs) to interact with external systems, APIs, or data sources,</p> <p>extending their capabilities beyond text generation.</p> <p>Two aspects of tools are crucial:</p> <ol> <li>How to create tools</li> <li>How LLM finds and uses these tools</li> </ol>"},{"location":"2025/07/how-llm-tools-work/#create-tool","title":"Create Tool","text":"<p>Tool system is a form of metaprogramming</p> <p>Tools are defined with metadata, including</p> <ul> <li>Name: A unique identifier (e.g., get_current_weather).</li> <li>Description: A natural language explanation of what the tool does (e.g., \"Retrieve the current weather for a given city\").</li> <li>Schema: A JSON schema or similar structure specifying the input parameters (e.g., {\"city\": {\"type\": \"string\"}}).</li> </ul> <pre><code># langchain tool\n@tool\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Return current weather in a city.\"\"\"\n    ...\n</code></pre> <p>name = \"get_weather\"</p> <p>description = \"Return current weather in a city.\"</p> <p>args = {\"city\": str}</p> <p>LangChain reads the metadata (function name, docstring, type hints)</p> <p> Note: More comprehensive descriptions and schemas help LLMs understand and use tools effectively.</p>"},{"location":"2025/07/how-llm-tools-work/#tool-detection","title":"Tool Detection","text":"<ul> <li>How LLMs Detect the Required Tool?<ul> <li>Query Parsing:<ul> <li>The LLM analyzes the user\u2019s query using its natural language processing capabilities.</li> <li>It matches the query\u2019s intent and content to the tool descriptions or keywords. For example, a query like \u201cWhat\u2019s the weather in New York?\u201d aligns with a tool described as \u201cRetrieve the current weather.\u201d</li> <li>Modern LLMs, especially those fine-tuned for tool calling (e.g., OpenAI\u2019s GPT-4o), use semantic understanding to infer intent rather than relying solely on keywords.</li> </ul> </li> <li>Tool Selection:</li> <li>Prompt-Based (LangChain): The LLM is given a prompt that includes tool descriptions and instructions to select the appropriate tool. The LLM reasons about the query (often using a framework like ReAct) and outputs a decision to call a specific tool with arguments.</li> <li>Fine-Tuned Tool Calling (OpenAI): The LLM is trained to output a structured JSON object specifying the tool name and arguments directly, based on the query and tool schemas provided in the API call.</li> </ul> </li> </ul>"},{"location":"2025/07/how-llm-tools-work/#mock-tool-implementation","title":"Mock Tool Implementation","text":"<ul> <li> Step 1: Define a Tool Function <pre><code>def add_numbers(x: int, y: int) -&gt; int:\n    \"\"\"Add two numbers and return the result.\"\"\"\n    return x + y\n</code></pre></li> <li> Step 2: Use inspect to Introspect <pre><code>import inspect\n\nsig = inspect.signature(add_numbers)\n\n# Print parameter names and types\nfor name, param in sig.parameters.items():\n    print(f\"{name}: {param.annotation} (default={param.default})\")\n\n# Print return type\nprint(f\"Returns: {sig.return_annotation}\")\n</code></pre></li> <li> Step 3: Dynamically Call the Function <pre><code># Assume this comes from LLM tool calling output\nllm_output = {\n    \"x\": 5,\n    \"y\": 7\n}\n\n# Dynamically call it\nresult = add_numbers(**llm_output)\nprint(result)  # \u279c 12\n</code></pre></li> </ul>"},{"location":"2025/07/how-llm-tools-work/#summary","title":"Summary","text":"<ol> <li>Uses <code>inspect.signature</code>(func) to introspect argument names and types.</li> <li>Formats this into metadata for LLM prompt.</li> <li>Parses LLM output ({tool_name, tool_args}).</li> <li>Validates the arguments.</li> <li>Calls the function like: tool.func(**tool_args).</li> </ol>"},{"location":"2025/07/fastmcp-mcp-server-hub/","title":"FastMCP MCP Server Hub","text":""},{"location":"2025/07/fastmcp-mcp-server-hub/#mcp-server-hub","title":"MCP Server Hub","text":"<p>Currently, our different projects are using various MCP servers.  To streamline and unify the process, we plan to implement a HUB MCP server that can handle multiple MCP Servers and reuse components.</p> <ul> <li>User Interface: User can view MCP servers from <code>MCP HUB</code></li> <li>MCP Hub: Service that manages multiple MCP servers metadata</li> <li>MCP Servers: Different <code>MCP server</code> and their metadata automatically register to <code>MCP HUB</code></li> </ul> <pre><code>graph TD\n  E[User Interface] --&gt; A[MCP Hub]\n  A --&gt; B[MCP Server 1];\n  A --&gt; C[MCP Server 2];\n  A --&gt; D[MCP Server 3];\n  A --&gt; e[MCP Server 4];</code></pre>"},{"location":"2025/07/fastmcp-mcp-server-hub/#implementation","title":"Implementation","text":"<ul> <li> <p> Prerequisites:</p> <ul> <li><code>fastmcp</code></li> <li><code>fastapi</code></li> <li><code>npx @modelcontextprotocol/inspector</code> : tool to debug MCP servers.</li> </ul> </li> <li> <p> MCP Hub: FastAPI</p> <ul> <li><code>register</code> endpoint: Each MCP server will call it to register metadata into this MCP Hub</li> <li><code>servers</code> endpoint: list all MCP Servers' detail</li> </ul> </li> </ul> <pre><code>app = FastAPI(title=\"MCP HUB\", lifespan=lifespan)\n_TAG=\"MCP-HUB\"\n\ndef get_mcp_hub(request:Request) -&gt; MCPHub:\n    return request.app.state.mcp_hub\n\n\n@app.post(\"/register\", tags=[_TAG])\ndef register(server_info: Annotated[MCPServerInfo, Form()], mcp_hub=Depends(get_mcp_hub)):\n    mcp_hub.servers.append(server_info)\n    return JSONResponse(content=mcp_hub.model_dump())\n\n@app.get(\"/servers\", tags=[_TAG])\ndef get(mcp_hub=Depends(get_mcp_hub)):\n    return JSONResponse(content=mcp_hub.model_dump())\n\n\nif __name__ == '__main__':\n    import uvicorn\n    uvicorn.run(app, host=\"localhost\", port=8001)\n</code></pre> <ul> <li> MCP Server: FastMCP and FastAPI<ul> <li>Create MCP server : <code>tool</code>, <code>resources</code>, <code>prompts</code> etc.</li> <li>FastAPI server: Mount MCP server to FastAPI APP</li> </ul> </li> </ul> <pre><code>#######################\n# MCP Server Setup    #\n#######################\n\n# Create MCP server\nmcp_server = FastMCP(\"MCP Server 1\")\n\n@mcp_server.tool()\nasync def greet1():\n    return \"Hello from MCP Server 1 !\"\n\n# Create the MCP HTTP app\n\nmcp_http_app = mcp_server.http_app(_MCP_SERVER_PATH, transport=\"streamable-http\")\n\n\n\n#######################\n# FastAPI Setup       #\n#######################\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Your startup logic here\n    logger.info(\"Starting up...\")\n    # Start MCP lifespan\n    async with mcp_http_app.lifespan(app):\n        # Mount after MCP is initialized\n        app.mount(_MCP_MOUNT_PATH, mcp_http_app)\n\n        # Register FastMCP Server into MCP hub\n        mcp_server_info = MCPServerInfo(\n            name=\"MCP Server 1\",\n            transport=\"streamable-http\",\n            endpoint=f\"http://localhost:{_MCP_SERVER_PORT}{_MCP_SERVER_PATH}\"\n        )\n        async with httpx.AsyncClient() as client:\n            try:\n                resp = await client.post(f\"{MCP_HUB_URL}/register\", data=mcp_server_info.model_dump())\n                resp.raise_for_status()\n                logger.info(f\"Registered to MCP Hub\\n{resp.json()}\")\n            except Exception as e:\n                logger.error(f\"[ERROR] MCP registration failed: {e}\")\n        yield\n\n        # Your shutdown logic here\n        logger.info(\"Shutting down...\")\n\napp = FastAPI(lifespan=lifespan)\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, port=8002)\n</code></pre>"},{"location":"2025/07/fastmcp-mcp-server-hub/#mcp-hub-server","title":"MCP Hub &amp; Server","text":"<p>Once MCP server start at the background, it will register itself to the MCP hub.  The MCP hub is responsible for inquiring about available servers and managing the metadata of these servers.</p> <ul> <li> <p> MCP Hub Swagger UI </p> </li> <li> <p> MCP Server in <code>Inspector</code> </p> </li> </ul>"},{"location":"2025/07/fastmcp-mcp-server-hub/#next","title":"Next...","text":"<p>Once MCP Hub is running, we can create a client to interact with the MCP hub and choose a server from the available servers.  You can use below frameworks to connect MCP server</p> <ul> <li><code>langchain</code></li> <li><code>google adk</code></li> <li><code>fastmcp client</code></li> <li>...</li> </ul>"},{"location":"2025/08/training-llm-from-zero/","title":"Training LLM From Zero","text":"<ol> <li>Objective</li> <li>Environment Setup</li> </ol>"},{"location":"2025/08/training-llm-from-zero/#objective","title":"Objective","text":"<p>The goal of this project is to design, implement, and train a small-scale Large Language Model (LLM) from scratch,  progressing through the full training lifecycle:</p> <ol> <li>Pre-training on large-scale unlabeled text.</li> <li>Supervised Fine-Tuning (SFT) on high-quality instruction-following datasets.</li> <li>Parameter-Efficient Fine-Tuning (LoRA) for resource-efficient adaptation.</li> <li>Direct Preference Optimization (DPO) for aligning the model with human preferences.</li> </ol> <p>The project aims to serve as a practical, hands-on implementation of LLM training concepts from recent research.</p>"},{"location":"2025/08/training-llm-from-zero/#environment-setup","title":"Environment Setup","text":"<ul> <li> <p><code>macOS</code> with <code>M Series</code> chip</p> <p>\u203c\ufe0f MPS is not optimized for training </p> </li> </ul> <pre><code>    Testing on: macOS MPS device (M4, 64GB RAM)\n    PyTorch version: 2.3.0\n    MPS available: True\n\n    Matrix 1024x1024: 10.40 TFLOPS | Time: 20.65ms\n    Matrix 2048x2048: 13.45 TFLOPS | Time: 127.76ms\n    Matrix 4096x4096: 13.49 TFLOPS | Time: 1018.53ms\n    Matrix 8192x8192: 12.82 TFLOPS | Time: 8573.45ms\n    Matrix 16384x16384: 9.37 TFLOPS | Time: 93871.68ms\n</code></pre> <ul> <li><code>windows</code> with <code>CUDA</code> (recommended)</li> </ul> <pre><code>    Testing on:CUDA device (2080Ti, Memory 11G)\n    PyTorch version: 2.8.0+cu129\n    CUDA available: True\n\n    Matrix 1024x1024: 65.62 TFLOPS | Time: 3.27ms\n    Matrix 2048x2048: 634.46 TFLOPS | Time: 2.71ms\n    Matrix 4096x4096: 4447.00 TFLOPS | Time: 3.09ms\n    Matrix 8192x8192: 34163.30 TFLOPS | Time: 3.22ms\n    Matrix 16384x16384: 199933.93 TFLOPS | Time: 4.40ms\n</code></pre> <ul> <li> <p>python package for <code>CUDA</code> support</p> <ul> <li> <p><code>torch</code> PyTorch \u203c\ufe0f Be careful: CUDA version must match the PyTorch (12.6, 12.8, 12.9) </p> <p><code>pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu129</code></p> </li> <li> <p><code>transformers</code> Install</p> <ul> <li><code>pip install transformers</code></li> <li>or <code>pip3 install --no-build-isolation transformer_engine[pytorch]</code> CUDA Transformer Engine</li> </ul> </li> <li> <p><code>peft</code></p> <ul> <li><code>pip install peft</code></li> </ul> </li> </ul> </li> <li> <p>cuda toolkit</p> <ul> <li>choose the right version for your system link</li> <li>install <code>cudnn</code> from link</li> <li>check your Nvidia driver and cuda version: <code>nvidia-smi</code></li> </ul> </li> </ul>"},{"location":"2025/08/training-llm-from-zero/#dataset","title":"Dataset","text":"<ul> <li>tokenizer dataset</li> <li>pre-training dataset</li> <li>sft (Supervised Fine-Tuning) dataset</li> <li>dpo (Direct Preference Optimization) dataset</li> </ul>"},{"location":"2025/09/langchainlanggraph-qa/","title":"LangChain/LangGraph Q&A","text":""},{"location":"2025/09/langchainlanggraph-qa/#langchainlanggraph-qa","title":"LangChain/LangGraph Q&amp;A","text":"<p>Question 1:  What is the core design philosophy of <code>LangGraph</code>? Compared to traditional sequential chains (e.g., the | operator in LangChain Expression Language,     LCEL), what advantages does it offer for complex multi-step agent workflows? Please discuss from the perspectives of state management and control flow.</p> Answer <p>LangGraph\u2019s core philosophy is to orchestrate multi-step, stateful agent workflows using a directed graph (DAG or DG) structure. Each step is modeled as a node, while data and control flow are defined through edges.</p> <p>Its advantages over traditional sequential chains are evident in two areas:</p> <ul> <li>State Management: LangGraph includes a built-in mechanism for managing shared state across nodes. Each node can read and update this state,   defined explicitly in a State class and updated incrementally through PartialState. This centralized and transparent approach makes data tracking and debugging far easier. In contrast, sequential chains are essentially stateless\u2014data simply passes downstream, and any persistent state must be handled  manually, often in fragile ways.</li> <li>Control Flow: LangGraph supports rich, declarative control flow via conditional edges, enabling branching, loops, parallel execution, and even   backtracking. This allows workflows to adapt dynamically at runtime. Sequential chains, by contrast, enforce a strictly linear flow. While conditional  logic can exist within individual components, inter-component branching or looping requires external coordination.</li> </ul> <p>Question 2: What is the difference between <code>StateGraph</code> and <code>MessageGraph</code> in <code>LangGraph</code>? Please explain their respective use cases in detail, and indicate the main factors that determine which type of graph to choose.</p> Answer <p>In <code>LangGraph</code>, both <code>StateGraph</code> and <code>MessageGraph</code> are used to define workflow graphs, but they differ in how they handle state management and input/output processing.</p> <ul> <li><code>StateGraph</code><ul> <li>State Management: Uses an explicitly defined, mergeable state object as the global state of the graph. You define a State class (typically a <code>TypedDict</code> or <code>Pydantic</code> model) to represent this state. Each node receives the complete state and returns a <code>PartialState</code> to update it through merging, rather than replacement.</li> <li>Use Cases:<ul> <li>Complex agents requiring fine-grained state control, e.g., multi-turn dialogue systems tracking user intent, history, entities, and tool outputs.</li> <li>Scenarios where multiple nodes collaboratively modify the same dataset, e.g., a data processing pipeline where each step enriches or transforms a central structure.</li> <li>Debugging and observability: because state is explicit and serializable, it\u2019s easy to inspect and log before and after node execution.</li> </ul> </li> </ul> </li> <li><code>MessageGraph</code><ul> <li>State Management: A specialization of StateGraph where state is implicitly handled as a sequence of messages (<code>BaseMessage</code> objects). Each node takes a list of messages (usually the latest ones) as input and outputs new messages, which are automatically appended to the global message list.</li> <li>Use Cases:<ul> <li>Message-driven conversational agents, especially those resembling LangChain\u2019s AgentExecutor, where interaction is centered on user\u2013AI message exchange.</li> <li>Simple request\u2013response flows where only message passing is required, not a multi-dimensional state.</li> <li>Rapid prototyping: ideal for many dialogue applications since message accumulation is handled automatically.</li> </ul> </li> </ul> </li> </ul> <p>Key Selection Factors:</p> <ul> <li>State Complexity: Use StateGraph if you need to track multiple dimensions of data modified by several nodes; use MessageGraph if the workflow is mainly message passing.</li> <li>Interaction Pattern: For dialogue agents, MessageGraph is usually more natural.</li> <li>Debugging Needs: StateGraph offers clearer visibility into complex state transitions.</li> <li>Customization: StateGraph provides greater flexibility for defining arbitrary, complex state structures.</li> </ul> <p>Question 3: How does LangGraph enable workflow dynamism?</p> Answer <p>LangGraph achieves workflow dynamism through its flexible graph-construction API, allowing workflows to branch, loop, and jump based on runtime conditions.</p> <ul> <li><code>add_edge(start_node, end_node)</code><ul> <li>Role: Adds an unconditional edge from start_node to end_node. Once start_node finishes, control flow always proceeds to end_node.</li> <li>Dynamism: Provides the foundation for sequential execution. While unconditional, it is the building block for more complex dynamic </li> </ul> </li> <li><code>add_conditional_edges(start_node, condition, end_node_map)</code><ul> <li>Role: The core mechanism for runtime dynamism. Defines conditional edges from start_node.<ul> <li><code>start_node</code>: the source node.</li> <li><code>condition</code>: a callable that inspects the output or current state and returns a string (or list of strings) representing the next node(s).</li> <li><code>end_node_map</code>: a dictionary mapping condition outputs to actual target nodes.</li> </ul> </li> <li>Dynamism: Enables branching decisions at runtime, allowing the workflow to adapt based on context. This is essential for implementing agent-like logic.</li> </ul> </li> <li><code>set_entry_point(node_name)</code><ul> <li>Role: Defines the starting node of the graph. Execution begins here when the graph is run.</li> <li>Dynamism: While not dynamic by itself, it establishes the workflow\u2019s entry. In applications with multiple possible entry points, external logic or conditional routing within the graph can direct execution into different initial flows.</li> </ul> </li> <li><code>set_finish_point(node_name)</code><ul> <li>Role: Defines one or more terminal nodes. Execution halts at these nodes, and the final state is returned.</li> <li>Dynamism: Allows workflows to terminate early when certain conditions are met. Combined with conditional edges, this makes it possible to end processes dynamically within complex decision paths.</li> </ul> </li> </ul> Sample Code <pre><code>from typing import Literal\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langgraph.graph import StateGraph, END\n\nclass AgentState(TypedDict):\n    question: str\n    results: list[str]\n    answer: str\n    num_retries: int\n\ndef search(state: AgentState):\n    question = state[\"question\"] # agent question\n    num_retries = state.get(\"num_retries\", 0) # initialize to 0 if not present\n    print(f\"Searching for: {question} (retry {num_retries})\")\n\n    try:\n        # Simulate search\n        search_result = [\"Relevant info A\", \"Relevant info B\"] # todo: llm call\n    except Exception as e:\n        # If the search fails, \n        # If the retry also\n        search_result = []\n\n    return {\"results\": search_result, \"num_retries\": num_retries + 1}\n\ndef answer(state: AgentState):\n    results = state[\"results\"] # get results from search\n    question = state[\"question\"]\n    print(f\"Generating answer using results: {results}\")\n    if results:\n        # todo: transformation of results to answer or llm call etc.\n        return {\"answer\": f\"Based on {results}, the answer to '{question}' is generated.\"}\n    else:\n        # empty results\n        return {\"answer\": \"Could not find enough information.\"}\n\ndef check_search_results(state: AgentState) -&gt; Literal[\"retry_search\", \"generate_answer\", \"early_stop\"]:\n    if not state[\"results\"] and state[\"num_retries\"] &lt; 2: # retry limit\n        print(\"Search results empty, retrying...\")\n        return \"retry_search\" # retry node\n    elif not state[\"results\"] and state[\"num_retries\"] &gt;= 2:\n        print(\"Max retries reached, failing early.\") # hit retry limit\n        return \"early_stop\" # terminate node\n    else:\n        print(\"Search results found, generating answer.\")\n        return \"generate_answer\"\n\n\n# build graph\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"search\", search)\nworkflow.add_node(\"generate\", answer)\n\nworkflow.set_entry_point(\"search\")\n\n\nworkflow.add_conditional_edges(\n    \"search\",\n    check_search_results,\n    {\n        \"retry_search\": \"search\",       # loop to retry\n        \"generate_answer\": \"answer\",\n        \"early_stop\": END               # determine to stop retry (avoid infinity loop)\n    }\n)\n\nworkflow.add_edge(\"generate\", END) # end after generating successfully. Or can add another node for different task, or subgraph\n\napp = workflow.compile() # compile graph to Runnable object\n\ninitial_state = {\"question\": \"What is LangGraph?\", \"results\": [], \"answer\": \"\", \"num_retries\": 0}\nfinal_state = app.invoke(initial_state) # or ainvoke for async call\nprint(\"Final State:\", final_state)\n</code></pre> <p>Question 4: How is LangGraph\u2019s AgentExecutor implemented? How does it leverage graph-based features to simulate\u2014and even surpass\u2014the traditional ReAct (Reasoning and Acting) pattern used by LangChain agents?</p> Answer <p>LangGraph\u2019s <code>AgentExecutor</code> is implemented by constructing a specially <code>structured MessageGraph</code>. This graph faithfully simulates the reasoning\u2013acting loop of an agent, while also providing greater flexibility and robustness than the traditional ReAct approach.</p> <p>Question 5: Discuss the importance of <code>idempotency</code> in LangGraph nodes when building robust agents. In what situations should nodes be designed to be idempotent, and how can idempotency be achieved?</p> Answer <p>In LangGraph, <code>idempotency</code> means that executing the same node multiple times with the same initial state and input produces identical results and side effects as executing it once. Idempotency is crucial for building robust agents.</p> <p>Why It Matters</p> <ul> <li>Fault tolerance: In distributed or unstable environments, external service calls may fail or timeout. If a node is idempotent, the agent can safely retry it without risking inconsistencies or unintended side effects (e.g., duplicate charges or duplicate notifications).</li> <li>Recoverability: If execution is interrupted (e.g., by a crash or restart), the agent can resume from the last successful idempotent node without repeating costly or side-effect-prone operations.</li> <li>Debuggability: Idempotent nodes make testing easier. The same test case can be rerun with the expectation of identical results, simplifying issue isolation.</li> <li>State consistency: Ensures global state remains consistent even if retries or concurrent executions cause multiple invocations of a node.</li> <li>Traceability and logging: Logs of idempotent operations are cleaner, reflecting only final outcomes rather than inconsistent intermediate states.</li> </ul> <p>When Nodes Should Be Idempotent</p> <p>Any node that interacts with the external world, performs expensive computations, or risks failure should ideally be idempotent.</p> <p>Common cases include:</p> <ul> <li>External API calls: Payments, sending emails, resource creation, database updates.</li> <li>File operations: Writing (with identical content), deleting files.</li> <li>Expensive deterministic computations: Safe to repeat if results are deterministic and side-effect free.</li> <li>Cache interactions: Typically idempotent by nature.</li> <li>State transitions: Logic should be deterministic so that given the same input, repeated calls converge to the same state.</li> </ul> <p> How to Achieve Idempotency</p> <ol> <li>Idempotency keys (unique identifiers):<ul> <li>The most common approach. The client generates a unique request ID and passes it to the external service. The service checks if the request was already processed; if so, it returns the previous result without re-execution.</li> <li>In LangGraph: Maintain a unique operation ID in AgentState and pass it to nodes calling external services.</li> </ul> </li> <li>Atomic operations:<ul> <li>Bundle multiple steps into a single transaction\u2014either all succeed or all fail. Database transactions are a typical example.</li> <li>In LangGraph: Ensure multi-step logic inside a node is transactional (complete-or-rollback).</li> </ul> </li> <li>Conditional updates:<ul> <li>Only update data if preconditions are met (e.g., optimistic locking with version checks).</li> <li>In LangGraph: Validate state before modifying it or calling an external service to avoid redundant updates.</li> </ul> </li> <li>Read-only operations:<ul> <li>Nodes that only read data and cause no side effects are inherently idempotent.</li> <li>In LangGraph: Design query/search nodes to be read-only.</li> </ul> </li> <li>Result caching:<ul> <li>Cache computation results or external call outputs. If inputs are identical, return the cached result.</li> <li>In LangGraph: Implement in-node caching or integrate with external caching systems.</li> </ul> </li> </ol> <p>Question 6: How does LangGraph handle concurrent execution? Explain Async Nodes and Parallel Edges, their roles in improving agent throughput and responsiveness, and design limitations/considerations.</p> Answer <p>LangGraph supports concurrency via <code>asynchronous nodes</code> and <code>parallel edges</code>, which together substantially improve agent efficiency and responsiveness.</p> <p> Async nodes</p> <ul> <li>Role: Nodes may be implemented as <code>async functions</code>. When invoked, an async node returns an awaitable; the runtime awaits it without blocking the event loop, allowing other tasks to run while waiting for I/O (e.g., external API calls or DB access).</li> <li>Benefits:<ul> <li>Non-blocking I/O: Long I/O waits don\u2019t stall the whole graph.</li> <li>Better resource utilization: In <code>asyncio-based servers</code>, async nodes let the system handle other requests while awaiting responses, increasing throughput.</li> </ul> </li> <li>How to use: Define node handlers with <code>async def</code>; LangGraph detects and calls them appropriately.</li> </ul> <p> Parallel edges</p> <ul> <li>Role: A node can emit outputs to multiple downstream edges (fan-out pattern) so downstream nodes run concurrently. LangGraph can run these active branches in parallel and then wait for all to complete (often via an aggregation/merge node).</li> <li>Benefits:<ul> <li>Task parallelism: Independent subtasks (e.g., calling two different tools) run simultaneously, reducing end-to-end latency.</li> <li>Fan-out / fan-in patterns: One node can fan out multiple parallel jobs; a later node can aggregate results.</li> </ul> </li> <li>How to implement:</li> </ul> Code <pre><code>from langgraph.graph import StateGraph, END, START\nfrom typing import TypedDict, Annotated, List\nfrom operator import add\nimport time\n\n\nclass State(TypedDict):\n    input: str\n    results: Annotated[List[str], add]\n\n\ndef dispatcher(state: State):\n    \"\"\"Fan-out: Dispatch work to parallel workers\"\"\"\n    print(f\"\ud83d\udce4 Dispatching: {state['input']}\")\n    return state\n\n\ndef worker_1(state: State):\n    \"\"\"Worker 1: Process task\"\"\"\n    print(\"  \u2699\ufe0f  Worker 1 processing...\")\n    time.sleep(0.5)\n    return {\"results\": [\"Worker 1: Analyzed sentiment\"]}\n\n\ndef worker_2(state: State):\n    \"\"\"Worker 2: Process task\"\"\"\n    print(\"  \u2699\ufe0f  Worker 2 processing...\")\n    time.sleep(0.3)\n    return {\"results\": [\"Worker 2: Extracted keywords\"]}\n\n\ndef worker_3(state: State):\n    \"\"\"Worker 3: Process task\"\"\"\n    print(\"  \u2699\ufe0f  Worker 3 processing...\")\n    time.sleep(0.4)\n    return {\"results\": [\"Worker 3: Generated summary\"]}\n\n\ndef aggregator(state: State):\n    \"\"\"Fan-in: Aggregate results from all workers\"\"\"\n    print(f\"\ud83d\udce5 Aggregating {len(state['results'])} results:\")\n    for result in state['results']:\n        print(f\"   \u2705 {result}\")\n    return state\n\n\n# Build the graph\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"dispatcher\", dispatcher)\nworkflow.add_node(\"worker_1\", worker_1)\nworkflow.add_node(\"worker_2\", worker_2)\nworkflow.add_node(\"worker_3\", worker_3)\nworkflow.add_node(\"aggregator\", aggregator)\n\n# Fan-out: dispatcher -&gt; all workers (parallel)\nworkflow.add_edge(START, \"dispatcher\")\nworkflow.add_edge(\"dispatcher\", \"worker_1\")\nworkflow.add_edge(\"dispatcher\", \"worker_2\")\nworkflow.add_edge(\"dispatcher\", \"worker_3\")\n\n# Fan-in: all workers -&gt; aggregator (using list syntax)\nworkflow.add_edge([\"worker_1\", \"worker_2\", \"worker_3\"], \"aggregator\")\n\nworkflow.add_edge(\"aggregator\", END)\n\n# Compile and run\ngraph = workflow.compile()\n\nprint(\"=\"*60)\nprint(\"FAN-OUT AND FAN-IN PATTERN\")\nprint(\"=\"*60)\nprint(\"\\nGraph: dispatcher -&gt; [worker_1, worker_2, worker_3] -&gt; aggregator\\n\")\n\nresult = graph.invoke({\n    \"input\": \"Process this data\",\n    \"results\": []\n})\n\nprint(f\"\\n\u2728 Done! Processed {len(result['results'])} tasks in parallel\")\n</code></pre> Graph <pre><code>    graph TD\n    START([START]) --&gt; dispatcher[Dispatcher&lt;br/&gt;Fan-Out]\n\n    dispatcher --&gt; worker_1[Worker 1&lt;br/&gt;Sentiment Analysis]\n    dispatcher --&gt; worker_2[Worker 2&lt;br/&gt;Keyword Extraction]\n    dispatcher --&gt; worker_3[Worker 3&lt;br/&gt;Summarization]\n\n    worker_1 --&gt; aggregator[Aggregator&lt;br/&gt;Fan-In]\n    worker_2 --&gt; aggregator\n    worker_3 --&gt; aggregator\n\n    aggregator --&gt; END([END])\n\n    style START fill:#90EE90\n    style END fill:#FFB6C1\n    style dispatcher fill:#87CEEB\n    style worker_1 fill:#DDA0DD\n    style worker_2 fill:#DDA0DD\n    style worker_3 fill:#DDA0DD\n    style aggregator fill:#F0E68C</code></pre> <p> limitations &amp; considerations</p> <ul> <li>Race conditions &amp; state merging:<ul> <li>When parallel branches modify the same state keys, conflicts can occur. In a <code>StateGraph</code>, <code>LangGraph</code> merges returned <code>PartialStates</code> (commonly <code>last-write-wins</code>or using custom merge logic defined on the State type). In a <code>MessageGraph</code>, messages are appended (so less state conflict) but ordering may be <code>nondeterministic</code>.</li> <li>Mitigation: Design parallel branches to write independent keys, or use an explicit aggregation node to reconcile and merge results deterministically.</li> </ul> </li> <li>Error handling:<ul> <li>If a parallel branch fails, you must decide whether to abort the whole graph or let other branches continue and handle failures later. <code>LangGraph</code> propagates exceptions; build <code>try/except</code> wrappers or dedicated error-handling nodes and conditional edges to manage partial failures.</li> </ul> </li> <li>Increased complexity:<ul> <li>Async and parallel flows make execution paths harder to reason about and debug. Good node responsibility separation, structured logging, and observability are critical.</li> </ul> </li> <li>Resource consumption:<ul> <li>Parallel tasks consume more CPU, memory, and network resources. Excessive parallelism can degrade performance or exhaust limits. Tune parallelism to available resources.</li> </ul> </li> <li>Determinism / ordering:<ul> <li>Parallel execution may produce nondeterministic ordering of results. If downstream nodes depend on a specific order, enforce ordering at an aggregation step or serialize the dependent parts.</li> </ul> </li> </ul> <p>Async nodes (non-blocking I/O) and parallel edges (concurrent branches) are powerful for speeding up LangGraph workflows, but they require careful attention to state merging, error handling, resource limits, and determinism. Proper design patterns\u2014independent state keys, aggregation nodes, clear error nodes, and bounded parallelism\u2014help avoid common concurrency pitfalls.</p> <p>Question 7: What is the significance of LangGraph\u2019s persistence mechanism for building long-running agents? Please explain different persistence strategies (e.g., in-memory, Redis, custom storage) and their use cases.</p> Answer <p>Persistence in <code>LangGraph</code> is crucial for building long-running agents \u2014 those that run for hours, days, or longer, or must maintain state across multiple sessions. Examples include advanced customer service bots, automated workflows, and long-term project management agents.</p> <p>Why persistence matters:</p> <ul> <li>Fault tolerance &amp; recovery: Agents can resume execution from the last checkpoint after crashes, restarts, or network failures instead of starting over.</li> <li>State save/load: The current state can be persisted to disk or a database and later restored, enabling agents to continue work across time or machines.</li> <li>Multi-user / multi-session support: Each user session can have its own persisted state, supporting concurrent usage.</li> <li>Debugging &amp; auditing: Historical checkpoints allow replaying and analyzing agent behavior.</li> <li>Compliance: Persistence enables recording of state transitions for audit and regulatory requirements.</li> </ul> <p>Persistence strategies</p> <ul> <li>In-memory (MemorySaver / InMemoryCheckpointSaver):<ul> <li>Features: Simplest, zero config, volatile (lost on restart).</li> <li>Use cases: Single-machine deployments, lightweight persistence, small-scale production or testing.</li> <li>Limitations: Not suited for high-concurrency writes or distributed setups.</li> </ul> </li> <li>Redis (RedisSaver):<ul> <li>Features: Uses Redis as backend; high-performance in-memory KV store.</li> <li>Use cases: Multi-process/multi-instance agents, medium-scale concurrent workloads, fast recovery.</li> <li>Limitations: Requires Redis service; not ideal for very large-scale or strong consistency needs.</li> </ul> </li> <li>SQLite (SQLiteSaver):<ul> <li>Features: Uses embedded SQLite DB; lightweight, no server needed.</li> <li>Use cases: Single-machine deployments, lightweight persistence, small-scale production or testing.</li> <li>Limitations: Not suited for high-concurrency writes or distributed setups.</li> </ul> </li> <li>Custom storage (Custom CheckpointSaver):<ul> <li>Features: Implement CheckpointSaver interface to integrate with any backend (Postgres/MySQL, MongoDB, Cassandra, cloud storage like S3/GCS).</li> <li>Use cases: Enterprise applications, integration with existing infra, high availability, scalability, or custom business/audit needs.</li> <li>Limitations: Requires custom implementation and careful design.</li> </ul> </li> </ul> <p>Challenges with persistence</p> <ul> <li>Version compatibility:<ul> <li>Problem: Changes in graph structure or State schema may break old checkpoints.</li> <li>Solutions: Include versioning in state, implement migrations, favor backward-compatible changes, use flexible serialization formats (e.g., JSON, Avro).</li> </ul> </li> <li>Concurrent updates:<ul> <li>Problem: Multiple instances/parallel nodes may update the same session state, leading to race conditions.</li> <li>Solutions: Optimistic locking (version/timestamp checks), pessimistic locking (with trade-offs), transactional updates, or custom merge strategies. LangGraph\u2019s StateGraph supports merging (often last-write-wins), but careful design is needed (e.g., whether lists append or overwrite, dicts merge or replace).</li> </ul> </li> <li>Data growth &amp; performance:<ul> <li>Problem: Long-running states can grow large, slowing read/write.</li> <li>Solutions: Store only essential info, use incremental updates, pick performant backends, shard large datasets.</li> </ul> </li> <li>Security &amp; privacy:<ul> <li>Problem: Persisted states may contain sensitive data.</li> <li>Solutions: Encrypt storage, enforce access controls, anonymize/mask unnecessary sensitive fields.</li> </ul> </li> </ul> <p>Question 8: What observability features does LangGraph provide?</p> Answer <p><code>LangGraph</code> offers strong built-in observability features, which are essential for debugging, monitoring, and understanding complex agent behavior. These are primarily enabled through the stream interface, the events system, the channels state model, and deep integration with LangSmith.</p> <ul> <li>Stream Interface<ul> <li>What it is: The app.stream() method produces a generator that yields real-time state updates and node outputs during execution.</li> <li>Role in observability:<ul> <li>Live progress tracking: See which node is running and what it returns, in real time.</li> <li>Step-by-step debugging: Inspect state changes and intermediate results without halting execution.</li> <li>Reactive UIs: Stream outputs can be pushed to a frontend to show live agent responses and reasoning.</li> </ul> </li> </ul> </li> <li>Events System<ul> <li>What it is: <code>LangGraph</code> emits events such as node <code>start/finish</code>, <code>state updates</code>, and <code>errors</code>. These can be captured by <code>LangSmith</code> or <code>custom listeners</code>.</li> <li>Role in observability:<ul> <li>Fine-grained tracing: Provides lower-level detail than streams, including node inputs, outputs, and timing.</li> <li>Performance profiling: Measure execution duration per node and identify bottlenecks.</li> <li>Behavior analysis: Understand decision-making, e.g., why an agent chose a branch or tool.</li> </ul> </li> </ul> </li> <li>Channels<ul> <li>What they are: Channels are the mechanism LangGraph uses to manage and propagate state. Each channel holds a type of data (e.g., messages, input, output), which nodes read from and write to.</li> <li>Role in observability:<ul> <li>Atomic state updates: Each node\u2019s writes to channels are isolated.</li> <li>Debugging state flow: Inspect channel contents before and after nodes to track how state evolves.</li> <li>Concurrency insights: In parallel execution, channel merge rules (e.g., <code>LastValue</code>, <code>BinaryOperator</code>) determine conflict resolution\u2014understanding these is key to debugging concurrency.</li> </ul> </li> </ul> </li> <li>LangSmith Integration<ul> <li>What it is: <code>LangSmith</code>, part of the <code>LangChain</code> ecosystem, is a platform for debugging, monitoring, and evaluating LLM apps. LangGraph integrates deeply with it.</li> <li>Role in observability:<ul> <li>Visual execution traces</li> <li>Comprehensive logs</li> <li>Diagnostics</li> <li>Performance metrics</li> <li>Evaluation &amp; A/B testing</li> <li>Collaboration</li> </ul> </li> </ul> </li> </ul> <p>Question 9: How do Prebuilt ToolNodes (e.g., <code>ToolNode</code>, <code>ToolsAgentOutputNode</code>) in LangGraph simplify tool usage, and how do they internally coordinate with the logic of <code>AgentExecutor</code>?</p> Answer <p>Prebuilt ToolNodes in LangGraph \u2014 particularly <code>ToolNode</code> and <code>ToolsAgentOutputNode</code> \u2014 are designed to greatly simplify how agents use and manage tools. They encapsulate the underlying tool-calling logic, allowing developers to integrate tools into the graph in a more declarative and modular way.</p> <ul> <li> <p>ToolNode</p> <ul> <li>Purpose: A general-purpose node for executing one or more tool calls. It takes ToolCall objects (usually produced by the LLM) as input, executes the corresponding tools, and returns the results as ToolMessages.</li> <li>How it simplifies tool usage:<ul> <li>Automated tool execution: No need to manually parse ToolCalls from the LLM output and invoke the corresponding Python functions\u2014the ToolNode handles this automatically.</li> <li>Unified interface: Whether the tool is a simple <code>Python</code> function or a complex <code>LangChain Tool object</code>, <code>ToolNode</code> provides a consistent execution layer.</li> <li>Error handling: Built-in logic can catch execution errors and package them into a <code>ToolMessage</code> so that the LLM can react accordingly.</li> </ul> </li> <li> <p>Coordination with AgentExecutor:</p> <p>Inside an AgentExecutor, the tools node is effectively an instance of ToolNode. When the agent node (LLM) outputs ToolCalls, conditional edges route them to ToolNode. After execution, the results (ToolMessages) are added back into the agent state and passed to the agent node for the next reasoning step.</p> </li> </ul> </li> <li> <p>ToolsAgentOutputNode</p> <ul> <li>Purpose: A specialized node (or, more generally, <code>AgentOutputNode</code>) in <code>MessageGraph</code> that inspects the LLM output (AIMessage) and decides the next step\u2014whether to execute a tool or end the process.</li> <li>How it simplifies tool usage:<ul> <li>Automated decision logic: No need to write custom branching logic to distinguish between tool calls and final answers. <code>ToolsAgentOutputNode</code> performs this check and routes execution accordingly.</li> <li>Integration with ToolNode: If the output includes <code>tool_calls</code>, they are extracted and sent via edges to the <code>ToolNode</code> for execution.</li> </ul> </li> <li> <p>Coordination with AgentExecutor:</p> <p>Within the core loop of AgentExecutor:</p> <pre><code>1.  The agent node (LLM) produces output.\n2.  ToolsAgentOutputNode inspects it:\n    - If the output is an AIMessage:\n    - Tool calls? Extract and route to ToolNode.\n    - Final answer? Route execution to END.\n3.  Tool results are fed back to the agent node for further reasoning (classic ReAct loop).\n</code></pre> </li> </ul> </li> <li> <p>Collaboration Flow Summary</p> <p>The core loop of AgentExecutor alternates between:</p> <pre><code>- Agent node (LLM): Produces reasoning steps and tool calls.\n- Decision node (ToolsAgentOutputNode): Decides whether to call tools or terminate.\n- Tool node (ToolNode): Executes tools and feeds results back into the loop.\n</code></pre> </li> </ul> <p>Question 10: What is the concept of a <code>CompiledGraph</code> in <code>LangGraph</code>, and why is it important? What optimizations does the compilation process perform, and why is compilation considered a critical step for building production-grade agents?</p> Answer <p>In <code>LangGraph</code>, a <code>CompiledGraph</code> (more precisely, an instance of CompiledGraph, typically a subclass of <code>RunnableWithMessage</code> such as <code>CompiledStateGraph</code>) represents an optimized and fully prepared runtime version of a graph. It is created by calling <code>.compile()</code> on a <code>StateGraph</code> or <code>MessageGraph</code>.</p> <ul> <li> <p>Concept of CompiledGraph: A CompiledGraph does not compile Python code into machine code. Instead, it transforms the declarative graph definition (nodes and edges) into an executable, optimized runtime representation. This representation usually includes</p> <ul> <li>Execution ordering: Precomputes possible execution paths and node dependencies from the graph topology.</li> <li>State manager initialization: Configures persistence (if enabled) and state merge logic.</li> <li>Node function wrapping: Wraps raw node functions into a uniform interface that handles type conversion, validation, and I/O.</li> <li>Optimized internal data structures: Uses efficient data structures for fast node and edge lookups.</li> </ul> </li> <li> <p>CompiledGraph is essential for production-grade agents because it delivers:</p> <ul> <li>Performance optimization<ul> <li>Reduced runtime overhead: Precomputes setup and parsing so each invoke or stream call avoids repeated initialization.</li> <li>Faster path resolution: Optimizes the graph so the runtime can quickly determine the next node, even with complex branching.</li> </ul> </li> <li>Error detection<ul> <li>Graph validation: Ensures the graph is structurally sound \u2014 no isolated or unreachable nodes, no invalid entry/exit points, and no unintended cyclic dependencies (unless explicitly allowed).</li> <li>Type checking: Where type hints (e.g., TypedDict) are used, performs basic type consistency checks.</li> </ul> </li> <li>Robustness and stability<ul> <li>Deterministic execution: Guarantees consistent outputs for the same input/state (except when randomness is intentionally introduced).</li> <li>Production readiness: Produces a validated, self-contained runtime unit suitable for deployment.</li> </ul> </li> <li>Integration and deployment ease<ul> <li>A compiled graph is a Runnable object, meaning it can be combined with other Runnables in LangChain and supports interfaces like invoke, batch, stream, ainvoke, abatch, and astream.</li> <li>Supports serialization/deserialization (if enabled), allowing agents to be stored, shared, and reloaded as configuration files.</li> </ul> </li> </ul> </li> <li>What Optimizations Are Done During Compilation?<ul> <li>Topological sorting &amp; path caching (for DAGs): Precomputes node execution orders or possible paths to reduce runtime lookup costs.</li> <li>Node and edge indexing: Maps names to efficient internal structures (e.g., hash tables) for quick lookup/navigation.</li> <li>Channel &amp; state manager setup: Initializes channels and persistence backends (checkpointers) to manage data flow/state.</li> <li>Validation and normalization:<ul> <li>Ensures all nodes are defined.</li> <li>Verifies conditional edges cover possible outcomes.</li> <li>Detects unreachable nodes or unintended dead loops.</li> <li>Validates entry and finish points.</li> </ul> </li> <li>Function wrapping: Standardizes node functions to handle serialization, deserialization, state updates, and error handling consistently.</li> </ul> </li> </ul> <p>Question 11: How does LangGraph implement \u201cmemory\u201d in its graphs? How the channels mechanism of StateGraph (e.g., LastValue, BinaryOperatorAggregate, Tuple, Set) works together to maintain complex state, and discuss the use cases for each channel type.</p> Answer <p>The core mechanism behind LangGraph\u2019s graph \u201cmemory\u201d is the <code>channels system</code> in <code>StateGraph</code>. Each <code>StateGraph</code>instance maintains a set of channels\u2014independent data streams that store and propagate different parts of the global state.</p> <p>When a node executes, it reads the current channel values, computes an update, and returns a <code>PartialState</code>. This update is written into the corresponding channels, and the runtime automatically merges it into the global state according to each channel\u2019s <code>merge policy</code>.</p> <p> How channels work</p> <ol> <li>State definition: You first define a <code>TypedDict</code> or <code>Pydantic</code> model (e.g., <code>AgentState</code>) to declare all state keys and their types.</li> <li>Channel creation: For each key, <code>LangGraph</code> creates a corresponding channel with a <code>predefined</code> or <code>custom merge</code> strategy (reducer).</li> <li>Node operation: A node receives the full <code>AgentState</code> and outputs a <code>PartialState</code> (dict) with the keys it wants to update.</li> <li>State merging: The runtime merges each updated key into its channel using that channel\u2019s <code>reducer</code>, producing the new global state.</li> </ol> <p> Built-in channel types and use cases</p> <ol> <li><code>LastValue</code> (<code>default</code> or via <code>Annotated</code>)<ul> <li>Strategy: Replace the old value with the new one.</li> <li>Use cases:<ul> <li>Single, non-accumulative variables (e.g., boolean flags, counters, final answers, current user intent).</li> <li>Situations where each update invalidates the previous value.</li> </ul> </li> </ul> </li> <li> <p><code>BinaryOperatorAggregate</code> (e.g., <code>operator.add</code>, <code>operator.mul</code>, or <code>custom</code>)</p> <ul> <li>Strategy: Merge old and new values with a binary operator. Most commonly operator.add for list accumulation.</li> <li>Use cases:<ul> <li>Lists/strings accumulation: e.g., message histories (messages: Annotated[list[BaseMessage], operator.add]).</li> <li>Numeric aggregation: e.g., summing values.</li> <li>Custom logic: Any binary function for specialized aggregation.</li> </ul> </li> </ul> </li> <li> <p><code>Tuple</code> (internal, rarely user-defined)</p> <ul> <li>Strategy: A node can update multiple channels simultaneously. Each key in the returned dict is written independently.</li> <li>Use cases:<ul> <li>Nodes producing multidimensional outputs (e.g., both search_results and source_urls).</li> <li>Parallel updates to different state variables.</li> </ul> </li> <li>Users don\u2019t define Tuple explicitly; it emerges when returning multi-key PartialState dicts.</li> </ul> </li> <li><code>Set</code> (via <code>operator.or_</code> or <code>custom</code>)<ul> <li>Strategy: Merge old and new values using set union.</li> <li>Use cases:<ul> <li>Collecting unique items (e.g., visited URLs, tool names used, deduplicated entities).</li> </ul> </li> </ul> </li> </ol> <p>Why channels are \u201cmemory\u201d</p> <p>Channels form the backbone of LangGraph\u2019s state management, functioning as the agent\u2019s \u201cmemory\u201d:     - Persistence: With a Checkpointer, all channel states can be stored externally, enabling agents to carry memory across sessions.     - Isolation + merging: Each channel manages a slice of state independently, while merge policies ensure predictable resolution of updates from different nodes (even concurrent ones).     - Traceability: Incremental updates via channels allow state history tracking, enabling rollback or versioning in theory.     - Concurrency safety: The merge logic ensures atomic updates under async/parallel execution, preventing data loss or inconsistency.</p> <p>Question 12: What are the fundamental differences between LangGraph\u2019s AgentExecutor and the traditional LangChain AgentExecutor? </p> Answer <p>LangGraph\u2019s AgentExecutor and the traditional LangChain AgentExecutor (e.g., the one created by initialize_agent) differ fundamentally in their design philosophy and implementation.</p> LangChain AgentExecutor LangGraph AgentExecutor Architecture <ul><li>Fixed loop: Internally, it typically uses a hard-coded <code>ReAct-style loop: LLM (Think) \u2192 Tool (Act) \u2192 LLM (Observe) \u2192 Tool (Act)\u2026</code> until the LLM decides to produce a final answer.</li> <li>Single-chain abstraction: Although internally complex, from the outside it appears as a <code>single Runnable</code> instance wrapping all logic.</li> <li>Implicit state management: State (like conversation history or tool outputs) is passed around as temporary context, with no explicit, externally accessible global state object.</li></ul> <ul><li>Graph structure: The core is an explicit, programmable directed graph. Each reasoning step, tool call, and decision point is a node, with data and control flow clearly defined by edges.</li><li>Transparent and customizable flow: The entire agent logic (including the ReAct loop) is exposed as a graph, so developers can see and modify any node or edge.</li> <li>Explicit state management: Uses StateGraph or MessageGraph to manage a well-defined, serializable global state object, which every node reads and writes.</li> </ul> Customizability <ul><li>Limited customization: You can swap in custom LLMs, tools, or prompts, but the core ReAct loop is fixed. It\u2019s hard to change the decision flow (e.g., adding retry logic after tool failures or switching to a non-ReAct path under special conditions).</li> <li>Extension model: Typically extended by writing new tools or tweaking LLM outputs.</li></ul> <ul><li>Extremely customizable: Because it\u2019s a graph, developers have full control over the agent\u2019s process. They can freely <code>add</code>, <code>remove</code>, or <code>modify</code> nodes, define <code>complex conditional branches</code>, <code>loops</code>, <code>parallel executions</code>, or even <code>subgraphs</code>.</li><li>Flexible control flow: Enables logic far beyond ReAct, such as:<ul><li>Multi-step tool interactions with retries.</li><li>Dynamically choosing the next tool or producing an answer directly.</li><li>Integrating external information sources into decision-making.</li><li>Introducing human-in-the-loop nodes for error handling.</li></ul></li><li>Modular design: Each node is an independent function, making it easy to reuse, test, and compose larger agents from smaller subgraphs.</li></ul> Performance <ul><li>Fixed overhead: Every run follows the same flow, even if the task is simple, leading to unnecessary checks or steps.</li><li>Synchronous execution: By default, execution is sequential, so multiple tool calls cannot run in parallel.</li></ul> <ul><li>Optimizable graph execution: The compiled graph can be structurally optimized, reducing runtime overhead.</li><li>Async and parallel support: Built-in support for asynchronous nodes and parallel edges enables significant efficiency gains, especially for independent tool calls.</li><li>Fine-grained control: Developers can decide precisely which steps to run concurrently to maximize throughput.</li></ul> Robustness <ul><li>Limited error handling: Relies mostly on <code>Python</code> exceptions. If LLM output parsing fails or a tool raises an error, the agent may crash or deadlock.</li><li>Opaque state: Internal state is not transparent, making it hard to debug or resume execution.</li></ul> <ul><li>Explicit error handling: Error handling can be modeled directly in the graph\u2014for example, branching to a retry node on tool failure, logging the error, or alerting a user.</li><li>Persistence and recovery: With the checkpointer mechanism, state can be persisted to external storage, allowing the agent to resume after crashes\u2014crucial for long-lived agents.</li><li>Observability: Deep integration with LangSmith plus streaming and events support provides exceptional observability for debugging and understanding agent behavior.</li><li>Determinism (via compilation): The graph is validated during compilation, reducing runtime errors.</li></ul> <p> Summary</p> <ul> <li>The traditional LangChain AgentExecutor is more like a black box\u2014a ready-made ReAct implementation suited for quick prototyping or cases where deep customization isn\u2019t needed.</li> <li>The LangGraph AgentExecutor is a white box\u2014a powerful, flexible, programmable graph framework that exposes the agent\u2019s logic as editable structure. It enables building highly customized, complex, performant, and robust production-grade agents.</li> </ul> <p>Question 13: Discuss the potential of LangGraph in building <code>autonomous agents</code>. How can LangGraph\u2019s graph-based design be leveraged to model and support <code>planning</code>, <code>reflection</code>, <code>self-correction</code>, and <code>continual learning</code>? </p> Answer LangGraph Support How Support Planning <ul><li>Multi-step decision making: Planning can be modeled as a sequence of decision and execution nodes. For example, an initial planner node receives a task, then conditional edges determine which subtasks or tools should be executed.</li><li>Hierarchical planning: Subgraphs can represent different levels of planning. A top-level graph manages goal-setting and task decomposition, while subgraphs handle detailed execution.</li><li>State-driven planning: Planning nodes can access the global state (e.g., completed tasks, available resources) and dynamically adjust the next steps.</li></ul> <ul><li>Conditional edges: Route the flow to different execution paths depending on identified subtasks.</li><li>Node specialization: A \u201cplanner\u201d node (LLM-driven) analyzes problems and proposes steps, while \u201cexecutor\u201d nodes carry them out.</li></ul> Reflection <ul><li>Dedicated reflection nodes: Triggered after a task or sequence of tasks completes.</li><li>Result evaluation: Reflection nodes can take execution outputs, current state, and original goals, then use an LLM to assess outcomes.</li><li>Feedback output: May yield evaluations (success/failure, improvements needed), revised plans, or corrective instructions.</li></ul> <ul><li>Loops: Control flow can loop from task execution into a reflection node, and from there back to planning or execution.</li><li>Error-handling branches: If reflection detects problems, it can trigger self-correction or error-recovery branches.</li></ul> Self-Correction <ul><li>Reflection-driven corrections: When reflection finds issues, it can suggest fixes (e.g., changing parameters, retrying tools, adjusting queries).</li><li>Dynamic path adjustment: These suggestions can flow back into prior planner/executor nodes, or trigger a dedicated correction node.</li><li>Bounded retries: Counters and conditional edges can enforce retry limits.</li></ul> <ul><li>Conditional edges and loops: Core to retrying or rerouting execution.</li><li>State updates: Correction nodes can update the agent\u2019s state (e.g., increment retry count, adjust strategy parameters, mark failures).</li></ul> Continual Learning/Adaptation <ul><li>Experience accumulation: While not a learning framework, LangGraph\u2019s state management allows accumulation of \u201cexperience\u201d (e.g., successful tool calls, common error patterns with fixes).</li><li>Decision refinement: LLM-driven nodes can use this accumulated experience in future reasoning \u2014 akin to prompt-based context learning.</li><li>External knowledge integration: Agents can write insights into external databases or vector stores for future retrieval.</li></ul> <ul><li>State as knowledge store: Patterns and key info can persist in state.</li><li>Tool nodes for persistence: Dedicated nodes can store reflection/correction insights into external systems.</li><li>Knowledge-driven decisions: Before making decisions, LLM nodes can query these external stores for relevant info.</li></ul> <p>Conclusion</p> <p>Question 14: In LangGraph, how can we effectively manage and update prompt engineering for long-running agents? Discuss the pros and cons of treating prompts as part of the graph state versus using external prompt template systems</p> Answer <p>Effectively managing and updating prompt engineering in LangGraph is crucial since prompts are the core driver of LLM behavior. There are several strategies, each with advantages and trade-offs.</p> Prompts as Part of Graph State External Prompt Template Systems Implementation Store the prompt string or template directly as a field in AgentState. Nodes generate prompts by reading from this state. <ul><li>LangChain Hub: Store templates remotely (lc://prompts/...) and load by reference.</li><li>Local files: Save templates in .txt, .yaml, or .json files and load at runtime.</li><li>Databases/config services: Centralize prompt storage for retrieval during execution.</li></ul> Advantages <ul><li>Dynamic adaptability: Agents can update or optimize prompts at runtime (e.g., a reflection node modifies prompts based on execution results).</li><li>Context-aware behavior: Prompts can adapt to user preferences, context, or task stages for more intelligent responses.</li><li>State tracking: Prompt changes are persisted as part of agent state, useful for auditing and debugging.</li></ul> <ul><li>Version control: Git or LangChain Hub provides strong versioning, rollback, and comparison.</li><li>Decoupling: Prompts are separate from code, reducing redeployment overhead.</li><li>Collaboration: Team members can edit/manage prompts independently of code changes.</li><li>Environment flexibility: Different prompt configs for dev, test, and production.</li><li>Structured management: External systems often support YAML/JSON, making complex prompts easier to manage.</li></ul> Disadvantages <ul><li>Management complexity: With many or frequently changing prompts, state objects may become large and unwieldy.</li><li>Versioning challenges: Difficult to manage and roll back historical versions.</li><li>Unstructured storage: Raw strings are hard to validate or edit systematically.</li><li>Testing overhead: Prompt changes directly affect runtime behavior, requiring extensive regression testing.</li></ul> <ul><li>Runtime immutability (by default): Prompts are static once loaded; dynamic adjustments require reloads or parameterization.</li><li>Deployment dependencies: Requires access to external systems/files at runtime.</li><li>Network latency: Remote services (like Hub) may introduce delays.</li></ul> <p>Question 15: How does LangGraph\u2019s channels system support asynchronous operations and concurrency? Specifically, when multiple parallel nodes write to the same channel simultaneously, how does LangGraph ensure data consistency and resolve conflicts?</p> Answer <p><code>LangGraph</code>\u2019s channels system is central to its asynchronous and concurrent execution model. It ensures consistency by defining explicit merge strategies (reducers) that determine how concurrent writes to the same channel are combined.</p> <p> How the Channels System Supports Async &amp; Concurrency</p> <ul> <li>Asyncio event loop foundation<ul> <li>LangGraph is built on Python\u2019s asyncio event loop.</li> <li>Node functions can be asynchronous (async def), so I/O-bound tasks don\u2019t block execution.</li> <li>LangGraph automatically awaits these async nodes until they complete.</li> </ul> </li> <li>Parallel edges<ul> <li>If a node has multiple outgoing edges (or conditionally activates multiple paths), LangGraph schedules downstream nodes in parallel.</li> <li>Under the hood, this is typically handled with <code>asyncio.gather</code> to run all concurrent tasks.</li> </ul> </li> <li>Atomic state &amp; channel isolation<ul> <li>Each <code>StateGraph</code> maintains a <code>global</code> state, divided into multiple channels, each storing a specific data type (e.g., messages, tool_calls, current_step).</li> <li>When a node returns a <code>PartialState</code>, it proposes updates to one or more channels.</li> <li>These updates are applied atomically at the channel level.</li> </ul> </li> </ul> <p> Summary</p> <ul> <li><code>LangGraph</code> leverages asyncio for asynchronous &amp; parallel execution.</li> <li>Its channels system uses reducers to resolve conflicts when multiple nodes write to the same channel.</li> <li>Choosing the right reducer is critical:<ul> <li><code>LastValue</code> \u2192 overwrite</li> <li><code>BinaryOperatorAggregate</code> \u2192 accumulate</li> <li><code>Custom reducer</code> \u2192 fine-grained conflict resolution</li> </ul> </li> <li>For workflows with complex concurrency requirements, developers must carefully design reducers or restructure the graph to avoid unsafe parallel writes.</li> </ul> <p>Question 16: What is the relationship between LangGraph\u2019s Graph and Runnable?</p> Answer <p>In the LangChain ecosystem, <code>Runnable</code> is a core abstraction that represents any executable unit that can be invoked synchronously or asynchronously, with support for input/output binding. It defines a unified interface (invoke, stream, batch, ainvoke, astream, abatch), which allows different types of components (e.g., LLM, PromptTemplate, OutputParser, Tools, Retrievers, etc.) to interact and compose in a consistent way.</p> <ul> <li>A Graph (whether <code>StateGraph</code> or <code>MessageGraph</code>) becomes a Runnable instance once compiled (via .compile()).</li> <li><code>CompiledGraph</code> is a special Runnable: the graph structure itself isn\u2019t directly Runnable, but its compiled result is fully compliant with the Runnable interface.</li> <li>Compatibility: because <code>CompiledGraph</code> implements the Runnable interface, it can seamlessly integrate with other Runnable components in the LangChain ecosystem. A LangGraph agent can be treated as a \u201ccomponent\u201d inside a larger LangChain chain.</li> <li>Composability: this makes LangGraph a powerful foundation for building complex LLM applications. For example, you can:<ul> <li>Use a LangGraph agent as a sub-chain or sub-graph inside a larger LangChain chain.</li> <li>Expose a LangGraph agent as a tool for other LangChain agents.</li> <li>Connect a LangGraph agent with other Runnables (like Prompt, Parser) via LCEL.</li> </ul> </li> </ul> <p>Question 17: How does LangGraph leverage the features of LangChain Expression Language (LCEL) to enhance itself?</p> Answer <p>LCEL (LangChain Expression Language) is a declarative, composable syntax for building complex chains. LangGraph integrates with LCEL in several key ways:</p> <ul> <li>Unified Runnable Interface<ul> <li>Node definition: each LangGraph node can be any object implementing the Runnable interface, not just Python functions. This means you can directly embed an LCEL chain (e.g., PromptTemplate | LLM | OutputParser) as a LangGraph node. This simplifies constructing complex nodes (e.g., involving multiple LLM calls or data processing).</li> <li>Composability: the entire CompiledGraph itself is a Runnable. Thus, a LangGraph agent can be treated as an atomic unit and combined with other LCEL expressions.</li> </ul> </li> <li>Input/Output Patterns (RunnableConfig, RunnablePassthrough, RunnableParallel, etc.)<ul> <li>LCEL provides flexible primitives for input/output transformation.</li> <li>LangGraph nodes can receive any LCEL-supported input and return LCEL-supported outputs.</li> <li>Example: use <code>RunnablePassthrough</code> to pass the full state object to a node, or use <code>RunnableParallel</code> to extract specific fields from the state as node input.</li> <li>Channels integration: LangGraph\u2019s channels mechanism aligns with LCEL\u2019s input/output processing. Reading from a state channel = reading from input; returning PartialState = routing/merging into channels.</li> </ul> </li> <li>Streaming (stream)<ul> <li>Since <code>CompiledGraph</code> implements the <code>Runnable</code> stream interface, a LangGraph agent can support end-to-end streaming responses.</li> <li>Benefits:<ul> <li>Users can observe intermediate reasoning and step-by-step outputs in real time instead of waiting for final results.</li> <li>Improves UX for long-running agents.</li> <li>stream output is central to LangGraph\u2019s observability, enabled by the Runnable design.</li> </ul> </li> </ul> </li> <li>Serializability (<code>.with_config(configurable=...</code>))<ul> <li>LangGraph can persist runtime state via <code>checkpointers</code>, but LCEL\u2019s Runnable abstraction extends <code>serializability</code>.</li> <li>This allows developers to serialize the definition of a LangGraph agent (not just its state) for deployment and loading across environments.</li> <li>This is critical for production deployment and management.</li> </ul> </li> <li>Remote Invocation (RemoteRunnable)<ul> <li>If a LangGraph agent is deployed as a <code>LangServe</code> endpoint, it effectively becomes a <code>RemoteRunnable</code>.</li> <li>This enables remote calls in distributed architectures, improving <code>scalability</code> and <code>service-oriented</code> workflows.</li> </ul> </li> </ul> <p>Question 18: How does LangGraph support error handling and backtracking in complex state transitions?</p> Answer <p><code>LangGraph</code> provides powerful error handling and backtracking mechanisms through its graph flexibility, state management, and conditional edges. The core idea is to treat error handling as special nodes and paths in the graph, rather than traditional try-except blocks.</p> <p> Mechanisms supporting error handling &amp; backtracking</p> <ol> <li>Explicit Error Nodes<ul> <li>Design: Create dedicated nodes to handle errors (e.g., <code>handle_api_error</code>, <code>log_and_retry</code>, <code>notify_human</code>).</li> <li>Trigger: In nodes that may throw exceptions, catch the exception and return a state update indicating the error (e.g.,set error_message with details, or set status = failed). Then use conditional edges to route control flow to error-handling nodes.</li> </ul> </li> <li>Conditional Edges<ul> <li>Core: <code>add_conditional_edges</code> is key for error recovery. After a node executes, its output or updated state is passed into a condition function, which decides whether to continue the normal flow or branch to an error handler.</li> <li>Routing Example: After a tool node executes, a condition function checks its result. If success \u2192 move forward; if failure \u2192 route to <code>error_handling_node</code>.</li> </ul> </li> <li>State Management<ul> <li>Error propagation: Pass error info as part of the <code>AgentState</code>. This makes it accessible throughout the graph \u2014 so the LLM or other nodes can read it when deciding what to do next.</li> <li>Retry counters: Maintain a retry counter in the state. Error nodes can increment this, and condition edges can check if the max retry threshold is reached.</li> <li>Rollback points: Specific states can be marked as \u201csafe rollback points\u201d. If a critical error occurs, the agent can reload a previous safe state.</li> </ul> </li> </ol> Example <ul> <li><code>AgentState</code> including:<ul> <li><code>current_query</code></li> <li><code>flight_info</code></li> <li><code>api_error</code></li> <li><code>api_retries</code></li> <li><code>llm_decision</code></li> </ul> </li> <li>Nodes Design<ul> <li>plan_flight_search (LLM Node):<ul> <li>Plans API query from user request (optionally using error info).</li> <li>Updates state with <code>current_query</code> or <code>llm_decision</code>.</li> </ul> </li> <li>call_flight_api (Tool Node):**<ul> <li>Calls flight API with query params.</li> <li>Updates state with <code>flight_info</code> or <code>api_error</code>.</li> </ul> </li> <li>check_api_status (Conditional Node / Function):<ul> <li>Checks if <code>call_flight_api</code> succeeded.</li> <li>Outputs <code>api_succeeded</code>, <code>api_failed_retry</code>, or <code>api_failed_no_retry</code>.</li> </ul> </li> <li>reflect_on_api_error (LLM Node):<ul> <li>Takes in <code>api_error</code> and asks LLM to decide: <code>retry</code>, <code>alternative API</code>, or <code>user explanation</code>.</li> <li>Updates state with <code>llm_decision</code>.</li> </ul> </li> <li>handle_llm_decision (Conditional Node / Function):<ul> <li>Parses <code>llm_decision</code>.</li> <li>Routes to <code>retry_same_api</code>, <code>try_alternative_api</code>, <code>respond_to_user</code>, or <code>end_session</code>.</li> </ul> </li> <li>respond_to_user (LLM Node):<ul> <li>Generates final user-facing message based on <code>flight_info</code> or <code>api_error</code> and <code>decision</code>.</li> </ul> </li> </ul> </li> </ul> <pre><code>\n    graph TD\n    A[Start: User Query] --&gt; B(plan_flight_search)\n    B --&gt; C(call_flight_api)\n    C --&gt; D(check_api_status)\n\n    D -- \"api_succeeded\" --&gt; E(respond_to_user)\n    D -- \"api_failed_retry\" --&gt; F(reflect_on_api_error)\n    D -- \"api_failed_no_retry\" --&gt; G(respond_to_user)\n\n    F --&gt; H(handle_llm_decision)\n    H -- \"retry_same_api\" --&gt; C\n    H -- \"try_alternative_api\" --&gt; I(call_alternative_flight_api)\n    H -- \"respond_to_user\" --&gt; E\n    H -- \"end_session\" --&gt; K[END]\n\n    I --&gt; D \n    E --&gt; K\n    G --&gt; K</code></pre> <p> Elegant Error Recovery Features</p> <ul> <li>Explicit error state: <code>api_error</code> in state captures error details.</li> <li>Retry counter: <code>api_retries</code> prevents infinite loops, guiding <code>check_api_status</code>.</li> <li> <p>LLM-driven decision-making:</p> <ul> <li><code>reflect_on_api_error</code> lets the LLM analyze error messages and decide next steps.</li> <li>More flexible than fixed retry logic.</li> <li>Possible actions:<ul> <li>Adjust params \u2192 retry API.</li> <li>Switch to alternative API.</li> <li>Explain to user &amp; end session.</li> </ul> </li> </ul> </li> <li> <p>Multiple paths: Conditional nodes route dynamically based on outcomes.</p> </li> <li>Backtracking: With <code>checkpointers</code>, states can be restored. More importantly, LLM \u201creflection\u201d enables logical correction and adaptive recovery.</li> </ul> <p> Summary:</p> <p>LangGraph enables highly robust agents by treating errors as graph nodes and flows rather than exceptions. This allows agents not only to detect errors but also to recover intelligently\u2014even changing strategies when needed. This is far more advanced than simple <code>try-except</code> or fixed retry loops.</p> <p>Question 19: Discuss the flexibility and limitations of <code>RunnableLambda</code> when building nodes in LangGraph. In which scenarios is it more advantageous than using a plain Python function directly as a node? Conversely, in which cases would using more complex Runnable compositions (via LCEL) be a better choice?</p> Answer <p>In LangGraph, nodes can be any callable object, and <code>RunnableLambda</code> is one important option. It allows wrapping a simple <code>Python</code> function as a Runnable, so it can integrate seamlessly with other LangChain Runnable components.</p> <p>RunnableLambda is an implementation of Runnable that wraps a Python function (or lambda expression). Its input is the function\u2019s argument(s), and its output is the return value.</p> <ul> <li>Flexibility and Advantages<ul> <li>LCEL Compatibility<ul> <li>Advantage: This is its core strength. RunnableLambda enables plain Python functions to participate in LCEL chains. You can connect RunnableLambda with PromptTemplate, LLM, OutputParser, or any other Runnable.</li> <li>Scenario: If you have a simple Python function that needs to integrate with other Runnable components (e.g., preprocessing before an LLM, postprocessing afterward), RunnableLambda makes that possible.</li> </ul> </li> <li>Asynchronous Support<ul> <li>Advantage: RunnableLambda can wrap async def functions, making nodes support async execution.</li> <li>Scenario: Useful for I/O-heavy operations (external services, DB queries) where async execution prevents blocking and improves overall efficiency.</li> </ul> </li> <li>Standard Runnable Interface<ul> <li>Advantage: Inherits all Runnable methods (invoke, batch, stream, ainvoke, abatch). This makes it testable like any other Runnable.</li> <li>Scenario: When you want to independently unit-test a node in a standardized way.</li> </ul> </li> <li>Configuration &amp; Binding<ul> <li>Advantage: Supports with_config, bind, partial, etc., so you can configure or partially apply parameters.</li> <li>Scenario: For example, setting metadata/tags for better observability in LangSmith, or pre-binding parameters that remain constant.</li> </ul> </li> </ul> </li> <li>Limitations of RunnableLambda<ul> <li>Single-function wrapper: RunnableLambda can only encapsulate one function. If a node requires multiple internal steps (e.g., LLM call \u2192 parse result \u2192 call tool), RunnableLambda isn\u2019t sufficient\u2014you\u2019ll need a more complex Runnable chain.</li> <li>No inherent state management: RunnableLambda itself doesn\u2019t handle LangGraph state transitions. It just wraps a function. Input comes from LangGraph state, and output is returned as PartialState. State merging is handled separately by LangGraph\u2019s channel system.</li> </ul> </li> </ul> <p> When Complex Runnable Compositions (via LCEL) Are Better:</p> <p>When a node\u2019s internal logic is itself multi-step and complex, using LCEL compositions directly as nodes is more powerful: - Complex LLM interaction nodes - Nodes with tool invocation - Data preprocessing/postprocessing nodes</p> <p> Summary</p> <ul> <li>RunnableLambda: Best for simple, single-step Python logic (sync or async) that needs to be \u201cRunnable-ized\u201d and combined with other Runnables.</li> <li>Complex Runnable compositions (via LCEL): Best for nodes that represent multi-step, structured LangChain workflows, where declarative chaining improves modularity and maintainability.</li> </ul> <p>Which approach to choose depends on the complexity of node logic and its interaction with other LangChain components.</p> <p>Question 20: What is the practical significance of LangGraph\u2019s <code>checkpointe</code>r mechanism for iterative development and deployment in long-lived agents?</p> Answer <p>The <code>checkpointer</code> mechanism is one of the core features of LangGraph and is essential for building and managing long-lived agents. It allows the agent\u2019s current execution state to be persisted to external storage and reloaded when needed. This enables fault recovery, state restoration, debugging, and iterative development.</p> <p> Roles of the checkpointer in different development stages:</p> <ol> <li> <p>Development Stage</p> <ul> <li>Rapid iteration and debugging: Developers can pause the agent at any point, save its state, modify the code, and then reload the state to continue execution from the pause point. This significantly accelerates the debugging cycle of complex agents and avoids rerunning the workflow from scratch after every change.</li> <li>Reproducing issues: When nondeterministic errors occur, the checkpointer helps reproduce specific error states for deeper analysis.</li> <li>State inspection: Developers can inspect stored states at any step to verify that the logic behaves as expected.</li> <li>Example: Use <code>InMemoryCheckpointSaver</code> or <code>SQLiteSaver</code> for lightweight, local state persistence.</li> </ul> </li> <li> <p>Testing Stage</p> <ul> <li>Regression testing: <code>Checkpointer</code> enables fixed test cases and expected states. By loading a historical state and executing a defined path, one can verify whether behavior remains consistent after code changes.</li> <li>Edge case testing: Developers can craft or simulate abnormal states, save them, and reload to test robustness under edge conditions.</li> <li>Performance testing: Capture execution times and resource usage across states.</li> <li>Example: Use <code>SQLiteSaver</code> or <code>Redis</code> configured for test environments.</li> </ul> </li> <li>Production Stage<ul> <li>Fault recovery &amp; high availability: Core production value \u2014 if the agent server crashes or restarts, the system can resume from the last saved state, minimizing downtime and data loss.</li> <li>Long-running task management: Ensures multi-day workflows or jobs survive redeployments, system maintenance, or external interruptions.</li> <li>Scalability: With distributed storage (e.g., <code>Redis</code>, databases), multiple agent instances can share and access the same states for horizontal scaling.</li> <li>A/B testing &amp; canary releases: Different agent versions can run side-by-side, loading shared states for seamless rollout.</li> <li>Audit &amp; compliance: Persistent states act as a complete execution log, meeting audit and compliance requirements.</li> <li>Example: Use <code>RedisSaver</code>, <code>PostgresSaver</code>, or enterprise-grade checkpoint stores.</li> </ul> </li> </ol> Takeaways <ul> <li> Choose the right storage backend<ul> <li>High concurrency/distributed: Use <code>Redis</code>, <code>PostgreSQL</code>, <code>MongoDB</code>, etc.</li> <li>Strong consistency: Prefer relational databases with transactions.</li> <li>Cost &amp; operations: Balance complexity vs. budget.</li> </ul> </li> <li> Version compatibility strategy<ul> <li>Forward-compatible design: Add fields rather than removing or renaming them.</li> <li>State versioning: Embed version numbers in AgentState with migration logic.</li> <li>Migration scripts: For breaking changes, upgrade old state data before rollout.</li> <li>Blue-green/canary deployments: Ensure new versions can process old states before switching traffic fully.</li> </ul> </li> <li> Security &amp; privacy<ul> <li>Encrypt sensitive data (at rest and in transit).</li> <li>Enforce strict access controls for read/write operations.</li> <li>Apply data masking to unnecessary or sensitive PII.</li> </ul> </li> <li> Performance optimization<ul> <li>Persist only essential state; avoid redundant/temporary data.</li> <li>Use incremental updates with LangGraph\u2019s channel-based merging.</li> <li>Add indexes for frequent queries in relational DBs.</li> <li>Periodically purge inactive or completed session states.</li> </ul> </li> <li> Monitoring &amp; alerting<ul> <li>Track backend health (e.g., Redis connections, DB latency).</li> <li>Monitor state read/write latency and throughput.</li> <li>Watch error rates on checkpoint operations.</li> <li>Set alerts for thresholds to quickly resolve storage issues.</li> </ul> </li> <li> Backup &amp; recovery<ul> <li>Regularly back up stored states.</li> <li>Test recovery procedures to ensure reliability.</li> </ul> </li> </ul>"},{"location":"2025/09/langchainlanggraph-qa/#implementation","title":"Implementation","text":"<ul> <li>Core Structure<ul> <li>Agent node: An LLM node that takes the dialogue history and tool descriptions as input, then reasons about the next step\u2014whether to call a tool (and with what parameters) or to produce a final answer.</li> <li>Tools node: Executes the tool call produced by the agent node. It runs the function and returns the result as a new message.</li> </ul> </li> <li>Conditional Edges<ul> <li>From <code>agent \u2192 tools</code>: If the agent outputs a ToolCall, control flows to the tools node. If it outputs a final AIMessage, control flows to the END</li> <li>From <code>tools \u2192 agent</code>: After executing a tool, control typically returns to the agent node so the LLM can reason further based on the tool\u2019s output (the ReAct loop).</li> </ul> </li> </ul>"},{"location":"2025/09/langchainlanggraph-qa/#beyond-the-traditional-react-pattern","title":"Beyond the Traditional ReAct Pattern","text":"<p>Traditional LangChain agents implement ReAct as a fixed loop:</p> <p>Observe \u2192 Think (LLM) \u2192 Act (Tool) \u2192 Observe \u2192 Think \u2026</p> <p>LangGraph expands this pattern in several ways:</p> <ul> <li>Parallel tools: Unlike ReAct, which usually invokes one tool at a time, LangGraph allows the agent node to output multiple ToolCalls. These can be executed in parallel or orchestrated sequentially inside the tools node.</li> <li>Repeated tool calls: The agent node can re-call the same tool (e.g., retry on failure), switch to another tool, or decide to produce the final answer\u2014all naturally supported by the graph\u2019s looping structure.</li> <li>Backtracking and correction: If a tool call fails or produces unexpected results, the agent node can \u201crewind\u201d to a prior reasoning step and try a different tool or parameters\u2014something difficult to achieve in a simple sequential chain.</li> <li>Conditional branching: The LLM can decide not only to call tools or answer directly, but also to request clarification, delegate tasks to sub-agents, or follow other runtime paths. This is implemented with <code>add_conditional_edges</code>, removing the restriction that tool use is the only possible \u201caction.\u201d</li> <li>Nested agents / subgraphs: LangGraph supports embedding an entire graph as a node. A main agent can delegate specialized tasks to a sub-agent (itself a LangGraph graph), enabling modular and hierarchical agent architectures far beyond simple tool calls.</li> <li>Error handling and recovery: Dedicated nodes can manage exceptions such as tool failures or malformed LLM outputs. These nodes may attempt recovery, log errors, or gracefully terminate execution\u2014capabilities typically handled by external try-except blocks in traditional agents.</li> </ul>"},{"location":"2025/10/langgraph-sample-project/","title":"LangGraph Sample Project","text":""},{"location":"2025/10/langgraph-sample-project/#langgraph-sample-project","title":"LangGraph Sample Project","text":""},{"location":"2025/10/langgraph-sample-project/#overview","title":"Overview","text":""},{"location":"2025/10/langgraph-sample-project/#key-points","title":"Key Points","text":""},{"location":"2025/10/langgraph-sample-project/#codes-explanation","title":"Codes Explanation","text":"<pre><code>  graph TD\n    Start([Start]) --&gt; ValidateQuery[Validate Query]\n\n    ValidateQuery --&gt;|Success| CreatePlan[Create Research Plan]\n    ValidateQuery --&gt;|Error &amp; Retries| ValidateQuery\n    ValidateQuery --&gt;|Error &amp; Max Retries| End([End])\n\n    CreatePlan --&gt;|Success| GatherInfo[Gather Information]\n    CreatePlan --&gt;|Error &amp; Retries| CreatePlan\n    CreatePlan --&gt;|Error &amp; Max Retries| End\n\n    GatherInfo --&gt;|Success| Synthesize[Synthesize Findings]\n    GatherInfo --&gt;|Error &amp; Retries| GatherInfo\n    GatherInfo --&gt;|Error &amp; Max Retries| End\n\n    Synthesize --&gt;|Success| GenerateReport[Generate Report]\n    Synthesize --&gt;|Error &amp; Retries| Synthesize\n    Synthesize --&gt;|Error &amp; Max Retries| End\n\n    GenerateReport --&gt;|Success| End\n    GenerateReport --&gt;|Error &amp; Retries| GenerateReport\n    GenerateReport --&gt;|Error &amp; Max Retries| End\n\n    style ValidateQuery fill:#e1f5ff\n    style CreatePlan fill:#e1f5ff\n    style GatherInfo fill:#e1f5ff\n    style Synthesize fill:#e1f5ff\n    style GenerateReport fill:#e1f5ff\n    style Start fill:#d4edda\n    style End fill:#f8d7da</code></pre> Full Code <pre><code>import asyncio\nimport logging\nfrom functools import wraps\nfrom typing import Protocol, Optional, TypeVar, Any, Callable, Annotated\nfrom langgraph.graph import StateGraph, END\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langgraph.graph.state import CompiledStateGraph\nfrom pydantic import BaseModel, Field\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\n# ============================================================================\n# ERROR HANDLER (from your code)\n# ============================================================================\n\nclass ErrorState(Protocol):\n    error_messages: list[str]\n    retry_count: int\n    max_retries: int\n    last_failed_node: Optional[str]\n    current_node: Optional[str]\n\n\nStateType = TypeVar('StateType', bound=ErrorState)\n\n\nclass ErrorHandler:\n    @staticmethod\n    def handle_error(state: StateType, error: Exception, node_name: str, custom_message: Optional[str] = None) -&gt; dict:\n        error_msg = custom_message or f\"Error in {node_name}: {str(error)}\"\n        logger.error(f\"Node '{node_name}' failed: {str(error)}\")\n\n        return {\n            \"error_messages\": [error_msg],\n            \"retry_count\": state.retry_count + 1,\n            \"last_failed_node\": node_name,\n            \"current_node\": node_name\n        }\n\n    @staticmethod\n    def should_retry(state: StateType) -&gt; bool:\n        return state.retry_count &lt; state.max_retries and len(state.error_messages) &gt; 0\n\n    @staticmethod\n    def clear_errors(state: StateType) -&gt; dict[str, Any]:\n        return {\n            \"error_messages\": [],\n            \"retry_count\": 0,\n            \"last_failed_node\": None,\n        }\n\n    @staticmethod\n    def get_error_summary(state: StateType) -&gt; dict[str, Any]:\n        return {\n            \"has_errors\": len(state.error_messages) &gt; 0,\n            \"error_count\": len(state.error_messages),\n            \"retry_count\": state.retry_count,\n            \"last_failed_node\": state.last_failed_node,\n            \"can_retry\": ErrorHandler.should_retry(state)\n        }\n\n\ndef handle_node_errors(node_name: str, custom_message: Optional[str] = None):\n    def decorator(func: Callable) -&gt; Callable:\n        @wraps(func)\n        async def async_wrapper(self, state: StateType) -&gt; dict[str, Any]:\n            try:\n                result = await func(self, state)\n                if result is None:\n                    result = {}\n                result.update(ErrorHandler.clear_errors(state))\n                return result\n            except Exception as e:\n                logger.exception(f\"Error in async node '{node_name}'\")\n                return ErrorHandler.handle_error(state, e, node_name, custom_message)\n\n        @wraps(func)\n        def sync_wrapper(self, state: StateType) -&gt; dict[str, Any]:\n            try:\n                result = func(self, state)\n                if result is None:\n                    result = {}\n                result.update(ErrorHandler.clear_errors(state))\n                return result\n            except Exception as e:\n                logger.exception(f\"Error in sync node '{node_name}'\")\n                return ErrorHandler.handle_error(state, e, node_name, custom_message)\n\n        if asyncio.iscoroutinefunction(func):\n            return async_wrapper\n        else:\n            return sync_wrapper\n\n    return decorator\n\n\n# ============================================================================\n# STATE DEFINITION\n# ============================================================================\n\nclass ResearchState(BaseModel):\n    \"\"\"State for the research assistant workflow\"\"\"\n    query: str = \"\"\n    research_plan: str = \"\"\n    search_results: list[str] = Field(default_factory=list)\n    summary: str = \"\"\n    final_report: str = \"\"\n\n    # Error handling fields\n    error_messages: list[str] = Field(default_factory=list)\n    retry_count: int = 0\n    max_retries: int = 3\n    last_failed_node: Optional[str] = None\n    current_node: Optional[str] = None\n\n    # Control flow\n    should_continue: bool = True\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\n# ============================================================================\n# RESEARCH NODES\n# ============================================================================\n\nclass ResearchNodes:\n    \"\"\"Collection of nodes for the research workflow\"\"\"\n\n    def __init__(self, llm: Optional[ChatOllama] = None):\n        self.llm = llm or ChatOllama(model=\"gpt-oss\", temperature=0)\n\n    @handle_node_errors(\"validate_query\", \"Failed to validate the research query\")\n    def validate_query(self, state: ResearchState) -&gt; dict[str, Any]:\n        \"\"\"Validate that the query is appropriate for research\"\"\"\n        logger.info(f\"Validating query: {state.query}\")\n\n        if not state.query or len(state.query.strip()) &lt; 5:\n            raise ValueError(\"Query must be at least 5 characters long\")\n\n        # Simulate potential validation issues\n        if \"error\" in state.query.lower():\n            raise ValueError(\"Query contains forbidden terms\")\n\n        return {\n            \"current_node\": \"validate_query\",\n            \"should_continue\": True\n        }\n\n    @handle_node_errors(\"create_research_plan\", \"Failed to create research plan\")\n    async def create_research_plan(self, state: ResearchState) -&gt; dict[str, Any]:\n        \"\"\"Create a research plan based on the query\"\"\"\n        logger.info(f\"Creating research plan for: {state.query}\")\n\n        messages = [\n            SystemMessage(content=\"You are a research planning assistant. Create a brief 3-step research plan.\"),\n            HumanMessage(content=f\"Create a research plan for: {state.query}\")\n        ]\n\n        response = await self.llm.ainvoke(messages)\n\n        if not response.content:\n            raise ValueError(\"LLM returned empty research plan\")\n\n        return {\n            \"research_plan\": response.content,\n            \"current_node\": \"create_research_plan\",\n            \"should_continue\": True\n        }\n\n    @handle_node_errors(\"gather_information\", \"Failed to gather information\")\n    async def gather_information(self, state: ResearchState) -&gt; dict[str, Any]:\n        \"\"\"Simulate gathering information from various sources\"\"\"\n        logger.info(\"Gathering information...\")\n\n        # Simulate API calls that might fail\n        await asyncio.sleep(0.5)\n\n        # Simulate random failures for demonstration\n        import random\n        if random.random() &lt; 0.2:  # 20% chance of failure\n            raise ConnectionError(\"Failed to connect to research database\")\n\n        # Simulate search results\n        search_results = [\n            f\"Research finding 1 about {state.query}\",\n            f\"Research finding 2 about {state.query}\",\n            f\"Research finding 3 about {state.query}\",\n        ]\n\n        return {\n            \"search_results\": search_results,\n            \"current_node\": \"gather_information\",\n            \"should_continue\": True\n        }\n\n    @handle_node_errors(\"synthesize_findings\", \"Failed to synthesize findings\")\n    async def synthesize_findings(self, state: ResearchState) -&gt; dict[str, Any]:\n        \"\"\"Synthesize the gathered information into a summary\"\"\"\n        logger.info(\"Synthesizing findings...\")\n\n        if not state.search_results:\n            raise ValueError(\"No search results available to synthesize\")\n\n        findings_text = \"\\n\".join(f\"- {result}\" for result in state.search_results)\n\n        messages = [\n            SystemMessage(content=\"You are a research synthesis assistant. Summarize the findings concisely.\"),\n            HumanMessage(\n                content=f\"Research Plan:\\n{state.research_plan}\\n\\nFindings:\\n{findings_text}\\n\\nProvide a brief summary.\")\n        ]\n\n        response = await self.llm.ainvoke(messages)\n\n        return {\n            \"summary\": response.content,\n            \"current_node\": \"synthesize_findings\",\n            \"should_continue\": True\n        }\n\n    @handle_node_errors(\"generate_report\", \"Failed to generate final report\")\n    async def generate_report(self, state: ResearchState) -&gt; dict[str, Any]:\n        \"\"\"Generate the final research report\"\"\"\n        logger.info(\"Generating final report...\")\n\n        messages = [\n            SystemMessage(content=\"You are a report writing assistant. Create a concise final report.\"),\n            HumanMessage(content=f\"Query: {state.query}\\n\\nSummary: {state.summary}\\n\\nCreate a final report.\")\n        ]\n\n        response = await self.llm.ainvoke(messages)\n\n        return {\n            \"final_report\": response.content,\n            \"current_node\": \"generate_report\",\n            \"should_continue\": False\n        }\n\n\n# ============================================================================\n# ROUTING LOGIC\n# ============================================================================\n\n\ndef create_universal_router(next_node: str, end_node: str = END):\n    \"\"\"Create a universal router that handles errors and retries\"\"\"\n\n    def router(state) -&gt; str:\n        # Handle both dict and Pydantic model\n        if isinstance(state, dict):\n            error_messages = state.get('error_messages', [])\n            retry_count = state.get('retry_count', 0)\n            max_retries = state.get('max_retries', 3)\n            last_failed_node = state.get('last_failed_node', 'validate_query')\n        else:\n            error_messages = state.error_messages\n            retry_count = state.retry_count\n            max_retries = state.max_retries\n            last_failed_node = state.last_failed_node or 'validate_query'\n\n        if len(error_messages) &gt; 0:\n            if retry_count &lt; max_retries:\n                logger.info(f\"Retrying {last_failed_node}, attempt {retry_count}/{max_retries}\")\n                return last_failed_node\n            else:\n                logger.error(f\"Max retries reached for {last_failed_node}, ending execution\")\n                return end_node\n        else:\n            return next_node\n\n    return router\n\n\ndef should_retry_node(state) -&gt; str:\n    \"\"\"Route back to the failed node for retry\"\"\"\n    if isinstance(state, dict):\n        last_failed = state.get('last_failed_node', 'validate_query')\n    else:\n        last_failed = state.last_failed_node or 'validate_query'\n\n    logger.info(f\"Routing to retry node: {last_failed}\")\n    return last_failed\n\n\n\n\n\n# ============================================================================\n# GRAPH CONSTRUCTION\n# ============================================================================\n\ndef create_research_graph(llm: Optional[ChatOllama] = None) -&gt; CompiledStateGraph:\n    \"\"\"Create the research assistant graph with error handling\"\"\"\n\n    nodes = ResearchNodes(llm)\n\n    # Create the graph\n    workflow = StateGraph(ResearchState)\n\n    # Add nodes\n    workflow.add_node(\"validate_query\", nodes.validate_query)\n    workflow.add_node(\"create_research_plan\", nodes.create_research_plan)\n    workflow.add_node(\"gather_information\", nodes.gather_information)\n    workflow.add_node(\"synthesize_findings\", nodes.synthesize_findings)\n    workflow.add_node(\"generate_report\", nodes.generate_report)\n\n    # Set entry point\n    workflow.set_entry_point(\"validate_query\")\n\n    # Add conditional edges using universal router\n    # Router will automatically retry the failed node or move to next node\n    workflow.add_conditional_edges(\n        \"validate_query\",\n        create_universal_router(next_node=\"create_research_plan\")\n    )\n    workflow.add_conditional_edges(\n        \"create_research_plan\",\n        create_universal_router(next_node=\"gather_information\")\n    )\n    workflow.add_conditional_edges(\n        \"gather_information\",\n        create_universal_router(next_node=\"synthesize_findings\")\n    )\n    workflow.add_conditional_edges(\n        \"synthesize_findings\",\n        create_universal_router(next_node=\"generate_report\")\n    )\n    workflow.add_conditional_edges(\n        \"generate_report\",\n        create_universal_router(next_node=END)\n    )\n\n    return workflow.compile()\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nasync def main():\n    \"\"\"Run the research assistant\"\"\"\n\n    print(\"=\" * 80)\n    print(\"RESEARCH ASSISTANT WITH ERROR HANDLING\")\n    print(\"=\" * 80)\n\n    # Create the graph\n    graph = create_research_graph()\n\n    # Test queries\n    queries = [\n        \"What are the latest developments in quantum computing?\",\n        \"err\",  # This will fail validation (too short)\n        \"Impact of artificial intelligence on healthcare\",\n    ]\n\n    for i, query in enumerate(queries, 1):\n        print(f\"\\n{'=' * 80}\")\n        print(f\"QUERY {i}: {query}\")\n        print(f\"{'=' * 80}\\n\")\n\n        initial_state = ResearchState(query=query)\n\n        try:\n\n            final_state = await graph.ainvoke(initial_state)\n\n            # Display results\n            print(\"\\n\" + \"=\" * 80)\n            print(\"RESULTS\")\n            print(\"=\" * 80)\n\n            # final_state is a dict, not a ResearchState object\n            error_messages = final_state.get(\"error_messages\", [])\n\n            if error_messages:\n                print(f\"\\n\u274c FAILED with errors:\")\n                for error in error_messages:\n                    print(f\"  - {error}\")\n                print(f\"\\nRetry count: {final_state.get('retry_count', 0)}/{final_state.get('max_retries', 3)}\")\n            else:\n                print(f\"\\n\u2705 SUCCESS!\")\n                research_plan = final_state.get('research_plan', 'N/A')\n                summary = final_state.get('summary', 'N/A')\n                final_report = final_state.get('final_report', 'N/A')\n\n                print(f\"\\nResearch Plan:\\n{research_plan[:200] if research_plan != 'N/A' else research_plan}...\")\n                print(f\"\\nSummary:\\n{summary[:200] if summary != 'N/A' else summary}...\")\n                print(f\"\\nFinal Report:\\n{final_report[:300] if final_report != 'N/A' else final_report}...\")\n\n            # Show error summary (convert dict to object-like for ErrorHandler)\n            print(f\"\\nError Summary:\")\n            print(f\"  - Has errors: {len(error_messages) &gt; 0}\")\n            print(f\"  - Error count: {len(error_messages)}\")\n            print(f\"  - Retry count: {final_state.get('retry_count', 0)}\")\n            print(f\"  - Last failed node: {final_state.get('last_failed_node', 'None')}\")\n\n\n\n        except Exception as e:\n            print(f\"\\n\u274c Unexpected error: {e}\")\n\n        await asyncio.sleep(1)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"2025/02/autogen-intro-and-rag-workflow/","title":"Autogen Intro and RAG Workflow","text":""},{"location":"2025/02/autogen-intro-and-rag-workflow/#introduction-to-autogen","title":"Introduction to Autogen","text":"<p>Autogen</p> <p>AutoGen is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.</p> <p>\ud83d\udd39 Agent Types in AutoGen</p> Agent Type Description Use Case BaseChatAgent The foundation for all other agents, handling basic messaging and conversation logic. Custom agent development, extending its capabilities. AssistantAgent A chatbot-like agent designed to assist with general problem-solving and respond to queries. AI assistants, research helpers, customer support. UserProxyAgent Simulates a human user by sending human-like inputs to other agents. Automating user interactions, debugging multi-agent systems. CodeExecutorAgent Specialized agent that executes Python code, typically used for coding tasks. Automating programming tasks, AI-assisted development. SocietyOfMindAgent Manages multiple agents as a hierarchical system to collaborate on complex tasks. Coordinating multiple AI agents for team-based problem-solving. <p>Tips </p> Need Agent Type Basic AI conversation AssistantAgent Simulating human input UserProxyAgent Executing Python code CodeExecutorAgent Coordinating multiple agents SocietyOfMindAgent Custom agent development BaseChatAgent"},{"location":"2025/02/autogen-intro-and-rag-workflow/#example-of-autogen-rag-workflow","title":"Example of Autogen RAG workflow","text":"<pre><code>doc_extractor_topic_type = \"DocExtractorAgent\"\nretrieval_topic_type = \"RetrievalAgent\"\nuser_topic_type = \"User\"\n\n\n@type_subscription(topic_type=doc_extractor_topic_type)\nclass DocExtractorAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, filename:str) -&gt; None:\n        super().__init__(\"A concept extractor agent.\")\n        self.filename = filename\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_description(self, message: Message, ctx: MessageContext) -&gt; None:\n        response = f\"Complete extracting content from File {message}\"\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(retrieval_topic_type, source=self.id.key))\n\n\n\n# Define the RetrievalAgent by extending BaseChatAgent\n@type_subscription(topic_type=retrieval_topic_type)\nclass RetrievalAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, v:PGVector) -&gt; None:\n        super().__init__(\"Custom Retrieval Agent\")\n        self.vector_db = v\n        self._model_client = model_client\n\n\n    @message_handler\n    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -&gt; None:\n        response = self.vector_db.similarity_search(message.content)\n        string_output_comprehension = \"\\n\".join([doc.page_content for doc in response])\n        assert isinstance(string_output_comprehension, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{string_output_comprehension}\")\n        await self.publish_message(Message(string_output_comprehension), topic_id=TopicId(user_topic_type, source=self.id.key))\n\n\n\n@type_subscription(topic_type=user_topic_type)\nclass UserAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -&gt; None:\n        super().__init__(\"A user agent that outputs\")\n        self._system_message = SystemMessage(\n            content=(\n                \"Your are a data analyst who is responsible for retrieving and summarizing data from provided content \"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_final_copy(self, message: Message, ctx: MessageContext) -&gt; None:\n        prompt = f\"Below is the info about the report:\\n\\n{message.content}\"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        print(f\"\\n{'-'*80}\\n{self.id.type} received final copy:\\n{response}\")\n\n\nasync def main():\n    config = {}\n    model_client = ChatCompletionClient.load_component(config)\n    runtime = SingleThreadedAgentRuntime()\n    await DocExtractorAgent.register(runtime, type=doc_extractor_topic_type,\n                                     factory=lambda : DocExtractorAgent(model_client=model_client))\n    await RetrievalAgent.register(runtime, type=retrieval_topic_type,\n                                  factory=lambda : RetrievalAgent(model_client=model_client))\n\n    await UserAgent.register(runtime, type=user_topic_type,\n                             factory=lambda : UserAgent(model_client=model_client))\n\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n</code></pre>"},{"location":"2025/02/local-llm-setup/","title":"Local LLM Setup","text":""},{"location":"2025/02/local-llm-setup/#local-llm-setup","title":"Local LLM Setup","text":""},{"location":"2025/02/local-llm-setup/#introduction","title":"Introduction","text":"<p>This guide will walk you through setting up a local language model (LLM) using Ollama. Ollama is an open-source, lightweight language model that can be run locally on your machine or server. This setup allows for greater privacy and control over the model, as well as potentially faster inference times compared to cloud-based services.</p>"},{"location":"2025/02/local-llm-setup/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following: - Python installed (version 3.11 or later) - Ollama installed - Basic knowledge of Python programming</p>"},{"location":"2025/02/local-llm-setup/#installation","title":"Installation","text":""},{"location":"2025/02/local-llm-setup/#step-1-python-virtual-environment","title":"Step 1: Python Virtual Environment","text":"<pre><code>python -m venv ollama_env\nsource ollama_env/bin/activate  # On Windows, use `ollama_env\\Scripts\\activate`\n</code></pre>"},{"location":"2025/02/local-llm-setup/#step-2-install-ollama","title":"Step 2: Install Ollama","text":"<pre><code>brew install ollama\n# After installation, you can verify it by running:\nbrew services start ollama # For macOS users or Linux users using systemd.\n# If you prefer to run it as a non-service:\nollama serve &amp;\n# -----------------------------------------------\nollama --version\nollama list\n</code></pre>"},{"location":"2025/02/local-llm-setup/#step-3-install-ollama-models","title":"Step 3: Install Ollama Models","text":"<ul> <li>Command line : <code>ollama pull &lt;model_name&gt;</code></li> <li>Manually import: Guide</li> </ul>"},{"location":"2025/02/local-llm-setup/#step-4-verify-llm-models","title":"Step 4: Verify LLM Models","text":""},{"location":"2025/02/local-llm-setup/#utilization-of-ollama","title":"Utilization of Ollama","text":""},{"location":"2025/02/local-llm-setup/#copilot","title":"Copilot","text":"<p>Without <code>GitHub Copilot</code>, you can use Ollama for code generation and assistance in your projects. Here\u2019s how to set it up:</p>"},{"location":"2025/02/local-llm-setup/#vscode-setup","title":"VSCode Setup","text":"<ul> <li>AI Toolkit for Visual Studio Code Extension </li> <li>Additionally, you can configure your OpenAI API key if needed</li> <li>Chatbot in Playground: Use Ollama as a chatbot to assist with coding tasks directly within VSCode. </li> <li>Continue Extension</li> <li>This extension allows you to continue from where the code snippet left off, making it easier to build on existing code without starting from scratch.</li> <li>Open <code>config.json</code> </li> <li> <p>Add Ollama config   </p> </li> <li> <p>If you find this in your VSCode, congratulations! You have successfully set up Ollama for code generation and assistance in Visual Studio Code. </p> </li> </ul>"},{"location":"2025/02/local-llm-setup/#pycharm-idea-setup","title":"Pycharm &amp; IDEA Setup","text":"<ul> <li>Continue Extension installed from the JetBrains Marketplace</li> <li>Follow the same steps as VSCode setup to configure Ollama in your preferred IDE.</li> </ul>"},{"location":"2025/02/local-llm-setup/#rag-with-ollama-a-comprehensive-guide","title":"RAG with Ollama: A Comprehensive Guide","text":"<p>RAG (Retrieval Augmented Generation) is a powerful technique that enhances the capabilities of large language models by integrating external data sources, such as documents or databases, into the generation process. This integration allows for more accurate and relevant responses to queries, making it an essential tool in various applications, including but not limited to:</p> <ul> <li>Technical Documentation Assistance</li> <li>Code Generation &amp; Debugging</li> <li>Research Paper Summarization</li> <li>Customer Support Chatbots</li> </ul>"},{"location":"2025/02/local-llm-setup/#setting-up-rag-with-ollama","title":"Setting Up RAG with Ollama","text":"<p>To set up RAG with Ollama, you'll need to follow these steps:</p> <ol> <li>Install Ollama: Ensure that Ollama is properly installed and configured in your preferred IDE or text editor.</li> <li>Configure Data Sources: Ollama supports various data sources such as local files, databases, and APIs. You can configure these data sources through the Ollama interface or configuration file.   </li> </ol>"},{"location":"2025/","title":"2025","text":""},{"location":"2024/","title":"2024","text":""},{"location":"2021/","title":"2021","text":""},{"location":"2020/","title":"2020","text":""},{"location":"2012/","title":"2012","text":""},{"location":"category/llm/","title":"LLM","text":""},{"location":"category/python/","title":"python","text":""},{"location":"category/spark/","title":"spark","text":""},{"location":"category/scala/","title":"Scala","text":""},{"location":"category/snowflake/","title":"Snowflake","text":""},{"location":"category/ml/","title":"ML","text":""},{"location":"category/k8s/","title":"k8s","text":""},{"location":"category/azure/","title":"Azure","text":""},{"location":"category/airflow/","title":"airflow","text":""},{"location":"page/2/","title":"Blog","text":""},{"location":"page/3/","title":"Blog","text":""},{"location":"page/4/","title":"Blog","text":""},{"location":"2025/page/2/","title":"2025","text":""},{"location":"category/llm/page/2/","title":"LLM","text":""}]}