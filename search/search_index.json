{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"#blog","title":"Blog","text":"<ul> <li> <p> Open Source</p> <p> Repos   Index</p> </li> </ul>"},{"location":"2020/02/airflow/","title":"Airflow","text":"","tags":["Data Engineer"]},{"location":"2020/02/airflow/#code-snippet","title":"Code snippet","text":"<pre><code>import airflow\nfrom airflow.models import DAG\nfrom airflow.operators.python_operator import PythonOperator\n\ndefault_args = {\n    'owner': 'ABC',\n    'start_date': airflow.utils.dates.days_ago(1),\n    'depends_on_past': False,\n    # failure email\n    'email': ['abc@xxx.com'],\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'pool': 'data_hadoop_pool',\n    'priority_weight': 900,\n    'queue': '66.66.0.66:8080'\n}\n\ndag = DAG(\n    dag_id='daily', \n    default_args=default_args,\n    schedule_interval='0 13 * * *')\n\ndef fetch_data_from_hdfs_function(ds, **kwargs):\n    pass\n\ndef push_data_to_mysql_function(ds, **kwargs):\n    pass\n\nfetch_data_from_hdfs = PythonOperator(\n    task_id='fetch_data_from_hdfs',\n    provide_context=True,\n    python_callable=fetch_data_from_hdfs_function,\n    dag=dag)\n\npush_data_to_mysql = PythonOperator(\n    task_id='push_data_to_mysql',\n    provide_context=True,\n    python_callable=push_data_to_mysql_function,\n    dag=dag)\n\nfetch_data_from_hdfs &gt;&gt; push_data_to_mysql\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/airflow/#update","title":"update","text":"<pre><code>#default parameters\nfetch_data_from_hdfs = PythonOperator(\n    task_id='fetch_data_from_hdfs',\n    provide_context=True,\n    python_callable=fetch_data_from_hdfs_function,\n    dag=dag)\n\n#overwrite parameters\npush_data_to_mysql = PythonOperator(\n    task_id='push_data_to_mysql',\n    queue='77.66.0.66:8080', #update\n    pool='data_mysql_pool', #update\n    provide_context=True,\n    python_callable=push_data_to_mysql_function,\n    dag=dag)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/airflow/#decouple","title":"decouple","text":"<pre><code>import xx.fetch_data_from_hdfs \n\ndef fetch_data_from_hdfs_function(ds, **kwargs):\n    if not fetch_data_from_hdfs: \n        raise AirflowException('run fail: fetch_data_from_hdfs')\n\nfetch_data_from_hdfs = PythonOperator(\n    task_id='fetch_data_from_hdfs',\n    provide_context=True,\n    python_callable=fetch_data_from_hdfs_function,\n    dag=dag)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/","title":"Azure Data Factory (Data Flow)","text":"<p>Recently I'm working in Azure to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work.</p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#task1","title":"Task1","text":"<p>Process CSV files and merge different system files into one file</p> <ul> <li>Source: CSV files with filename format (abcd_yyyymmdd_uuid.csv), where abcd is system id.<ul> <li>a_20180101_9ca2bed1-2ed0-eaeb-8401-784f43755025.csv</li> <li>a_20180101_cca2bed1-aed0-11eb-8401-784f73755025.csv</li> <li>b_20190202_ece2bed1-2ed0-abeb-8401-784f43755025.csv</li> <li>c_20180101_ada2bed1-2ed0-22eb-8401-784f43755025.csv</li> </ul> </li> <li>Sink: yyyymmdd.csv<ul> <li>20180101.csv</li> <li>20190202.csv</li> </ul> </li> </ul>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#adf-pipeline","title":"ADF Pipeline","text":"","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#activities","title":"Activities","text":"","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#get-metadata","title":"Get Metadata","text":"<ul> <li>Input: source directory/parameters</li> <li>Output: metadata of each object</li> </ul> <p>Get Metadata activity iterate source directory to obtain each object. The most important one is Argument </p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#foreach","title":"ForEach","text":"<ul> <li>Input: output of Get Metadata</li> <li>Output: None</li> </ul> <p>ForEach activity is used to process each object in source direcoty.</p> <p></p><pre><code>@activity('Get Metadata1').output.childItems\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#set-variables","title":"Set Variables","text":"<p>It's convenient to predefine a value used in next step.</p> <p></p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#dataflow","title":"Dataflow","text":"<p>The dataflow merge all files with same date, and source1 and sink are the same destination. So, initially source1 is empty and check this options. </p> <p>The only configuration in Sink is the  File name option </p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#aggregation-of-filenames","title":"Aggregation of filenames","text":"<p>The last problem in dataflow is how to merge files with same date in dataflow, which means we firstly find out all these files. The solution to this problems is regex expression.</p> <p></p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#task2","title":"Task2","text":"<p>Generally CSV file has a header and we can process it easily in ADF. However, a special case is a large CSV file has multiple different headers and we need to automatically split it into regular csv files with headers respectively.</p> <ul> <li> <p>Sample data:     &gt; h1,h1_col1,h1_col2,h1_col3     &gt; h2,h2_col1,h2_col2,h2_col3,h2_col4,h2_col5     &gt; h3,h3_col1,h3_col2     &gt; h1,h1_row1_1,h1_row1_2,h1_row1_3     &gt; h1,h1_row2_1,h1_row2_2,h1_row2_3     &gt; h1,h1_row3_1,h1_row3_2,h1_row3_3     &gt; h2,h2_row1_1,h2_row1_2,h2_row1_3,h2_row1_4,h2_row1_5     &gt; h2,h2_row2_1,h2_row2_2,h2_row2_3,h2_row2_4,h2_row2_5     &gt; h2,h2_row3_1,h2_row3_2,h2_row3_3,h2_row3_4,h2_row3_5     &gt; h2,h2_row4_1,h2_row4_2,h2_row4_3,h2_row4_4,h2_row4_5     &gt; h2,h2_row5_1,h2_row5_2,h2_row5_3,h2_row5_4,h2_row5_5     &gt; h3,h3_row1_1,h3_row1_2     &gt; h3,h3_row2_1,h3_row2_2</p> </li> <li> <p>Explanation:</p> <ul> <li>header format: header name, columns names</li> <li>3 headers : h1, h2 and h3</li> <li>the 1<sup>st</sup> column of each row is header name and rest of columns are values</li> </ul> </li> <li> <p>Output:</p> <ul> <li>h1 file     &gt; h1_col1,h1_col2,h1_col3     &gt; h1_row1_1,h1_row1_2,h1_row1_3     &gt; h1_row2_1,h1_row2_2,h1_row2_3     &gt; h1_row3_1,h1_row3_2,h1_row3_3</li> <li>h2 file     &gt; h2_col1,h2_col2,h2_col3,h2_col4,h2_col5     &gt; h2_row1_1,h2_row1_2,h2_row1_3,h2_row1_4,h2_row1_5     &gt; h2_row2_1,h2_row2_2,h2_row2_3,h2_row2_4,h2_row2_5     &gt; h2_row3_1,h2_row3_2,h2_row3_3,h2_row3_4,h2_row3_5     &gt; h2_row4_1,h2_row4_2,h2_row4_3,h2_row4_4,h2_row4_5     &gt; h2_row5_1,h2_row5_2,h2_row5_3,h2_row5_4,h2_row5_5</li> <li>h3 file     &gt; h3_col1,h3_col2     &gt; h3_row1_1,h3_row1_2     &gt; h3_row2_1,h3_row2_2</li> </ul> </li> </ul>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#dataflow_1","title":"Dataflow","text":"<p>The dataset used in source and sink must uncheck this </p> <p></p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#derivedcolumn","title":"DerivedColumn","text":"<p>Because no header is in the dataset, ADF automatically assign a column name to each one. The column name format is _colindex_</p> <p>In this task the header column is _col0_ and we can map this one to another name like filename </p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#sink","title":"Sink","text":"<p>This dataflow will automatically split composite CSV file into different files and save them at container root path. To save them at another directory, you can add folder name to the mapping column name in DerivedColumn activity.</p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#trigger","title":"Trigger","text":"<p>We use blob event trigger to implement automation. Once uploading a new file is done, these pipeline will process it automatically. How to create event trigger</p> <p>Two values in trigger are used by pipeline - @triggerBody().folderPath : /container name/folder/ - @triggerBody().fileName : blob name</p>","tags":["Data Engineer"]},{"location":"2020/11/azure-data-factory-data-flow/#pandas-processing","title":"Pandas Processing","text":"<pre><code>import pandas as pd\nimport csv\n\ndf = pd.read_csv('sample.csv', sep='^([^,]+),',engine='python', header=None)\ndf.drop(df.columns[0], axis=1, inplace=True)\n\nheads = df[df.columns[0]].unique()\nd = dict(tuple(df.groupby(df.columns[0])))\n\nfor h in heads:\n    outputfile = d[h]\n    outputfile.drop(outputfile.columns[0], axis=1, inplace=True)\n    outputfile.to_csv('{0}.csv'.format(h), sep=' ', index=False, header=False)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/batch-normalization/","title":"Batch Normalization","text":"<p>Batch Normalization is one of important parts in our NN.</p>"},{"location":"2020/02/batch-normalization/#why-need-normalization","title":"Why need Normalization","text":"<p>This paper title tells me the reason Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift - accelerating traning - reduce internal covariate shift</p>"},{"location":"2020/02/batch-normalization/#independent-and-identically-distributed-iid","title":"Independent and identically distributed (IID)","text":"<p>If our data is independent and identically distributed, training model can be simplified and its predictive ability is improved. One important step of data preparation is whitening which is used to</p>"},{"location":"2020/02/batch-normalization/#whitening","title":"Whitening","text":"<ul> <li>reduce features' coralation     =&gt; Independent</li> <li>all features have zero mean and unit variances =&gt; Identically distributed</li> </ul>"},{"location":"2020/02/batch-normalization/#internal-covariate-shift-ics","title":"Internal Covariate Shift (ICS)","text":"<p>What is problem of ICS? Generally data is not IID - Previous layer should update hyper-parameters to adjust new data so that reduce learning speed - Get stuck in the saturation region as the network grows deeper and network stop learning earlier</p>"},{"location":"2020/02/batch-normalization/#covariate-shift","title":"Covariate Shift","text":"<p>What is covariate shift? While in the process \\(X \\rightarrow Y\\) \\(\\(P^{train}(y|x) = P^{test}(y|x)\\)\\) \\(\\(but\\; P^{train}(x) \\neq P^{test}(x)\\)\\)</p>"},{"location":"2020/02/batch-normalization/#todo","title":"ToDo","text":""},{"location":"2020/02/batch-normalization/#normalizations","title":"Normalizations","text":"<ul> <li>weight scale invariance</li> <li>data scale invariance</li> </ul>"},{"location":"2020/02/batch-normalization/#batch-normalization","title":"Batch Normalization","text":""},{"location":"2020/02/batch-normalization/#layer-normalization","title":"Layer Normalization","text":""},{"location":"2020/02/batch-normalization/#weight-normalization","title":"Weight Normalization","text":""},{"location":"2020/02/batch-normalization/#cosine-normalization","title":"Cosine Normalization","text":""},{"location":"2020/02/gradient-descent/","title":"Gradient Descent","text":""},{"location":"2020/02/gradient-descent/#gradient-based-optimization-algorithms","title":"gradient-based optimization algorithms","text":""},{"location":"2020/02/gradient-descent/#gradient-descent-variants","title":"Gradient Descent variants","text":""},{"location":"2020/02/gradient-descent/#batch-gradient-descent-bgd","title":"Batch Gradient Descent (BGD)","text":"<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters \u03b8</p> <p>Batch gradient descent is guaranteed to converge  - to the global minimum for convex error surfaces - to a local minimum for non-convex surfaces</p>"},{"location":"2020/02/gradient-descent/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, - enables it to jump to new and potentially better local minima - this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting</p> <p>when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.</p>"},{"location":"2020/02/gradient-descent/#mini-batch-gradient-descent-mb-gd","title":"Mini-batch Gradient Descent (MB-GD)","text":"<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples</p> <ul> <li>reduces the variance of the parameter updates, which can lead to more stable convergence</li> <li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient</li> <li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li> </ul>"},{"location":"2020/02/gradient-descent/#challenges","title":"Challenges","text":"<ul> <li> <p>Choosing a proper learning rate can be difficult.</p> <p>A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p> </li> <li> <p>Learning rete schedules try to adjust the learning rate during training</p> <p>e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics</p> </li> <li> <p>The same learning rate applies to all parameter updates</p> <p>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features</p> </li> <li> <p>Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima</p> <p>The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p> </li> </ul>"},{"location":"2020/02/gradient-descent/#gradient-descent-optimization-algorithms","title":"Gradient Descent Optimization Algorithms","text":"<p>We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton's method.</p>"},{"location":"2020/02/gradient-descent/#momentum","title":"Momentum","text":"<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima.</p> <p>Some implementations exchange the signs in the equations. The momentum term \u03b3 is usually set to 0.9 or a similar value.</p> <p>When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill,  becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \u03b3&lt;1).  The same thing happens to our parameter updates: </p> <p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>"},{"location":"2020/02/gradient-descent/#nesterov-accelerated-gradient","title":"Nesterov Accelerated Gradient","text":"<p>We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again. Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience.  We know that we will use our momentum term \u03b3v\u03b8<sub>t-1</sub> to move the parameters \u03b8.  Computing \u03b8\u2212\u03b3v<sub>t-1</sub> thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update),  a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient  not w.r.t. to our current parameters \u03b8 but w.r.t. the approximate future position of our parameters</p> <p>we are able to adapt our updates to the slope of our error function and speed up SGD in turn,  we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance</p> <p>The distinction between Momentum method and Nesterov Accelerated Gradient updates was - Both methods are distinct only when the learning rate \u03b7 is reasonably large.  - When the learning rate \u03b7 is relatively large, Nesterov Accelerated Gradients allows larger decay rate \u03b1 than Momentum method, while preventing oscillations.  - Both Momentum method and Nesterov Accelerated Gradient become equivalent when \u03b7 is small</p>"},{"location":"2020/02/gradient-descent/#adagrad","title":"Adagrad","text":"<p>Adagrad is an algorithm for gradient-based optimization that does just this:  It adapts the learning rate to the parameters,  - performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features,  - and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</p> <p>For this reason, it is well-suited for dealing with sparse data.</p> <p>Previously, we performed an update for all parameters \u03b8 at once as every parameter \u03b8<sub>i</sub> used the same learning rate \u03b7.  As Adagrad uses a different learning rate for every parameter \u03b8<sub>i</sub> at every time step t, we first show Adagrad's per-parameter update, which we then vectorize. For brevity, we use gt to denote the gradient at time step t. g<sub>t,i</sub> is then the partial derivative of the objective function w.r.t. to the parameter \u03b8<sub>i</sub> at time step t</p> <p>In its update rule, Adagrad modifies the general learning rate \u03b7 at each time step t for every parameter \u03b8<sub>i</sub> based on the past gradients that have been computed for \u03b8<sub>i</sub></p> <p>\u03b8<sub>t+1,i</sub>=\u03b8<sub>t,i</sub>\u2212\u03b7/\u221a(G<sub>t,ii</sub>+\u03f5)\u22c5g<sub>t,i</sub></p> <p>G<sub>t</sub>\u2208R<sup>d\u00d7d</sup> here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. \u03b8<sub>i</sub> up to time step t, while \u03f5 is a smoothing term that avoids division by zero.  Interestingly, without the square root operation, the algorithm performs much worse.</p> <ul> <li>One of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate</li> <li>Adagrad's main weakness is its accumulation of the squared gradients in the denominator <p>Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p> </li> </ul>"},{"location":"2020/02/gradient-descent/#adadelta","title":"Adadelta","text":"<p>Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p> <p>Instead of inefficiently storing w previous squared gradients,  the sum of gradients is recursively defined as a decaying average of all past squared gradients. </p>"},{"location":"2024/09/how-to-execute-python-modules/","title":"How to execute python modules","text":""},{"location":"2024/09/how-to-execute-python-modules/#runpy-module","title":"runpy module","text":"<p>We can use internal <code>runpy</code> to execute different moduls in our project.</p> <p>This is used in my pyspark project.</p> submit.py<pre><code>import runpy\nimport sys\n\nif __name__ == '__main__':\n    module_name = sys.argv[1]\n    function_name = sys.argv[2]\n    sys.argv = sys.argv[2:] # this is important for next python entry point\n    runpy.run_module(module_name, run_name=function_name)\n</code></pre> <p>Now, the spark job can be invoked by</p> <pre><code>spark-submit submit.py \"&lt;module_name&gt;\" \"&lt;function_name&gt;\"\n</code></pre> <p>Also, we can wrapper this shell command into a script.</p> run.sh<pre><code>module_name=$1\nfunction_name=$2\nspark-submit submit.py \"$module_name\" \"$function_name\" \"${@:3}\"\n</code></pre>"},{"location":"2021/07/setup-minikube/","title":"Setup Minikube","text":"","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#setup-minikube","title":"Setup Minikube","text":"","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#installation","title":"Installation","text":"<pre><code>brew upgrade \nbrew install minikube\nbrew install kubectl\n# minikube kubectl -- get pods -A\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#startstop-cluster","title":"Start/Stop Cluster","text":"<pre><code>#minikube start\nminikube start --driver=hyperkit\nminikube stop\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#interact-with-cluster","title":"Interact with Cluster","text":"<pre><code>minikube dashboard --alsologtostderr\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#deploy-application","title":"Deploy application","text":"<pre><code>kubectl create deployment balanced --image=k8s.gcr.io/echoserver:1.4  \nkubectl expose deployment balanced --type=LoadBalancer --port=8080\n</code></pre>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#spark-on-k8s","title":"Spark on K8s","text":"<p>Official Reference</p>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#build-spark-image","title":"Build Spark image","text":"<ul> <li>brew install spark</li> <li>build image via docker-image-tool.sh <pre><code>sh /usr/local/Cellar/apache-spark/3.1.2/bin/docker-image-tool.sh -m -t spark-docker build\n</code></pre></li> <li>check image <pre><code>minikube ssh\ndocker image ls\n</code></pre> </li> </ul>","tags":["Data Engineer"]},{"location":"2021/07/setup-minikube/#submit-a-sample-job","title":"Submit a sample job","text":"<ul> <li>Find master node of K8s <pre><code>kubectl cluster-info\n</code></pre></li> <li>submit job <pre><code># spark-submit --master k8s://https://192.168.64.2:8443 --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=3 --conf spark.kubernetes.container.image=gcr.io/spark-operator/spark:v2.4.5 --conf spark.kubernetes.namespace=default local:///usr/local/opt/apache-spark/libexec/examples/jars/spark-examples_2.12-2.4.5.jar\n\nbin/spark-submit \\\n--master k8s://https://192.168.99.100:8443 \\\n--deploy-mode cluster \\\n--name spark-pi \\\n--class org.apache.spark.examples.SparkPi \\\n--conf spark.driver.cores=1 \\\n--conf spark.driver.memory=512m \\\n--conf spark.executor.instances=2 \\\n--conf spark.executor.memory=512m \\\n--conf spark.executor.cores=1 \\\n--conf spark.kubernetes.container.image=gcr.io/spark-operator/spark:v2.4.5 \\\n--conf spark.kubernetes.container.image.pullPolicy=IfNotPresent \\\n--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\nlocal:///opt/spark/examples/jars/spark-examples_2.11-2.4.5.jar\n</code></pre></li> </ul>","tags":["Data Engineer"]},{"location":"2024/08/model-registry/","title":"Model Registry","text":"<p>Problem: How to introduce ml-based production/features to cross-functional teams.</p> <p>Question: - Where can we find the best version of this model?   - audit   - test   - deploy   - reuse - How was this model trained? - How can we track this docs for each models? - How can we review models? - How can we integrate with tools and services?</p>"},{"location":"2024/08/model-registry/#model-registry","title":"Model Registry","text":"<p>An model registry serves as a centralized repository, enabling effective model management and documentation.</p>"},{"location":"2024/10/snowflake-data-science-training-summary/","title":"Snowflake Data Science Training Summary","text":"","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#snowflake-data-science","title":"Snowflake Data Science","text":"<p>I have enrolled in a private Snowflake Data Science Training. Let me list what I learned from it.</p> <ul> <li>SQL worksheets</li> <li>Snowpark in notebook</li> </ul> <pre><code>mkdocs build\nmkdocs serve\nmkdocs gh-deploy --force\n</code></pre>","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#sql-worksheets","title":"SQL Worksheets","text":"<p>ML functions:</p> <ul> <li>forecast</li> <li>anomaly_detection</li> <li>classification</li> <li>top_insights</li> </ul>","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#add-object-name-into-session","title":"Add object name into session","text":"<pre><code>show parameters like 'SEARCH_PATH';\n\nset cur_search_path = (select \"value\" from table(result_scan(-1)));\nset new_search_path = (select $cur_search_path || ', snowflake.ml'); -- append `snowflake.ml` into search_path\n\nalter session set search_path = $new_search_path;\n\n-- now below two statements are interchangeable \nshow snowflake.ml.forecast;\nshow forecast;\n</code></pre>","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#snowpark-notebook","title":"Snowpark Notebook","text":"","tags":["Data Science"]},{"location":"2024/10/snowflake-data-science-training-summary/#snowpark-configuration-snowflak-spark-configuration","title":"Snowpark Configuration &amp; Snowflak-Spark Configuration","text":"<p> Attributes are different</p> <ul> <li>Snowpark config   <pre><code>{\n  \"account\":\"\",\n  \"user\":\"\",\n  \"authenticator\":\"externalbrowser\",\n  \"role\":\"\",\n  \"warehouse\":\"\",\n  \"database\":\"\",\n  \"schema\":\"\"\n}\n</code></pre></li> <li>Snowflake Spark config   <pre><code>{\n  \"sfURL\":\"\",\n  \"sfRole\":\"\",\n  \"sfWarehouse\":\"\",\n  \"sfDatabase\":\"\",\n  \"sfSchema\":\"\",\n  \"sfUser\":\"\",\n  \"sfPassword\":\"\",\n  \"authenticator\":\"externalbrowser\",\n}\n</code></pre></li> </ul>","tags":["Data Science"]},{"location":"2020/03/spark-dataframe-window-function/","title":"Spark Dataframe window function","text":"<p>scala ref</p>","tags":["Data Engineer"]},{"location":"2020/03/spark-dataframe-window-function/#create-dataframe","title":"create dataframe","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/","title":"Spark Optimization","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#spark-run-faster-and-faster","title":"Spark run faster and faster","text":"<ul> <li>Cluster Optimization</li> <li>Parameters Optimization</li> <li>Code Optimization</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#cluster-optimization","title":"Cluster Optimization","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#locality-level","title":"Locality Level","text":"<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data\u2019s current location. In order from closest to farthest:</p> <ul> <li>PROCESS_LOCAL data is in the same JVM as the running code. This is the best locality possible</li> <li>NODE_LOCAL data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li> <li>NO_PREF data is accessed equally quickly from anywhere and has no locality preference</li> <li>RACK_LOCAL data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li> <li>ANY data is elsewhere on the network and not in the same rack</li> </ul> <p>Performance: PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#locality-settting","title":"Locality settting","text":"<ul> <li>spark.locality.wait.process</li> <li>spark.locality.wait.node</li> <li>spark.locality.wait.rack</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#data-format","title":"Data Format","text":"<ul> <li>text</li> <li>orc</li> <li>parquet</li> <li>avro</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#format-setting","title":"format setting","text":"<ul> <li>spark.sql.hive.convertCTAS</li> <li>spark.sql.sources.default</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#parallelising","title":"parallelising","text":"<ul> <li>spark.sql.shuffle.partitions : default is 200</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#computing","title":"computing","text":"<ul> <li>--executor-memory : default is 1G</li> <li>--executor-cores : default is 1 if large memory cause resource throtle in cluster, if small memory cause task termination if more cores cause IO issue, if less cores slow dow computing</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#memory","title":"memory","text":"<ul> <li>spark.executor.overhead.memory</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#table-join","title":"table join","text":"<ul> <li>spark.sql.autoBroadcastJoinThreshold : default 10M</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#predicate-push-down-in-spark-sql-queries","title":"predicate push down in Spark SQL queries","text":"<ul> <li>spark.sql.parquet.filterPushdown : default True</li> <li>spark.sql.orc.filterPushdown=true : default False</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#reuse-rdd","title":"reuse RDD","text":"<pre><code>    df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#spark-operators","title":"Spark operators","text":"<ul> <li>shuffle operators</li> <li>avoid using  reduceByKey, join, distinct, repartition etc</li> <li> <p>Broadcast small dataset</p> </li> <li> <p>High performance operator</p> </li> <li>reduceByKey &gt; groupByKey (reduceByKey works at map side)</li> <li>mapPartitions &gt; map (reduce function calls)</li> <li>treeReduce &gt; reduce (treeReduce works at executor not driver)<ul> <li>treeReduce &amp; reduce return some result to driver</li> <li>treeReduce does more work on the executors while reduce bring everything back to the driver.</li> </ul> </li> <li>foreachPartitions &gt; foreach (reduce function calls)</li> <li>filter -&gt; coalesce (reduce number of partitions and reduce tasks)</li> <li>repartitionAndSortWithinPartitions &gt; repartition &amp; sort</li> <li>broadcast (100M)</li> </ul>","tags":["Data Engineer"]},{"location":"2020/02/spark-optimization/#shuffle","title":"shuffle","text":"<ul> <li>spark.shuffle.sort.bypassMergeThreshold</li> <li>spark.shuffle.io.retryWait</li> <li>spark.shuffle.io.maxRetries</li> </ul> <p>TBC</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/","title":"Spark Structured Streaming","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#spark-structured-streaming","title":"Spark Structured Streaming","text":"<p>Recently reading a blog Structured Streaming in PySpark It's implemented in Databricks platform. Then I try to implement in my local Spark. Some tricky issue happened during my work.</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#reading-data","title":"Reading Data","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField\n\nspark = SparkSession.builder.appName(\"Test Streaming\").enableHiveSupport().getOrCreate()\n\njson_schema = StructType([\n    StructField(\"time\", TimestampType(), True),\n    StructField(\"customer\", StringType(), True),\n    StructField(\"action\", StringType(), True),\n    StructField(\"device\", StringType(), True)\n])\n\nfile_path = \"local_file_path&lt;file:///...\"\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#read-json-as-same-as-method-in-the-blog","title":"read json as same as method in the blog","text":"<pre><code>input = spark.read.schema(json_schema).json(file_path)\n\ninput.show()\n# +----+--------+------+------+\n# |time|customer|action|device|\n# +----+--------+------+------+\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# +----+--------+------+------+\ninput.count()\n# 20000\n</code></pre> All values are null, however, the count is right. It means spark has already read all data but the schema is not correctly mapped.","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#read-a-single-json-file-to-check-schema","title":"read a single json file to check schema","text":"<pre><code>input = spark.read.schema(json_schema).json(file_path+'/1.json')\n\ninput.show()\n\n# +----+--------+------+------+\n# |time|customer|action|device|\n# +----+--------+------+------+\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# |null|    null|  null|  null|\n# +----+--------+------+------+\n\n# same error\n# Then I drop schema option and use inferSchema\ninput = spark.read.json(file_path+'/1.json')\n\ninput.show()\n\n# +--------------------+-----------+-----------------+--------------------+---------------+\n# |     _corrupt_record|     action|         customer|              device|           time|\n# +--------------------+-----------+-----------------+--------------------+---------------+\n# |[{\"time\":\"3:57:09...|       null|             null|                null|           null|\n# |                null|  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|\n# |                null|   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|\n# |                null|  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|\n# |                null|  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|\n# |                null|  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|\n# |                null|low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|\n# |                null|   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|\n# |                null|  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|\n# |                null|  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|\n# |                null|   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|\n# |                null|low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|\n# |                null|low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|\n# |                null|  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|\n# |                null|   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|\n# |                null|  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|\n# |                null|   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|\n# |                null|low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|\n# |                null|  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|\n# |                null|low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|\n# +--------------------+-----------+-----------------+--------------------+---------------+\n</code></pre> A weird column is _corrupt_record and first value is [{\"time\":\"3:57:09... in this column. Go back to check source file and notice that it's a list of object in json file.","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#remove-and-in-source-file","title":"Remove   [  and ]  in source file","text":"<pre><code>input = spark.read.json(file_path+'/1.json')\n\ninput.show()\n\n# +-----------+-----------------+--------------------+---------------+\n# |     action|         customer|              device|           time|\n# +-----------+-----------------+--------------------+---------------+\n# |  power off|      Alexi Barts|  GreenIQ Controller| 3:57:09.000 PM|\n# |  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|\n# |   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|\n# |  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|\n# |  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|\n# |  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|\n# |low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|\n# |   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|\n# |  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|\n# |  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|\n# |   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|\n# |low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|\n# |low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|\n# |  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|\n# |   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|\n# |  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|\n# |   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|\n# |low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|\n# |  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|\n# |low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|\n# +-----------+-----------------+--------------------+---------------+\n</code></pre> Woo, the dataframe is correct. Let's check schema <pre><code>input.printSchema()\n# root\n#  |-- action: string (nullable = true)\n#  |-- customer: string (nullable = true)\n#  |-- device: string (nullable = true)\n#  |-- time: string (nullable = true)\n</code></pre> So far I manually modify source file and drop external schema to obtain a corret dataframe. Is there anyway to read these files without these steps.","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#add-one-feature-multiline","title":"add one feature multiLine","text":"<p>Read the file without schema but add one feature multiLine</p> <pre><code>input = spark.read.json(\"file:///path/pyspark_test_data\", multiLine=True)\n\n# OR input = spark.read.option('multiLine', True).json(\"file:///path/pyspark_test_data\")\n\n# +-----------+--------------------+--------------------+---------------+\n# |     action|            customer|              device|           time|\n# +-----------+--------------------+--------------------+---------------+\n# |   power on|     Raynor Blaskett|Nest T3021US Ther...| 3:35:09.000 AM|\n# |   power on|Stafford Blakebrough|  GreenIQ Controller|10:59:46.000 AM|\n# |   power on|      Alex Woolcocks|Nest T3021US Ther...| 6:26:36.000 PM|\n# |   power on|      Clarice Nayshe|Footbot Air Quali...| 4:46:28.000 AM|\n# |  power off|      Killie Pirozzi|Footbot Air Quali...| 8:58:43.000 AM|\n# |   power on|    Lynne Dymidowicz|Footbot Air Quali...| 4:20:49.000 PM|\n# |   power on|       Shaina Dowyer|             ecobee4| 3:41:33.000 AM|\n# |low battery|       Barbee Melato| August Doorbell Cam|10:40:24.000 PM|\n# |  power off|        Clem Westcot|Nest T3021US Ther...|11:13:38.000 PM|\n# |  power off|       Kerri Galfour|         Amazon Echo|10:12:15.000 PM|\n# |low battery|        Trev Ashmore|  GreenIQ Controller|11:04:41.000 AM|\n# |   power on|      Coral Jahnisch| August Doorbell Cam| 3:06:31.000 AM|\n# |   power on|      Feliza Cowdrey|Nest T3021US Ther...| 2:49:02.000 AM|\n# |  power off|   Amabelle De Haven|Footbot Air Quali...|12:11:59.000 PM|\n# |  power off|     Benton Redbourn|Nest T3021US Ther...| 3:57:39.000 AM|\n# |low battery|        Asher Potten| August Doorbell Cam| 1:34:44.000 AM|\n# |low battery|    Lorianne Hullyer| August Doorbell Cam| 7:26:42.000 PM|\n# |  power off|     Ruperto Aldcorn|Footbot Air Quali...| 3:54:49.000 AM|\n# |   power on|   Agatha Di Giacomo|Footbot Air Quali...| 7:15:20.000 AM|\n# |   power on|    Eunice Penwright|             ecobee4|11:14:14.000 PM|\n# +-----------+--------------------+--------------------+---------------+\n\ninput.printSchema()\n\n# root\n#  |-- action: string (nullable = true)\n#  |-- customer: string (nullable = true)\n#  |-- device: string (nullable = true)\n#  |-- time: string (nullable = true)\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#change-the-schema","title":"change the schema","text":"<p>Set time as StringType </p><pre><code>json_schema = StructType([\n    StructField(\"time\", StringType(), True),\n    StructField(\"customer\", StringType(), True),\n    StructField(\"action\", StringType(), True),\n    StructField(\"device\", StringType(), True)\n])\n\n\ninput = spark.read.schema(json_schema).json(\"file:///path/pyspark_test_data\", multiLine=True)\n\ninput.show()\n\n# +---------------+--------------------+-----------+--------------------+\n# |           time|            customer|     action|              device|\n# +---------------+--------------------+-----------+--------------------+\n# | 3:35:09.000 AM|     Raynor Blaskett|   power on|Nest T3021US Ther...|\n# |10:59:46.000 AM|Stafford Blakebrough|   power on|  GreenIQ Controller|\n# | 6:26:36.000 PM|      Alex Woolcocks|   power on|Nest T3021US Ther...|\n# | 4:46:28.000 AM|      Clarice Nayshe|   power on|Footbot Air Quali...|\n# | 8:58:43.000 AM|      Killie Pirozzi|  power off|Footbot Air Quali...|\n# | 4:20:49.000 PM|    Lynne Dymidowicz|   power on|Footbot Air Quali...|\n# | 3:41:33.000 AM|       Shaina Dowyer|   power on|             ecobee4|\n# |10:40:24.000 PM|       Barbee Melato|low battery| August Doorbell Cam|\n# |11:13:38.000 PM|        Clem Westcot|  power off|Nest T3021US Ther...|\n# |10:12:15.000 PM|       Kerri Galfour|  power off|         Amazon Echo|\n# |11:04:41.000 AM|        Trev Ashmore|low battery|  GreenIQ Controller|\n# | 3:06:31.000 AM|      Coral Jahnisch|   power on| August Doorbell Cam|\n# | 2:49:02.000 AM|      Feliza Cowdrey|   power on|Nest T3021US Ther...|\n# |12:11:59.000 PM|   Amabelle De Haven|  power off|Footbot Air Quali...|\n# | 3:57:39.000 AM|     Benton Redbourn|  power off|Nest T3021US Ther...|\n# | 1:34:44.000 AM|        Asher Potten|low battery| August Doorbell Cam|\n# | 7:26:42.000 PM|    Lorianne Hullyer|low battery| August Doorbell Cam|\n# | 3:54:49.000 AM|     Ruperto Aldcorn|  power off|Footbot Air Quali...|\n# | 7:15:20.000 AM|   Agatha Di Giacomo|   power on|Footbot Air Quali...|\n# |11:14:14.000 PM|    Eunice Penwright|   power on|             ecobee4|\n# +---------------+--------------------+-----------+--------------------+\n</code></pre> Pyspark can load json files successfully without TimestampType. However, how to handle timestamp issue in this job?","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#timestamptype","title":"TimestampType","text":"<p>In offical document, the class pyspark.sql.DataFrameReader has one parameter - timestampFormat </p> <p>sets the string that indicates a timestamp format. </p> <p>Custom date formats follow the formats at java.text.SimpleDateFormat. </p> <p>This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd'T'HHss.SSSXXX.</p> <pre><code>input = spark.read.schema(schema).option(\"multiLine\", True).json(\"file:///path/pyspark_test_data\", timestampFormat=\"h:mm:ss.SSS aa\")\n\ninput.show()\n# +-------------------+--------------------+-----------+--------------------+\n# |               time|            customer|     action|              device|\n# +-------------------+--------------------+-----------+--------------------+\n# |1970-01-01 03:35:09|     Raynor Blaskett|   power on|Nest T3021US Ther...|\n# |1970-01-01 10:59:46|Stafford Blakebrough|   power on|  GreenIQ Controller|\n# |1970-01-01 18:26:36|      Alex Woolcocks|   power on|Nest T3021US Ther...|\n# |1970-01-01 04:46:28|      Clarice Nayshe|   power on|Footbot Air Quali...|\n# |1970-01-01 08:58:43|      Killie Pirozzi|  power off|Footbot Air Quali...|\n# |1970-01-01 16:20:49|    Lynne Dymidowicz|   power on|Footbot Air Quali...|\n# |1970-01-01 03:41:33|       Shaina Dowyer|   power on|             ecobee4|\n# |1970-01-01 22:40:24|       Barbee Melato|low battery| August Doorbell Cam|\n# |1970-01-01 23:13:38|        Clem Westcot|  power off|Nest T3021US Ther...|\n# |1970-01-01 22:12:15|       Kerri Galfour|  power off|         Amazon Echo|\n# |1970-01-01 11:04:41|        Trev Ashmore|low battery|  GreenIQ Controller|\n# |1970-01-01 03:06:31|      Coral Jahnisch|   power on| August Doorbell Cam|\n# |1970-01-01 02:49:02|      Feliza Cowdrey|   power on|Nest T3021US Ther...|\n# |1970-01-01 12:11:59|   Amabelle De Haven|  power off|Footbot Air Quali...|\n# |1970-01-01 03:57:39|     Benton Redbourn|  power off|Nest T3021US Ther...|\n# |1970-01-01 01:34:44|        Asher Potten|low battery| August Doorbell Cam|\n# |1970-01-01 19:26:42|    Lorianne Hullyer|low battery| August Doorbell Cam|\n# |1970-01-01 03:54:49|     Ruperto Aldcorn|  power off|Footbot Air Quali...|\n# |1970-01-01 07:15:20|   Agatha Di Giacomo|   power on|Footbot Air Quali...|\n# |1970-01-01 23:14:14|    Eunice Penwright|   power on|             ecobee4|\n# +-------------------+--------------------+-----------+--------------------+\n</code></pre> <p>All yyyy-MM-dd are 1970-01-01 because source file only hh-mm-ss.  These source files are in wrong format in Windows.</p>","tags":["Data Engineer"]},{"location":"2020/02/spark-structured-streaming/#streaming-our-data","title":"Streaming Our Data","text":"<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.types import TimestampType, StringType, StructType, StructField\n\n\nspark = SparkSession.builder.appName(\"Test Streaming\").enableHiveSupport().getOrCreate()\n\njson_schema = StructType([\n    StructField(\"time\", StringType(), True),\n    StructField(\"customer\", StringType(), True),\n    StructField(\"action\", StringType(), True),\n    StructField(\"device\", StringType(), True)\n])\n\nstreamingDF = spark.readStream.schema(json_schema) \\\n              .option(\"maxFilesPerTrigger\", 1) \\\n              .option(\"multiLine\", True) \\\n              .json(\"file:///path/pyspark_test_data\")\n\nstreamingActionCountsDF = streamingDF.groupBy('action').count()\n# streamingActionCountsDF.isStreaming\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n\n\n# View stream in real-time\n# query = streamingActionCountsDF.writeStream \\\n#         .format(\"memory\").queryName(\"counts\").outputMode(\"complete\").start()\n\n# format choice:\n# parquet\n# kafka\n# console\n# memory\n\n# query = streamingActionCountsDF.writeStream \\\n#         .format(\"console\").queryName(\"counts\").outputMode(\"complete\").start()\n\nquery = streamingActionCountsDF.writeStream.format(\"console\") \\\n        .queryName(\"counts\").outputMode(\"complete\").start().awaitTermination(timeout=10)\n# Output Mode choice:\n# append\n# complete\n# update\n</code></pre>","tags":["Data Engineer"]},{"location":"2020/02/whitening-transformation/","title":"Whitening Transformation","text":""},{"location":"2012/10/repo-list/","title":"Repo List","text":""},{"location":"2012/10/repo-list/#repos","title":"Repos","text":"Repo List language link"},{"location":"2020/02/spark-sql/","title":"Spark SQL","text":"","tags":["Data Engineer"]},{"location":"2020/02/spark-sql/#spark-submit-options","title":"Spark Submit options","text":"<pre><code>--master MASTER_URL --&gt; \u8fd0\u884c\u6a21\u5f0f\n    \u4f8b\uff1aspark://host:port, mesos://host:port, yarn, or local.\n\n--deploy-mode DEPLOY_MODE \n    Whether to launch the driver program locally (\"client\") or\n    on one of the worker machines inside the cluster (\"cluster\")\n    (Default: client).\n\n--class CLASS_NAME  --&gt; \u8fd0\u884c\u7a0b\u5e8f\u7684main_class \n    \u4f8b\uff1a com.jhon.hy.main.Test\n\n--name NAME --&gt;application \u7684\u540d\u5b57\n    \u4f8b\uff1amy_application\n\n--jars JARS  --&gt;\u9017\u53f7\u5206\u9694\u7684\u672c\u5730jar\u5305\uff0c\u5305\u542b\u5728driver\u548cexecutor\u7684classpath\u4e0b\n    \u4f8b\uff1a/home/app/python_app/python_v2/jars/datanucleus-api-jdo-3.2.6.jar,/home/app/python_app/python_v2/jars/datanucleus-core-3.2.10.jar,/home/app/python_app/python_v2/jars/datanucleus-rdbms-3.2.9.jar,/home/app/python_app/python_v2/jars/mysql-connector-java-5.1.37-bin.jar,/home/app/python_app/python_v2/jars/hive-bonc-plugin-2.0.0.jar\\\n\n--exclude-packages --&gt;\u7528\u9017\u53f7\u5206\u9694\u7684\u201dgroupId:artifactId\u201d\u5217\u8868\n\n--repositories  --&gt;\u9017\u53f7\u5206\u9694\u7684\u8fdc\u7a0b\u4ed3\u5e93\n\n--py-files PY_FILES  --&gt;\u9017\u53f7\u5206\u9694\u7684\u201d.zip\u201d,\u201d.egg\u201d\u6216\u8005\u201c.py\u201d\u6587\u4ef6\uff0c\u8fd9\u4e9b\u6587\u4ef6\u653e\u5728python app\u7684\n\n--files FILES    --&gt;\u9017\u53f7\u5206\u9694\u7684\u6587\u4ef6\uff0c\u8fd9\u4e9b\u6587\u4ef6\u653e\u5728\u6bcf\u4e2aexecutor\u7684\u5de5\u4f5c\u76ee\u5f55\u4e0b\u9762\uff0c\u6d89\u53ca\u5230\u7684k-vge\u683c\u5f0f\u7684\u53c2\u6570\uff0c\u7528 \u2018#\u2019 \u8fde\u63a5,\u5982\u679c\u6709\u81ea\u5b9a\u4e49\u7684log4j \u914d\u7f6e\uff0c\u4e5f\u653e\u5728\u6b64\u914d\u7f6e\u4e0b\u9762\n    \u4f8b\uff1a/home/app/python_app/python_v2/jars/kafka_producer.jar,/home/app/python_app/python_v2/resources/....cn.keytab#.....keytab,/home/app/python_app/python_v2/resources/app.conf#app.conf,/home/app/python_app/python_v2/resources/hive-site.xml,/home/app/python_app/python_v2/resources/kafka_client_jaas.conf#kafka_client_jaas.conf\n\n--properties-file FILE   --&gt; \u9ed8\u8ba4\u7684spark\u914d\u7f6e\u9879\uff0c\u9ed8\u8ba4\u8def\u5f84 conf/spark-defaults.conf\n\n--conf PROP=VALUE  --&gt;  \u4efb\u610f\u7684spark\u914d\u7f6e\u9879\n    \u4f8b\uff1a --conf \"spark.driver.maxResultSize=4g\" \\\n        --conf spark.sql.shuffle.partitions=2600 \\\n        --conf spark.default.parallelism=300 \\\n\n--driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n--driver-java-options       Extra Java options to pass to the driver.\n--driver-library-path       Extra library path entries to pass to the driver.\n--driver-class-path         Extra class path entries to pass to the driver. Note that\n                            jars added with --jars are automatically included in the\n                            classpath.\n\n--executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n\n--proxy-user NAME           User to impersonate when submitting the application.\n                            This argument does not work with --principal / --keytab.\n\n--help, -h                  Show this help message and exit.\n--verbose, -v               Print additional debug output.\n--version,                  Print the version of current Spark.\n\n Spark standalone with cluster deploy mode only:\n  --driver-cores NUM          Cores for driver (Default: 1).\n\n Spark standalone or Mesos with cluster deploy mode only:\n  --supervise                 If given, restarts the driver on failure.\n  --kill SUBMISSION_ID        If given, kills the driver specified.\n  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n\n Spark standalone and Mesos only:\n  --total-executor-cores NUM  Total cores for all executors.\n\n Spark standalone and YARN only:\n  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n                              or all available cores on the worker in standalone mode)\n\n YARN-only:\n  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                              (Default: 1).\n  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n  --num-executors NUM         Number of executors to launch (Default: 2).\n                              If dynamic allocation is enabled, the initial number of\n                              executors will be at least NUM.\n  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n                              working directory of each executor.\n  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n                              secure HDFS.\n  --keytab KEYTAB             The full path to the file that contains the keytab for the\n                              principal specified above. This keytab will be copied to\n                              the node running the Application Master via the Secure\n                              Distributed Cache, for renewing the login tickets and the\n                              delegation tokens periodically.\n\n\n\n--num-executors 30 \\    \u542f\u52a8\u7684executor\u6570\u91cf\u3002\u9ed8\u8ba4\u4e3a2\u3002\u5728yarn\u4e0b\u4f7f\u7528\n--executor-cores 4 \\    \u6bcf\u4e2aexecutor\u7684\u6838\u6570\u3002\u5728yarn\u6216\u8005standalone\u4e0b\u4f7f\u7528\n--driver-memory 8g \\    Driver\u5185\u5b58\uff0c\u9ed8\u8ba41G\n--executor-memory 16g   \u6bcf\u4e2aexecutor\u7684\u5185\u5b58\uff0c\u9ed8\u8ba4\u662f1G\n\n`\u901a\u5e38\u6211\u4eec\u8bb2\u7528\u4e86\u591a\u5c11\u8d44\u6e90\u662f\u6307: num-executor * executor-cores \u6838\u5fc3\u6570\uff0c--num-executors*--executor-memory \u5185\u5b58`\n\n`\u56e0\u73b0\u5728\u6240\u5728\u516c\u53f8\u7528\u7684\u662fspark on yarn \u6a21\u5f0f\uff0c\u4ee5\u4e0b\u6d89\u53ca\u8c03\u4f18\u4e3b\u8981\u9488\u5bf9\u76ee\u524d\u6240\u7528`\n\n--num-executors \u8fd9\u4e2a\u53c2\u6570\u51b3\u5b9a\u4e86\u4f60\u7684\u7a0b\u5e8f\u4f1a\u542f\u52a8\u591a\u5c11\u4e2aExecutor\u8fdb\u7a0b\u6765\u6267\u884c\uff0cYARN\u96c6\u7fa4\u7ba1\u7406\n                \u5668\u4f1a\u5c3d\u53ef\u80fd\u6309\u7167\u4f60\u7684\u8bbe\u7f6e\u6765\u5728\u96c6\u7fa4\u7684\u5404\u4e2a\u5de5\u4f5c\u8282\u70b9\u4e0a\uff0c\u542f\u52a8\u76f8\u5e94\u6570\u91cf\u7684Executor\n                \u8fdb\u7a0b\u3002\u5982\u679c\u5fd8\u8bb0\u8bbe\u7f6e\uff0c\u9ed8\u8ba4\u542f\u52a8\u4e24\u4e2a\uff0c\u8fd9\u6837\u4f60\u540e\u9762\u7533\u8bf7\u7684\u8d44\u6e90\u518d\u591a\uff0c\u4f60\u7684Spark\u7a0b\n                \u5e8f\u6267\u884c\u901f\u5ea6\u4e5f\u662f\u5f88\u6162\u7684\u3002\n    \u8c03\u4f18\u5efa\u8bae\uff1a \u8fd9\u4e2a\u8981\u6839\u636e\u4f60\u7a0b\u5e8f\u8fd0\u884c\u60c5\u51b5\uff0c\u4ee5\u53ca\u591a\u6b21\u6267\u884c\u7684\u7ed3\u8bba\u8fdb\u884c\u8c03\u4f18\uff0c\u592a\u591a\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8d44\n             \u6e90\uff0c\u592a\u5c11\uff0c\u5219\u4fdd\u8bc1\u4e0d\u4e86\u6548\u7387\u3002\n\n--executor-memory   \u8fd9\u4e2a\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u7684\u5185\u5b58\uff0cExecutor\u7684\u5185\u5b58\u5f88\u591a\u65f6\u5019\u51b3\n                    \u5b9a\u4e86Spark\u4f5c\u4e1a\u7684\u6027\u80fd\uff0c\u800c\u4e14\u8ddf\u5e38\u89c1\u7684JVM OOM\u4e5f\u6709\u76f4\u63a5\u8054\u7cfb\n    \u8c03\u4f18\u5efa\u8bae\uff1a\u53c2\u8003\u503c --&gt; 4~8G,\u907f\u514d\u7a0b\u5e8f\u5c06\u6574\u4e2a\u96c6\u7fa4\u7684\u8d44\u6e90\u5168\u90e8\u5360\u7528,\u9700\u8981\u5148\u770b\u4e00\u4e0b\u4f60\u961f\u5217\u7684\u6700\u5927\n            \u5185\u5b58\u9650\u5236\u662f\u591a\u5c11\uff0c\u5982\u679c\u662f\u516c\u7528\u4e00\u4e2a\u961f\u5217\uff0c\u4f60\u7684num-executors * executor-memory\n            \u6700\u597d\u4e0d\u8981\u8d85\u8fc7\u961f\u5217\u76841/3 ~ 1/2\n-- executor-cores\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u7684CPU core\u6570\u91cf\u3002\u8fd9\u4e2a\u53c2\u6570\u51b3\u5b9a\u4e86\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u5e76\u884c\u6267\u884ctask\u7ebf\u7a0b\u7684\u80fd\u529b\u3002\u56e0\u4e3a\u6bcf\u4e2aCPU core\u540c\u4e00\u65f6\u95f4\u53ea\u80fd\u6267\u884c\u4e00\u4e2a\n    task\u7ebf\u7a0b\uff0c\u56e0\u6b64\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u7684CPU core\u6570\u91cf\u8d8a\u591a\uff0c\u8d8a\u80fd\u591f\u5feb\u901f\u5730\u6267\u884c\u5b8c\u5206\u914d\u7ed9\u81ea\u5df1\u7684\u6240\u6709task\u7ebf\u7a0b\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    Executor\u7684CPU core\u6570\u91cf\u8bbe\u7f6e\u4e3a2~4\u4e2a\u8f83\u4e3a\u5408\u9002\u3002\u540c\u6837\u5f97\u6839\u636e\u4e0d\u540c\u90e8\u95e8\u7684\u8d44\u6e90\u961f\u5217\u6765\u5b9a\uff0c\u53ef\u4ee5\u770b\u770b\u81ea\u5df1\u7684\u8d44\u6e90\u961f\u5217\u7684\u6700\u5927CPU core\u9650\u5236\u662f\u591a\u5c11\uff0c\u518d\u4f9d\u636e\u8bbe\u7f6e\u7684\n    Executor\u6570\u91cf\uff0c\u6765\u51b3\u5b9a\u6bcf\u4e2aExecutor\u8fdb\u7a0b\u53ef\u4ee5\u5206\u914d\u5230\u51e0\u4e2aCPU core\u3002\u540c\u6837\u5efa\u8bae\uff0c\u5982\u679c\u662f\u8ddf\u4ed6\u4eba\u5171\u4eab\u8fd9\u4e2a\u961f\u5217\uff0c\u90a3\u4e48num-executors * executor-cores\u4e0d\u8981\u8d85\u8fc7\n    \u961f\u5217\u603bCPU core\u76841/3~1/2\u5de6\u53f3\u6bd4\u8f83\u5408\u9002\uff0c\u4e5f\u662f\u907f\u514d\u5f71\u54cd\u5176\u4ed6\u540c\u5b66\u7684\u4f5c\u4e1a\u8fd0\u884c\u3002\n\n--driver-memory\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6eDriver\u8fdb\u7a0b\u7684\u5185\u5b58\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    Driver\u7684\u5185\u5b58\u901a\u5e38\u6765\u8bf4\u4e0d\u8bbe\u7f6e\uff0c\u6216\u8005\u8bbe\u7f6e1G\u5de6\u53f3\u5e94\u8be5\u5c31\u591f\u4e86\u3002\u552f\u4e00\u9700\u8981\u6ce8\u610f\u7684\u4e00\u70b9\u662f\uff0c\u5982\u679c\u9700\u8981\u4f7f\u7528collect\u7b97\u5b50\u5c06RDD\u7684\u6570\u636e\u5168\u90e8\u62c9\u53d6\u5230Driver\u4e0a\u8fdb\u884c\u5904\u7406\uff0c\n    \u90a3\u4e48\u5fc5\u987b\u786e\u4fddDriver\u7684\u5185\u5b58\u8db3\u591f\u5927\uff0c\u5426\u5219\u4f1a\u51fa\u73b0OOM\u5185\u5b58\u6ea2\u51fa\u7684\u95ee\u9898\u3002\n\n--spark.default.parallelism\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u6bcf\u4e2astage\u7684\u9ed8\u8ba4task\u6570\u91cf\u3002\u8fd9\u4e2a\u53c2\u6570\u6781\u4e3a\u91cd\u8981\uff0c\u5982\u679c\u4e0d\u8bbe\u7f6e\u53ef\u80fd\u4f1a\u76f4\u63a5\u5f71\u54cd\u4f60\u7684Spark\u4f5c\u4e1a\u6027\u80fd\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    Spark\u4f5c\u4e1a\u7684\u9ed8\u8ba4task\u6570\u91cf\u4e3a500~1000\u4e2a\u8f83\u4e3a\u5408\u9002\u3002\u5f88\u591a\u540c\u5b66\u5e38\u72af\u7684\u4e00\u4e2a\u9519\u8bef\u5c31\u662f\u4e0d\u53bb\u8bbe\u7f6e\u8fd9\u4e2a\u53c2\u6570\uff0c\u90a3\u4e48\u6b64\u65f6\u5c31\u4f1a\u5bfc\u81f4Spark\u81ea\u5df1\u6839\u636e\u5e95\u5c42HDFS\u7684block\u6570\u91cf\n    \u6765\u8bbe\u7f6etask\u7684\u6570\u91cf\uff0c\u9ed8\u8ba4\u662f\u4e00\u4e2aHDFS block\u5bf9\u5e94\u4e00\u4e2atask\u3002\u901a\u5e38\u6765\u8bf4\uff0cSpark\u9ed8\u8ba4\u8bbe\u7f6e\u7684\u6570\u91cf\u662f\u504f\u5c11\u7684\uff08\u6bd4\u5982\u5c31\u51e0\u5341\u4e2atask\uff09\uff0c\u5982\u679ctask\u6570\u91cf\u504f\u5c11\u7684\u8bdd\uff0c\u5c31\u4f1a\n    \u5bfc\u81f4\u4f60\u524d\u9762\u8bbe\u7f6e\u597d\u7684Executor\u7684\u53c2\u6570\u90fd\u524d\u529f\u5c3d\u5f03\u3002\u8bd5\u60f3\u4e00\u4e0b\uff0c\u65e0\u8bba\u4f60\u7684Executor\u8fdb\u7a0b\u6709\u591a\u5c11\u4e2a\uff0c\u5185\u5b58\u548cCPU\u6709\u591a\u5927\uff0c\u4f46\u662ftask\u53ea\u67091\u4e2a\u6216\u800510\u4e2a\uff0c\u90a3\u4e4890%\u7684\n    Executor\u8fdb\u7a0b\u53ef\u80fd\u6839\u672c\u5c31\u6ca1\u6709task\u6267\u884c\uff0c\u4e5f\u5c31\u662f\u767d\u767d\u6d6a\u8d39\u4e86\u8d44\u6e90\uff01\u56e0\u6b64Spark\u5b98\u7f51\u5efa\u8bae\u7684\u8bbe\u7f6e\u539f\u5219\u662f\uff0c\u8bbe\u7f6e\u8be5\u53c2\u6570\u4e3anum-executors * executor-cores\u76842~3\u500d\n    \u8f83\u4e3a\u5408\u9002\uff0c\u6bd4\u5982Executor\u7684\u603bCPU core\u6570\u91cf\u4e3a300\u4e2a\uff0c\u90a3\u4e48\u8bbe\u7f6e1000\u4e2atask\u662f\u53ef\u4ee5\u7684\uff0c\u6b64\u65f6\u53ef\u4ee5\u5145\u5206\u5730\u5229\u7528Spark\u96c6\u7fa4\u7684\u8d44\u6e90\u3002\n\n--spark.storage.memoryFraction\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6eRDD\u6301\u4e45\u5316\u6570\u636e\u5728Executor\u5185\u5b58\u4e2d\u80fd\u5360\u7684\u6bd4\u4f8b\uff0c\u9ed8\u8ba4\u662f0.6\u3002\u4e5f\u5c31\u662f\u8bf4\uff0c\u9ed8\u8ba4Executor 60%\u7684\u5185\u5b58\uff0c\u53ef\u4ee5\u7528\u6765\u4fdd\u5b58\u6301\u4e45\u5316\u7684RDD\u6570\u636e\u3002\u6839\u636e\u4f60\u9009\u62e9\n    \u7684\u4e0d\u540c\u7684\u6301\u4e45\u5316\u7b56\u7565\uff0c\u5982\u679c\u5185\u5b58\u4e0d\u591f\u65f6\uff0c\u53ef\u80fd\u6570\u636e\u5c31\u4e0d\u4f1a\u6301\u4e45\u5316\uff0c\u6216\u8005\u6570\u636e\u4f1a\u5199\u5165\u78c1\u76d8\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    \u5982\u679cSpark\u4f5c\u4e1a\u4e2d\uff0c\u6709\u8f83\u591a\u7684RDD\u6301\u4e45\u5316\u64cd\u4f5c\uff0c\u8be5\u53c2\u6570\u7684\u503c\u53ef\u4ee5\u9002\u5f53\u63d0\u9ad8\u4e00\u4e9b\uff0c\u4fdd\u8bc1\u6301\u4e45\u5316\u7684\u6570\u636e\u80fd\u591f\u5bb9\u7eb3\u5728\u5185\u5b58\u4e2d\u3002\u907f\u514d\u5185\u5b58\u4e0d\u591f\u7f13\u5b58\u6240\u6709\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u6570\u636e\u53ea\n    \u80fd\u5199\u5165\u78c1\u76d8\u4e2d\uff0c\u964d\u4f4e\u4e86\u6027\u80fd\u3002\u4f46\u662f\u5982\u679cSpark\u4f5c\u4e1a\u4e2d\u7684shuffle\u7c7b\u64cd\u4f5c\u6bd4\u8f83\u591a\uff0c\u800c\u6301\u4e45\u5316\u64cd\u4f5c\u6bd4\u8f83\u5c11\uff0c\u90a3\u4e48\u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u9002\u5f53\u964d\u4f4e\u4e00\u4e9b\u6bd4\u8f83\u5408\u9002\u3002\u6b64\u5916\uff0c\u5982\u679c\u53d1\u73b0\n    \u4f5c\u4e1a\u7531\u4e8e\u9891\u7e41\u7684gc\u5bfc\u81f4\u8fd0\u884c\u7f13\u6162\uff08\u901a\u8fc7spark web ui\u53ef\u4ee5\u89c2\u5bdf\u5230\u4f5c\u4e1a\u7684gc\u8017\u65f6\uff09\uff0c\u610f\u5473\u7740task\u6267\u884c\u7528\u6237\u4ee3\u7801\u7684\u5185\u5b58\u4e0d\u591f\u7528\uff0c\u90a3\u4e48\u540c\u6837\u5efa\u8bae\u8c03\u4f4e\u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u3002\n\n--spark.shuffle.memoryFraction\n\u53c2\u6570\u8bf4\u660e\uff1a\n    \u8be5\u53c2\u6570\u7528\u4e8e\u8bbe\u7f6eshuffle\u8fc7\u7a0b\u4e2d\u4e00\u4e2atask\u62c9\u53d6\u5230\u4e0a\u4e2astage\u7684task\u7684\u8f93\u51fa\u540e\uff0c\u8fdb\u884c\u805a\u5408\u64cd\u4f5c\u65f6\u80fd\u591f\u4f7f\u7528\u7684Executor\u5185\u5b58\u7684\u6bd4\u4f8b\uff0c\u9ed8\u8ba4\u662f0.2\u3002\u4e5f\u5c31\u662f\u8bf4\uff0cExecutor\n    \u9ed8\u8ba4\u53ea\u670920%\u7684\u5185\u5b58\u7528\u6765\u8fdb\u884c\u8be5\u64cd\u4f5c\u3002shuffle\u64cd\u4f5c\u5728\u8fdb\u884c\u805a\u5408\u65f6\uff0c\u5982\u679c\u53d1\u73b0\u4f7f\u7528\u7684\u5185\u5b58\u8d85\u51fa\u4e86\u8fd9\u4e2a20%\u7684\u9650\u5236\uff0c\u90a3\u4e48\u591a\u4f59\u7684\u6570\u636e\u5c31\u4f1a\u6ea2\u5199\u5230\u78c1\u76d8\u6587\u4ef6\u4e2d\u53bb\uff0c\u6b64\u65f6\n    \u5c31\u4f1a\u6781\u5927\u5730\u964d\u4f4e\u6027\u80fd\u3002\n\u53c2\u6570\u8c03\u4f18\u5efa\u8bae\uff1a\n    \u5982\u679cSpark\u4f5c\u4e1a\u4e2d\u7684RDD\u6301\u4e45\u5316\u64cd\u4f5c\u8f83\u5c11\uff0cshuffle\u64cd\u4f5c\u8f83\u591a\u65f6\uff0c\u5efa\u8bae\u964d\u4f4e\u6301\u4e45\u5316\u64cd\u4f5c\u7684\u5185\u5b58\u5360\u6bd4\uff0c\u63d0\u9ad8shuffle\u64cd\u4f5c\u7684\u5185\u5b58\u5360\u6bd4\u6bd4\u4f8b\uff0c\u907f\u514dshuffle\u8fc7\u7a0b\u4e2d\u6570\u636e\u8fc7\u591a\n    \u65f6\u5185\u5b58\u4e0d\u591f\u7528\uff0c\u5fc5\u987b\u6ea2\u5199\u5230\u78c1\u76d8\u4e0a\uff0c\u964d\u4f4e\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5982\u679c\u53d1\u73b0\u4f5c\u4e1a\u7531\u4e8e\u9891\u7e41\u7684gc\u5bfc\u81f4\u8fd0\u884c\u7f13\u6162\uff0c\u610f\u5473\u7740task\u6267\u884c\u7528\u6237\u4ee3\u7801\u7684\u5185\u5b58\u4e0d\u591f\u7528\uff0c\u90a3\u4e48\u540c\u6837\u5efa\u8bae\u8c03\u4f4e\n    \u8fd9\u4e2a\u53c2\u6570\u7684\u503c\u3002\n\u8d44\u6e90\u53c2\u6570\u7684\u8c03\u4f18\uff0c\u6ca1\u6709\u4e00\u4e2a\u56fa\u5b9a\u7684\u503c\uff0c\u9700\u8981\u6839\u636e\u81ea\u5df1\u7684\u5b9e\u9645\u60c5\u51b5\uff08\u5305\u62ecSpark\u4f5c\u4e1a\u4e2d\u7684shuffle\u64cd\u4f5c\u6570\u91cf\u3001RDD\u6301\u4e45\u5316\u64cd\u4f5c\u6570\u91cf\u4ee5\u53caspark web ui\u4e2d\u663e\u793a\u7684\u4f5c\u4e1agc\u60c5\u51b5\uff09\uff0c\n\u5408\u7406\u5730\u8bbe\u7f6e\u4e0a\u8ff0\u53c2                \n</code></pre>","tags":["Data Engineer"]},{"location":"2024/09/autogen-httpclient/","title":"AutoGen HttpClient","text":""},{"location":"2024/09/autogen-httpclient/#httpclient","title":"HttpClient","text":"my_client.py<pre><code>import httpx\n\nclass MyHttpClient(httpx.Client):\n    def __deepcopy__(self, dummy):\n        return self\n</code></pre>"},{"location":"2024/09/autogen-httpclient/#autogen-llm-config","title":"AutoGen LLM Config","text":"<p>Now we can add our proxy config in our client, which will resolve connection issue. </p><pre><code>\"http_client\": MyHttpClient(verify=False, proxy=\"http://&lt;&gt;\")\n</code></pre>"},{"location":"2024/10/reflex-learning/","title":"Reflex Learning","text":""},{"location":"2024/10/reflex-learning/#reflex-pynecone","title":"Reflex (pynecone)","text":"<p>Reflex is a library to build full-stack web apps in pure Python.</p> <ul> <li>Repo</li> <li>Video  </li> </ul>"},{"location":"2024/10/zio/","title":"ZIO","text":""},{"location":"2024/10/zio/#scala-full-stack","title":"Scala Full Stack","text":"<p>Recently I read a Blog that introducing how to use scala framework ZIO to build an application.</p> <p>This video is helpful to understand it. </p>"},{"location":"2024/10/python-decorator/","title":"Python Decorator","text":""},{"location":"2024/10/python-decorator/#python-decorators","title":"Python decorators","text":"<p> Why we need decorator</p> <p> It will extend your function behaviors during runtime.</p> <p>For example, you already have a function <code>say_hi</code></p> <pre><code>def say_hi(name: str):\n    return f\"Hi! {name}\"\n</code></pre> <ul> <li>function name <code>say_hi</code></li> <li>parameter <code>name</code> and type <code>str</code></li> <li>one output <code>say_hi('Apple') =&gt; Hi! Apple</code></li> </ul> <p>Next we plan to add one introduction to the output, such as <code>Hi! Apple. I'm Bob</code>.</p> <ol> <li> <p>Modify your function </p><pre><code>def say_hi(name: str, my_name:str):\n    return f\"Hi! {name}. I'm {my_name}\"\n</code></pre> If this is only used once in our project, it\u2019s manageable.  However, modifying the function signature means that every instance of its use throughout the project must be updated, which is both tedious and time-consuming. </li> <li> <p>Use decorator </p><pre><code>def add_intro(my_name):\n    def dec(func):\n        def wrapper(name):\n            return func(name) + f\". I'm {my_name}\"\n        return wrapper\n    return dec\n\n@add_intro(\"Bob\")\ndef say_hi(name: str):\n    return f\"Hi! {name}\"\n</code></pre> <code>Function signature is not changed and function behavior is enriched</code> </li> </ol>"},{"location":"2024/10/python-decorator/#how-to-create-decorator","title":"How to create decorator","text":""},{"location":"2024/10/python-decorator/#decorator-function","title":"Decorator Function","text":"<p>Before starting decorator, we have to understand</p> <ul> <li>original function<ul> <li>function name</li> <li>function parameters and types</li> </ul> </li> <li>decorator function<ul> <li>extra parameter</li> <li>new features</li> </ul> </li> </ul> original function<pre><code>def hello(name:str) -&gt; str:\n    return f\"hello, {name}\"\n</code></pre> <p>We have an original function</p> <ul> <li>function name: <code>hello</code></li> <li>parameters: <code>name</code></li> <li>types: <code>str</code></li> </ul> <p>Now we can use these to build decorator function <code>my_dec</code> </p>my_dec<pre><code>def my_dec(func):\n    def wrapper(name:str):\n        return func(name)\n    return wrapper\n</code></pre> <p>Explanation:</p> <ul> <li><code>line 1</code><ul> <li>decorator name : <code>my_dec</code></li> <li>decorator parameter : <code>func</code> ( we did not use type hint here)</li> <li>it mean <code>my_dec</code> will decorate function <code>func</code></li> </ul> </li> <li><code>line 2</code><ul> <li>inner function <code>wrapper</code> ( any function names)</li> <li>inner function signature. It MUST be a superset of your original signature <p>e.g. only 1 parameter <code>name</code> at <code>hello</code> the wrapper function should include <code>name</code> at least it could be <code>wrapper(name)</code>, <code>wrapper(name, name1=None)</code>, <code>wrapper(name, *args, **kwargs)</code> <code>wrapper(*args, **kwargs)</code> etc.</p> </li> </ul> </li> <li><code>line 3</code><ul> <li><code>return value</code></li> </ul> </li> <li><code>line 4</code><ul> <li> return a function name <code>wrapper</code></li> </ul> </li> </ul> <p>Now the decorator is working as <code>func = my_dec(func)</code></p> <ul> <li>function IN : <code>func</code></li> <li>function OUT: <code>wrapper</code> and reassigned <code>wrapper</code> to <code>func</code></li> </ul>"},{"location":"2024/10/python-decorator/#decorator-class","title":"Decorator Class","text":"Decorator Class<pre><code>class DecoratorClass:\n    def __init__(self, decorator_param: str):\n        self.decorator_param = decorator_param\n\n    def __call__(self, func):\n        def wrapper(original_param):\n            \"\"\"wrapper doc\"\"\"\n            return func(original_param) + self.decorator_param\n        return wrapper\n\n@DecoratorClass(decorator_param=\"!!!\")\ndef hello_again(name: str):\n    \"\"\"original docStr\"\"\"\n    return f\"Hello {name}\"\n</code></pre> <p>it's obviously to understand how to setup decorator's parameters.</p>"},{"location":"2024/10/python-decorator/#built-in-python-decorator","title":"built-in python decorator","text":"<p>Each function in <code>python</code> has metadata</p> <ul> <li><code>__name__</code></li> <li><code>__doc__</code></li> </ul> <p>Either <code>function</code> or <code>class</code> cannot update decorator function metadata.</p> <pre><code>print(hello_again.__name__) # wrapper\n\nprint(hello_again.__doc__) # Wrapper Doc\n</code></pre> <p> How to update function metadata</p> <ul> <li>manually update     <pre><code>class DecoratorClass1:\ndef __init__(self, decorator_param: str):\n    self.decorator_param = decorator_param\n\ndef __call__(self, func):\n    def wrapper(original_param):\n        \"\"\"Wrapper Doc\"\"\"\n        return func(original_param) + self.decorator_param\n\n    # Manually update metadata\n    wrapper.__doc__ = func.__doc__\n    wrapper.__name__ = func.__name__\n\n    return wrapper\n</code></pre></li> <li>use python built-in <code>wrapper</code></li> </ul>"},{"location":"2024/11/databricks-wheel-job/","title":"Databricks Wheel Job","text":""},{"location":"2024/11/databricks-wheel-job/#databricks-jobs","title":"Databricks Jobs","text":"<p>Recently I successfully deploy my python wheel to Databricks Cluster. Here are some tips if you plan to deploy <code>pyspark</code>.</p> <ul> <li><code>pyspark</code> project</li> <li><code>pytest</code></li> </ul>"},{"location":"2024/11/databricks-wheel-job/#pyspark-project","title":"<code>pyspark</code> project","text":"<p>My previous spark project is <code>scala</code> based and I use <code>IDEA</code> to <code>compile</code> and <code>test</code> conveniently.</p> <p></p> <p><code>Databricks</code> Job nice UI save your time to create <code>JAR</code> job.</p> <p>This is official guide: Databricks Wheel Job</p> <p>What I did:</p> <ol> <li> <p>Initialize a python project     </p><pre><code># create python virtual environment\npython -m venv pyspark_venv\n\n# active your venv\nsource pyspark_venv/bin/activate\n\n# check your current python\nwhich python\n\n# install python lib\npip install uv ruff pyspark pytest wheel\n\n## if pip failed at proxy error\n## adding your proxy \n## --proxy http://proxy:port\n\n# create your project\nuv init --package &lt;your package name&gt;\n</code></pre> <p>After <code>uv</code> command complete, a nice python project is created. </p><pre><code>pyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 pyspark_app\n        \u2514\u2500\u2500 __init__.py\n</code></pre> </li> <li> <p> pyspark <code>entry point</code></p> <ul> <li>add one file <code>__main__.py</code> in pyspark_app</li> <li>modify <code>[project.scripts]</code> in <code>pyproject.toml</code> and this is <code>entry point</code> of Databricks job</li> </ul> <p>Now the project is </p><pre><code>pyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 pyspark_app\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 __main__.py\n</code></pre> </li> </ol>"},{"location":"2024/11/databricks-wheel-job/#pytest","title":"<code>pytest</code>","text":"<p>Please check your <code>pytest</code> installed. Let create a new package <code>test</code></p> <pre><code>pyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 pyspark_app\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 __main__.py\n        \u2514\u2500\u2500 test\n            \u251c\u2500\u2500 __init__.py\n            \u251c\u2500\u2500 conftest.py\n            \u2514\u2500\u2500 test_spark.py\n</code></pre> test_spark<pre><code>def test_spark(init_spark):\n    spark = init_spark\n    df = spark.range(10)\n    df.show()\n\n\"\"\" output\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/11/01 20:59:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nPASSED                                         [100%]+---+\n| id|\n+---+\n|  0|\n|  1|\n|  2|\n|  3|\n|  4|\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\"\"\"\n</code></pre> <p>Now you can work on your spark application with test</p>"},{"location":"2024/11/databricks-wheel-job/#wheel-file","title":"wheel file","text":"<p>Final step is building <code>wheel</code> file</p> <p></p><pre><code># 1. change your work directory to pyproject.toml\n# 2. run below command\npython -m build --wheel\n\n# project is now changing to\n\npyspark-app\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 build\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bdist.macosx-12.0-x86_64\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lib\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 pyspark_app\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __main__.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 test\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0             \u251c\u2500\u2500 conftest.py\n\u2502\u00a0\u00a0             \u2514\u2500\u2500 test_spark.py\n\u251c\u2500\u2500 dist\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 pyspark_app-0.1.0-py3-none-any.whl\n\u251c\u2500\u2500 pyproject.toml\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 pyspark_app\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __main__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 test\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 conftest.py\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 test_spark.py\n    \u2514\u2500\u2500 pyspark_app.egg-info\n        \u251c\u2500\u2500 PKG-INFO\n        \u251c\u2500\u2500 SOURCES.txt\n        \u251c\u2500\u2500 dependency_links.txt\n        \u251c\u2500\u2500 entry_points.txt\n        \u2514\u2500\u2500 top_level.txt\n</code></pre> Your wheel file is at line <code>20</code> <p>Go to view all at  Project template</p>"},{"location":"2024/11/pyspark-dataframe-transformation/","title":"PySpark Dataframe Transformation","text":""},{"location":"2024/11/pyspark-dataframe-transformation/#migration-from-scala-to-python","title":"Migration from <code>Scala</code> to <code>Python</code>","text":"<p>Migrating a history <code>Scala</code> project to <code>Python</code>, I find some tips that can help me forget the <code>type</code> system in <code>scala</code>. Feel good!!! </p>"},{"location":"2024/11/pyspark-dataframe-transformation/#dataclass-vs-case-class","title":"<code>dataclass</code> vs <code>case class</code>","text":"<p>You have to create a <code>case class</code> for each data model in Scala,  while<code>dataclass</code> is your alternative in python</p> <pre><code>@dataclass()\nclass Event:\n    event_id: int\n    event_name: str\n</code></pre>"},{"location":"2024/11/pyspark-dataframe-transformation/#create-dataframe-from-dataclass","title":"Create Dataframe from <code>dataclass</code>","text":"<pre><code>spark = (\n    SparkSession.builder.master(\"local[*]\")\n    .appName(\"test\")\n    .getOrCreate()\n)\nd = [\n    Event(1, \"abc\"),\n    Event(2, \"ddd\"),\n]\n\n# Row object\ndf = spark.createDataFrame(Row(**e.__dict__) for e in d)\ndf.show()\n# +--------+----------+\n# |event_id|event_name|\n# +--------+----------+\n# |       1|       abc|\n# |       2|       ddd|\n# +--------+----------+\n</code></pre>"},{"location":"2024/12/gradio-with-ollama/","title":"Gradio with Ollama","text":""},{"location":"2024/12/gradio-with-ollama/#simple-unstructured-file-processing","title":"Simple Unstructured file processing","text":"<p>We have a lot of pdf files that contain import information, however, the information are unstructured (text, table, image, etc...). To extract and utilize them in our downstream job, an open source unstructured is helpful to implement what we want</p> <p></p>"},{"location":"2024/12/gradio-with-ollama/#demo-app","title":"Demo App","text":"<pre><code>\n%%{init: { 'look':'handDrawn' } }%%\n\nflowchart LR\n  A[Gradio UI] --&gt; B(Ollama Server)\n  B --&gt; C[\"\n  #bull; Gemma2\n  #bull; Llama3\n  #bull; Phi3\n  #bull; Mistral\n  \"]\n  style C color:#FFFFFF,text-align:left,fill:#D2691E\n  style B fill:#FFE4C4\n</code></pre>"},{"location":"2025/02/langgraph-vs-autogen/","title":"LangGraph VS AutoGen","text":""},{"location":"2025/02/langgraph-vs-autogen/#langgraph-vs-autogen","title":"LangGraph VS AutoGen","text":"Feature LangGraph AutoGen Core Concept Graph-based workflow for LLM chaining Multi-agent system with customizable agents Architecture Node-based computation graph Message-passing system between agents Ease of Use Requires defining workflows explicitly as graphs Provides high-level agents for easy configuration Flexibility High (can create complex workflows, DAGs) High (supports various agent types and interactions) Concurrency Supports async execution for parallel nodes Supports multi-agent asynchronous interactions Customization Fully customizable workflows and control flow Customizable agents and message routing strategies LLM Integration Supports OpenAI, Anthropic, and other providers via LangChain Primarily supports OpenAI but extensible State Management Built-in graph state tracking Agent state managed via messages Error Handling Easier to debug with structured DAG execution Debugging can be complex due to emergent agent behavior Use Cases Workflow automation, decision trees, RAG pipelines Autonomous multi-agent collaboration, code execution, RAG Complexity Handling High control over execution paths More emergent behavior, less structured execution Multi-Agent Support Limited (single LLM per node, multi-step workflows) Strong support for multiple interacting agents Pros Fine-grained control over execution paths and state management Easily integrates with LangChain\u2019s ecosystem (retrievers, tools, memory) Supports parallel execution and dependency-based workflows Better for structured workflows like data pipelines, RAG, and decision trees Designed for multi-agent  collaboration, making it ideal for autonomous  agents Easier to  set up  for  conversational  AI, coding  assistants,  and  team-based LLM  interactions Includes  specialized  agents  like CodeExecutorAgent  and SocietyOfMindAgent Strong asynchronous processing capabilities for real-time interactions Cons Requires explicit graph  definition, which can be verbose Less emergent behavior compared to agent-based approaches Multi-agent interactions are not as native as in AutoGen State  management is more  implicit via messages  rather than a  structured graph More opinionated, requiring adaptation to its agent-based paradigm <p>Tips </p> Use Case Recommended Framework Workflow automation (DAGs, logic flows) LangGraph Multi-agent collaboration (AI teams, autonomous systems) AutoGen RAG pipeline with structured retrieval and ranking LangGraph Conversational AI with multiple agents AutoGen Decision trees or conditional logic workflows LangGraph Autonomous coding assistants (e.g., pair programming) AutoGen Parallel execution of tasks LangGraph Emergent multi-agent reasoning AutoGen"},{"location":"2025/02/crawling-the-web-with-llm/","title":"Crawling the Web with LLM","text":""},{"location":"2025/02/crawling-the-web-with-llm/#crawling-the-web-with-large-language-models-llms","title":"Crawling the Web with Large Language Models (LLMs)","text":"<p>Frameworks</p> <ul> <li>Skyvern</li> <li>ScrapegraphAI</li> <li>Crawl4AI</li> <li>Reader</li> <li>Firecrawl</li> <li>Markdowner</li> </ul> <p>Code Examples</p> <pre><code>import json\n\nfrom scrapegraphai.graphs import SmartScraperGraph\n\ngraph_config = {\n    \"llm\": {\n        \"model\": \"ollama/phi4\",\n        \"temperature\": 0,\n        \"format\": \"json\",  # Ollama needs the format to be specified explicitly\n        \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n    },\n    \"embeddings\": {\n        \"model\": \"ollama/jina/jina-embeddings-v2-base-en\",\n        \"base_url\": \"http://localhost:11434\",  # set Ollama URL\n    }\n}\n\nsmart_scraper_graph = SmartScraperGraph(\n    prompt=\"Extract useful information from the webpage, including a description of what the company does, founders and social media links\",\n    source=\"https://scrapegraphai.com/\",\n    config=graph_config\n)\n\nresult = smart_scraper_graph.run()\nprint(json.dumps(result, indent=4))\n</code></pre> <p>Output: </p><pre><code>{\n    \"title\": \"ScrapeGraphAI: AI-Powered Web Scraping Made Easy\",\n    \"description\": \"ScrapeGraphAI is a cutting-edge tool designed to simplify web scraping by leveraging Large Language Models (LLMs). It offers an easy-to-use API that allows developers and AI agents to extract structured data from various types of websites without the need for complex coding. With features like handling website changes, scalability, and integration with popular programming languages such as Python, JavaScript, and TypeScript, ScrapeGraphAI is perfect for anyone looking to harness web data efficiently.\",\n    \"features\": {\n        \"ease_of_use\": \"ScrapeGraphAI provides a user-friendly API that requires minimal coding effort, making it accessible even for those without extensive technical expertise.\",\n        \"integration\": \"The tool seamlessly integrates with Python, JavaScript, and TypeScript, allowing developers to incorporate web scraping into their existing projects effortlessly.\",\n        \"website_handling\": \"Capable of handling diverse websites including e-commerce platforms, social media sites, blogs, forums, and more. It efficiently manages dynamic content loaded via JavaScript or AJAX.\",\n        \"adaptability\": \"ScrapeGraphAI is designed to adapt to website changes automatically, reducing the need for manual maintenance and ensuring consistent data extraction.\",\n        \"performance_and_reliability\": \"The platform offers high performance and reliability, with features like rate limiting, IP rotation, and CAPTCHA solving to ensure smooth operation even under challenging conditions.\"\n    },\n    \"pricing\": {\n        \"free_plan\": \"Includes 1000 requests per month, suitable for small-scale projects or testing purposes.\",\n        \"paid_plans\": [\n            {\n                \"starter\": \"$9.99/month\",\n                \"requests\": \"10,000\"\n            },\n            {\n                \"pro\": \"$49.99/month\",\n                \"requests\": \"50,000\"\n            },\n            {\n                \"enterprise\": \"Custom pricing\",\n                \"requests\": \"Unlimited\"\n            }\n        ]\n    },\n    \"founders\": {\n        \"marco_perini\": \"Founder &amp; CTO - [LinkedIn](https://www.linkedin.com/in/perinim/)\",\n        \"marco_vinciguerra\": \"Founder &amp; Software Engineer - [LinkedIn](https://www.linkedin.com/in/marco-vinciguerra-7ba365242/)\",\n        \"lorenzo_padoan\": \"Founder &amp; CEO - [LinkedIn](https://www.linkedin.com/in/lorenzo-padoan-4521a2154/)\"\n    },\n    \"contact_info\": {\n        \"email\": \"contact@scrapegraphai.com\",\n        \"social_media\": {\n            \"linkedin\": \"[ScrapeGraphAI LinkedIn](https://www.linkedin.com/company/101881123)\",\n            \"twitter\": \"[Twitter](https://x.com/scrapegraphai)\",\n            \"github\": \"[GitHub](https://github.com/ScrapeGraphAI/Scrapegraph-ai)\",\n            \"discord\": \"[Discord](https://discord.gg/uJN7TYcpNa)\"\n        }\n    },\n    \"legal_info\": {\n        \"privacy_policy\": \"[Privacy Policy](/privacy)\",\n        \"terms_of_service\": \"[Terms of Service](/terms)\",\n        \"api_status\": \"[API Status](https://scrapegraphapi.openstatus.dev)\"\n    },\n    \"footer\": \"\\u00a9 2025 - ScrapeGraphAI, Inc\"\n}\n</code></pre>"},{"location":"2025/02/autogen-intro-and-rag-workflow/","title":"Autogen Intro and RAG Workflow","text":""},{"location":"2025/02/autogen-intro-and-rag-workflow/#introduction-to-autogen","title":"Introduction to Autogen","text":"<p>Autogen</p> <p>AutoGen is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.</p> <p>\ud83d\udd39 Agent Types in AutoGen</p> Agent Type Description Use Case BaseChatAgent The foundation for all other agents, handling basic messaging and conversation logic. Custom agent development, extending its capabilities. AssistantAgent A chatbot-like agent designed to assist with general problem-solving and respond to queries. AI assistants, research helpers, customer support. UserProxyAgent Simulates a human user by sending human-like inputs to other agents. Automating user interactions, debugging multi-agent systems. CodeExecutorAgent Specialized agent that executes Python code, typically used for coding tasks. Automating programming tasks, AI-assisted development. SocietyOfMindAgent Manages multiple agents as a hierarchical system to collaborate on complex tasks. Coordinating multiple AI agents for team-based problem-solving. <p>Tips </p> Need Agent Type Basic AI conversation AssistantAgent Simulating human input UserProxyAgent Executing Python code CodeExecutorAgent Coordinating multiple agents SocietyOfMindAgent Custom agent development BaseChatAgent"},{"location":"2025/02/autogen-intro-and-rag-workflow/#example-of-autogen-rag-workflow","title":"Example of Autogen RAG workflow","text":"<pre><code>doc_extractor_topic_type = \"DocExtractorAgent\"\nretrieval_topic_type = \"RetrievalAgent\"\nuser_topic_type = \"User\"\n\n\n@type_subscription(topic_type=doc_extractor_topic_type)\nclass DocExtractorAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, filename:str) -&gt; None:\n        super().__init__(\"A concept extractor agent.\")\n        self.filename = filename\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_user_description(self, message: Message, ctx: MessageContext) -&gt; None:\n        response = f\"Complete extracting content from File {message}\"\n        assert isinstance(response, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{response}\")\n\n        await self.publish_message(Message(response), topic_id=TopicId(retrieval_topic_type, source=self.id.key))\n\n\n\n# Define the RetrievalAgent by extending BaseChatAgent\n@type_subscription(topic_type=retrieval_topic_type)\nclass RetrievalAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient, v:PGVector) -&gt; None:\n        super().__init__(\"Custom Retrieval Agent\")\n        self.vector_db = v\n        self._model_client = model_client\n\n\n    @message_handler\n    async def handle_intermediate_text(self, message: Message, ctx: MessageContext) -&gt; None:\n        response = self.vector_db.similarity_search(message.content)\n        string_output_comprehension = \"\\n\".join([doc.page_content for doc in response])\n        assert isinstance(string_output_comprehension, str)\n        print(f\"{'-'*80}\\n{self.id.type}:\\n{string_output_comprehension}\")\n        await self.publish_message(Message(string_output_comprehension), topic_id=TopicId(user_topic_type, source=self.id.key))\n\n\n\n@type_subscription(topic_type=user_topic_type)\nclass UserAgent(RoutedAgent):\n    def __init__(self, model_client: ChatCompletionClient) -&gt; None:\n        super().__init__(\"A user agent that outputs\")\n        self._system_message = SystemMessage(\n            content=(\n                \"Your are a data analyst who is responsible for retrieving and summarizing data from provided content \"\n            )\n        )\n        self._model_client = model_client\n\n    @message_handler\n    async def handle_final_copy(self, message: Message, ctx: MessageContext) -&gt; None:\n        prompt = f\"Below is the info about the report:\\n\\n{message.content}\"\n        llm_result = await self._model_client.create(\n            messages=[self._system_message, UserMessage(content=prompt, source=self.id.key)],\n            cancellation_token=ctx.cancellation_token,\n        )\n        response = llm_result.content\n        print(f\"\\n{'-'*80}\\n{self.id.type} received final copy:\\n{response}\")\n\n\nasync def main():\n    config = {}\n    model_client = ChatCompletionClient.load_component(config)\n    runtime = SingleThreadedAgentRuntime()\n    await DocExtractorAgent.register(runtime, type=doc_extractor_topic_type,\n                                     factory=lambda : DocExtractorAgent(model_client=model_client))\n    await RetrievalAgent.register(runtime, type=retrieval_topic_type,\n                                  factory=lambda : RetrievalAgent(model_client=model_client))\n\n    await UserAgent.register(runtime, type=user_topic_type,\n                             factory=lambda : UserAgent(model_client=model_client))\n\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n</code></pre>"},{"location":"2025/02/local-llm-setup/","title":"Local LLM Setup","text":""},{"location":"2025/02/local-llm-setup/#local-llm-setup","title":"Local LLM Setup","text":""},{"location":"2025/02/local-llm-setup/#introduction","title":"Introduction","text":"<p>This guide will walk you through setting up a local language model (LLM) using Ollama. Ollama is an open-source, lightweight language model that can be run locally on your machine or server. This setup allows for greater privacy and control over the model, as well as potentially faster inference times compared to cloud-based services.</p>"},{"location":"2025/02/local-llm-setup/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following: - Python installed (version 3.11 or later) - Ollama installed - Basic knowledge of Python programming</p>"},{"location":"2025/02/local-llm-setup/#installation","title":"Installation","text":""},{"location":"2025/02/local-llm-setup/#step-1-python-virtual-environment","title":"Step 1: Python Virtual Environment","text":"<pre><code>python -m venv ollama_env\nsource ollama_env/bin/activate  # On Windows, use `ollama_env\\Scripts\\activate`\n</code></pre>"},{"location":"2025/02/local-llm-setup/#step-2-install-ollama","title":"Step 2: Install Ollama","text":"<pre><code>brew install ollama\n# After installation, you can verify it by running:\nbrew services start ollama # For macOS users or Linux users using systemd.\n# If you prefer to run it as a non-service:\nollama serve &amp;\n# -----------------------------------------------\nollama --version\nollama list\n</code></pre>"},{"location":"2025/02/local-llm-setup/#step-3-install-ollama-models","title":"Step 3: Install Ollama Models","text":"<ul> <li>Command line : <code>ollama pull &lt;model_name&gt;</code></li> <li>Manually import: Guide</li> </ul>"},{"location":"2025/02/local-llm-setup/#step-4-verify-llm-models","title":"Step 4: Verify LLM Models","text":""},{"location":"2025/02/local-llm-setup/#utilization-of-ollama","title":"Utilization of Ollama","text":""},{"location":"2025/02/local-llm-setup/#copilot","title":"Copilot","text":"<p>Without <code>GitHub Copilot</code>, you can use Ollama for code generation and assistance in your projects. Here\u2019s how to set it up:</p>"},{"location":"2025/02/local-llm-setup/#vscode-setup","title":"VSCode Setup","text":"<ul> <li>AI Toolkit for Visual Studio Code Extension </li> <li>Additionally, you can configure your OpenAI API key if needed</li> <li>Chatbot in Playground: Use Ollama as a chatbot to assist with coding tasks directly within VSCode. </li> <li>Continue Extension</li> <li>This extension allows you to continue from where the code snippet left off, making it easier to build on existing code without starting from scratch.</li> <li>Open <code>config.json</code> </li> <li> <p>Add Ollama config   </p> </li> <li> <p>If you find this in your VSCode, congratulations! You have successfully set up Ollama for code generation and assistance in Visual Studio Code. </p> </li> </ul>"},{"location":"2025/02/local-llm-setup/#pycharm-idea-setup","title":"Pycharm &amp; IDEA Setup","text":"<ul> <li>Continue Extension installed from the JetBrains Marketplace</li> <li>Follow the same steps as VSCode setup to configure Ollama in your preferred IDE.</li> </ul>"},{"location":"2025/02/local-llm-setup/#rag-with-ollama-a-comprehensive-guide","title":"RAG with Ollama: A Comprehensive Guide","text":"<p>RAG (Retrieval Augmented Generation) is a powerful technique that enhances the capabilities of large language models by integrating external data sources, such as documents or databases, into the generation process. This integration allows for more accurate and relevant responses to queries, making it an essential tool in various applications, including but not limited to:</p> <ul> <li>Technical Documentation Assistance</li> <li>Code Generation &amp; Debugging</li> <li>Research Paper Summarization</li> <li>Customer Support Chatbots</li> </ul>"},{"location":"2025/02/local-llm-setup/#setting-up-rag-with-ollama","title":"Setting Up RAG with Ollama","text":"<p>To set up RAG with Ollama, you'll need to follow these steps:</p> <ol> <li>Install Ollama: Ensure that Ollama is properly installed and configured in your preferred IDE or text editor.</li> <li>Configure Data Sources: Ollama supports various data sources such as local files, databases, and APIs. You can configure these data sources through the Ollama interface or configuration file.   </li> </ol>"},{"location":"2025/","title":"2025","text":""},{"location":"2024/","title":"2024","text":""},{"location":"2021/","title":"2021","text":""},{"location":"2020/","title":"2020","text":""},{"location":"2012/","title":"2012","text":""},{"location":"category/llm/","title":"LLM","text":""},{"location":"category/python/","title":"python","text":""},{"location":"category/spark/","title":"spark","text":""},{"location":"category/scala/","title":"Scala","text":""},{"location":"category/snowflake/","title":"Snowflake","text":""},{"location":"category/ml/","title":"ML","text":""},{"location":"category/k8s/","title":"k8s","text":""},{"location":"category/azure/","title":"Azure","text":""},{"location":"category/airflow/","title":"airflow","text":""},{"location":"page/2/","title":"Blog","text":""},{"location":"page/3/","title":"Blog","text":""}]}