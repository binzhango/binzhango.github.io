<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Bin&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Bin&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Bin&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Bin&#039;s Blog"><meta property="og:url" content="https://github.com/binzhango/binzhango.github.io"><meta property="og:site_name" content="Bin&#039;s Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://github.com/img/og_image.png"><meta property="article:author" content="Bin Zhang"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/binzhango/binzhango.github.io"},"headline":"Bin's Blog","image":["https://github.com/img/og_image.png"],"author":{"@type":"Person","name":"Bin Zhang"},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="atom.xml" title="Bin's Blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="index.html"><img src="/img/logo.svg" alt="Bin&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="index.html">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-19T02:27:29.000Z" title="2020-11-19T02:27:29.000Z">2020-11-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-19T02:27:29.000Z" title="2020-11-19T02:27:29.000Z">2020-11-18</time></span><span class="level-item"><a class="link-muted" href="categories/DataFactory/">DataFactory</a></span><span class="level-item">a minute read (About 193 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/11/18/Azure-Data-Factory-Data-Flow/">Azure Data Factory (Data Flow)</a></h1><div class="content"><p>Recently I’m working in Azure to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work.</p>
<h2 id="task"><a class="markdownIt-Anchor" href="#task"></a> Task</h2>
<p>Process CSV files and merge different system files into one file</p>
<ul>
<li>Source: CSV files with filename format (<em>abcd_yyyymmdd_uuid.csv</em>), where abcd is system id.
<ul>
<li>a_20180101_9ca2bed1-2ed0-eaeb-8401-784f43755025.csv</li>
<li>a_20180101_cca2bed1-aed0-11eb-8401-784f73755025.csv</li>
<li>b_20190202_ece2bed1-2ed0-abeb-8401-784f43755025.csv</li>
<li>c_20180101_ada2bed1-2ed0-22eb-8401-784f43755025.csv</li>
</ul>
</li>
<li>Sink: yyyymmdd.csv
<ul>
<li>20180101.csv</li>
<li>20190202.csv</li>
</ul>
</li>
</ul>
<h2 id="adf-pipeline"><a class="markdownIt-Anchor" href="#adf-pipeline"></a> ADF Pipeline</h2>
<p><img src="/images/adf_pipeline.png" alt="Pipeline" title="Pipeline" /><br />
<img src="/images/parameter.png" alt="" title="parameters" /><br />
<img src="/images/variables.png" alt="" title="variables" /></p>
<h2 id="activities"><a class="markdownIt-Anchor" href="#activities"></a> Activities</h2>
<h4 id="get-metadata"><a class="markdownIt-Anchor" href="#get-metadata"></a> Get Metadata</h4>
<ul>
<li>Input: source directory/parameters</li>
<li>Output: metadata of each object</li>
</ul>
<p>Get Metadata activity iterate source directory to obtain each object. The most important one is <strong>Argument</strong><br />
<img src="/images/getmetadata.png" alt="Get Metadata" title="Get Metadata" /></p>
<h4 id="foreach"><a class="markdownIt-Anchor" href="#foreach"></a> ForEach</h4>
<ul>
<li>Input: output of <em>Get Metadata</em></li>
<li>Output: None</li>
</ul>
<p>ForEach activity is used to process each object in source direcoty.</p>
<pre><code class="hljs sh">@activity(<span class="hljs-string">'Get Metadata1'</span>).output.childItems</code></pre>
<p><img src="/images/foreach.png" alt="" title="ForEach" /></p>
<h4 id="set-variables"><a class="markdownIt-Anchor" href="#set-variables"></a> Set Variables</h4>
<p>It’s convenient to predefine a value used in next step.</p>
<p><img src="/images/variable_activity.png" alt="" title="Set Variable" /></p>
<h4 id="dataflow"><a class="markdownIt-Anchor" href="#dataflow"></a> Dataflow</h4>
<p><img src="/images/dataflow.png" alt="" title="Dataflow" /></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-03-01T23:35:36.000Z" title="2020-03-01T23:35:36.000Z">2020-03-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-03-01T23:35:36.000Z" title="2020-03-01T23:35:36.000Z">2020-03-01</time></span><span class="level-item"><a class="link-muted" href="categories/Spark/">Spark</a></span><span class="level-item">a few seconds read (About 15 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/03/01/Spark-Dataframe-window-function/">Spark Dataframe window function</a></h1><div class="content"><p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">scala ref</a></p>
<h2 id="create-dataframe"><a class="markdownIt-Anchor" href="#create-dataframe"></a> create dataframe</h2>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-21T19:10:36.000Z" title="2020-02-21T19:10:36.000Z">2020-02-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-21T19:10:36.000Z" title="2020-02-21T19:10:36.000Z">2020-02-21</time></span><span class="level-item"><a class="link-muted" href="categories/Spark/">Spark</a></span><span class="level-item">3 minutes read (About 381 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/21/Spark-Optimizaion/">Spark Optimizaion</a></h1><div class="content"><h1 id="spark-run-faster-and-faster"><a class="markdownIt-Anchor" href="#spark-run-faster-and-faster"></a> Spark run faster and faster</h1>
<ul>
<li>Cluster Optimization</li>
<li>Parameters Optimization</li>
<li>Code Optimization</li>
</ul>
<h2 id="cluster-optimization"><a class="markdownIt-Anchor" href="#cluster-optimization"></a> Cluster Optimization</h2>
<h4 id="locality-level"><a class="markdownIt-Anchor" href="#locality-level"></a> Locality Level</h4>
<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from closest to farthest:</p>
<ul>
<li><strong>PROCESS_LOCAL</strong> data is in the same JVM as the running code. This is the best locality possible</li>
<li><strong>NODE_LOCAL</strong> data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li>
<li><strong>NO_PREF</strong> data is accessed equally quickly from anywhere and has no locality preference</li>
<li><strong>RACK_LOCAL</strong> data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li>
<li><strong>ANY</strong> data is elsewhere on the network and not in the same rack</li>
</ul>
<p>Performance: PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL</p>
<h6 id="locality-settting"><a class="markdownIt-Anchor" href="#locality-settting"></a> Locality settting</h6>
<ul>
<li>spark.locality.wait.process</li>
<li>spark.locality.wait.node</li>
<li>spark.locality.wait.rack</li>
</ul>
<h4 id="data-format"><a class="markdownIt-Anchor" href="#data-format"></a> Data Format</h4>
<ul>
<li>text</li>
<li>orc</li>
<li>parquet</li>
<li>avro</li>
</ul>
<h6 id="format-setting"><a class="markdownIt-Anchor" href="#format-setting"></a> format setting</h6>
<ul>
<li>spark.sql.hive.convertCTAS</li>
<li>spark.sql.sources.default</li>
</ul>
<h4 id="parallelising"><a class="markdownIt-Anchor" href="#parallelising"></a> parallelising</h4>
<ul>
<li>spark.sql.shuffle.partitions : default is 200</li>
</ul>
<h4 id="computing"><a class="markdownIt-Anchor" href="#computing"></a> computing</h4>
<ul>
<li>–executor-memory : default is 1G</li>
<li>–executor-cores : default is 1<br />
if large memory cause resource throtle in cluster, if small memory cause task termination<br />
if more cores cause IO issue, if less cores slow dow computing</li>
</ul>
<h4 id="memory"><a class="markdownIt-Anchor" href="#memory"></a> memory</h4>
<ul>
<li>spark.executor.overhead.memory</li>
</ul>
<h4 id="table-join"><a class="markdownIt-Anchor" href="#table-join"></a> table join</h4>
<ul>
<li>spark.sql.autoBroadcastJoinThreshold : default 10M</li>
</ul>
<h4 id="predicate-push-down-in-spark-sql-queries"><a class="markdownIt-Anchor" href="#predicate-push-down-in-spark-sql-queries"></a> predicate push down in Spark SQL queries</h4>
<ul>
<li>spark.sql.parquet.filterPushdown : default True</li>
<li>spark.sql.orc.filterPushdown=true : default False</li>
</ul>
<h4 id="reuse-rdd"><a class="markdownIt-Anchor" href="#reuse-rdd"></a> reuse RDD</h4>
<pre><code class="hljs pytthon">df.persist(pyspark.StorageLevel.MEMORY_ONLY)</code></pre>
<h4 id="spark-operators"><a class="markdownIt-Anchor" href="#spark-operators"></a> Spark operators</h4>
<ul>
<li>
<p>shuffle operators</p>
<ul>
<li>avoid using <span style="color:blue"> <strong>reduceByKey</strong>, <strong>join</strong>, <strong>distinct</strong>, <strong>repartition</strong> etc</span></li>
<li>Broadcast small dataset</li>
</ul>
</li>
<li>
<p>High performance operator</p>
<ul>
<li>reduceByKey &gt; groupByKey (reduceByKey works at map side)</li>
<li>mapPartitions &gt; map (reduce function calls)</li>
<li>treeReduce &gt; reduce (treeReduce works at executor not driver)
<ul>
<li>treeReduce &amp; reduce return some result to driver</li>
<li>treeReduce does more work on the executors while reduce bring everything back to the driver.</li>
</ul>
</li>
<li>foreachPartitions &gt; foreach (reduce function calls)</li>
<li>filter -&gt; coalesce (reduce number of partitions and reduce tasks)</li>
<li>repartitionAndSortWithinPartitions &gt; repartition &amp; sort</li>
<li>broadcast (100M)</li>
</ul>
</li>
</ul>
<h4 id="shuffle"><a class="markdownIt-Anchor" href="#shuffle"></a> shuffle</h4>
<ul>
<li>spark.shuffle.sort.bypassMergeThreshold</li>
<li>spark.shuffle.io.retryWait</li>
<li>spark.shuffle.io.maxRetries</li>
</ul>
<p>TBC</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-12T03:20:40.000Z" title="2020-02-12T03:20:40.000Z">2020-02-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-12T03:20:40.000Z" title="2020-02-12T03:20:40.000Z">2020-02-11</time></span><span class="level-item"><a class="link-muted" href="categories/airflow/">airflow</a></span><span class="level-item">3 minutes read (About 464 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/11/Airflow-1/">Airflow-- 1</a></h1><div class="content"><h2 id="all-in-one"><a class="markdownIt-Anchor" href="#all-in-one"></a> all in one</h2>
<pre><code class="hljs python"><span class="hljs-comment">#调度机中的dag代码示例</span>
<span class="hljs-comment">#生产机中的dag要放置到与调度机同样的目录下，并且将执行过程增加到*_function()</span>
 
<span class="hljs-keyword">import</span> airflow
<span class="hljs-keyword">from</span> airflow.models <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.python_operator <span class="hljs-keyword">import</span> PythonOperator
 
default_args = &#123;
	<span class="hljs-string">'owner'</span>: <span class="hljs-string">'xiaoming'</span>,
	<span class="hljs-string">'start_date'</span>: airflow.utils.dates.days_ago(<span class="hljs-number">1</span>),
	<span class="hljs-string">'depends_on_past'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-comment"># 失败发邮件</span>
	<span class="hljs-string">'email'</span>: [<span class="hljs-string">'xiaoming@163.com'</span>],
	<span class="hljs-string">'email_on_failure'</span>: <span class="hljs-literal">True</span>,
	<span class="hljs-string">'email_on_retry'</span>: <span class="hljs-literal">True</span>,
	<span class="hljs-comment"># 重试相关</span>
	<span class="hljs-string">'retries'</span>: <span class="hljs-number">3</span>,
	<span class="hljs-string">'retry_delay'</span>: timedelta(minutes=<span class="hljs-number">5</span>),
	<span class="hljs-comment"># 并发限制</span>
	<span class="hljs-string">'pool'</span>: <span class="hljs-string">'data_hadoop_pool'</span>,
	<span class="hljs-string">'priority_weight'</span>: <span class="hljs-number">900</span>,
	<span class="hljs-comment"># 按机器名指定运行位置</span>
	<span class="hljs-string">'queue'</span>: <span class="hljs-string">'66.66.0.66:8080'</span>
&#125;
 
dag = DAG(
    dag_id=<span class="hljs-string">'daily'</span>, 
    default_args=default_args, <span class="hljs-comment">#配置默认参数</span>
    schedule_interval=<span class="hljs-string">'0 13 * * *'</span>)
 
<span class="hljs-comment">#生产机中，将具体执行过程放置在该函数下</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_data_from_hdfs_function</span><span class="hljs-params">(ds, **kwargs)</span>:</span>
	<span class="hljs-keyword">pass</span>
 
<span class="hljs-comment">#生产机中，将具体执行过程放置在该函数下</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">push_data_to_mysql_function</span><span class="hljs-params">(ds, **kwargs)</span>:</span>
	<span class="hljs-keyword">pass</span>
 
fetch_data_from_hdfs = PythonOperator(
	task_id=<span class="hljs-string">'fetch_data_from_hdfs'</span>,
	provide_context=<span class="hljs-literal">True</span>,
	python_callable=fetch_data_from_hdfs_function,
	dag=dag)
 
push_data_to_mysql = PythonOperator(
	task_id=<span class="hljs-string">'push_data_to_mysql'</span>,
	provide_context=<span class="hljs-literal">True</span>,
	python_callable=push_data_to_mysql_function,
	dag=dag)
 
fetch_data_from_hdfs &gt;&gt; push_data_to_mysql</code></pre>
<h2 id="update"><a class="markdownIt-Anchor" href="#update"></a> update</h2>
<pre><code class="hljs python"><span class="hljs-comment">#该task未修改参数，采用默认参数</span>
fetch_data_from_hdfs = PythonOperator(
	task_id=<span class="hljs-string">'fetch_data_from_hdfs'</span>,
	provide_context=<span class="hljs-literal">True</span>,
	python_callable=fetch_data_from_hdfs_function,
	dag=dag)
 
<span class="hljs-comment">#该task修改通过指定参数，覆盖默认参数，调整调度行为</span>
push_data_to_mysql = PythonOperator(
    task_id=<span class="hljs-string">'push_data_to_mysql'</span>,
    queue=<span class="hljs-string">'77.66.0.66:8080'</span>, <span class="hljs-comment">#通过修改参数，调整调度</span>
    pool=<span class="hljs-string">'data_mysql_pool'</span>, <span class="hljs-comment">#通过修改参数，调整调度</span>
    provide_context=<span class="hljs-literal">True</span>,
    python_callable=push_data_to_mysql_function,
    dag=dag)</code></pre>
<h2 id="decouple"><a class="markdownIt-Anchor" href="#decouple"></a> decouple</h2>
<pre><code class="hljs python"><span class="hljs-keyword">import</span> xx.fetch_data_from_hdfs <span class="hljs-comment">#将包装成函数的业务代码引入</span>
 
<span class="hljs-comment">#生产机中，将具体执行过程放置在该函数下</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fetch_data_from_hdfs_function</span><span class="hljs-params">(ds, **kwargs)</span>:</span>
	<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> fetch_data_from_hdfs: <span class="hljs-comment">#判断业务代码是否执行成功，不成功报错</span>
        <span class="hljs-keyword">raise</span> AirflowException(<span class="hljs-string">'run fail: fetch_data_from_hdfs'</span>)
 
fetch_data_from_hdfs = PythonOperator(
	task_id=<span class="hljs-string">'fetch_data_from_hdfs'</span>,
	provide_context=<span class="hljs-literal">True</span>,
	python_callable=fetch_data_from_hdfs_function,
	dag=dag)</code></pre>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-11T13:17:08.000Z" title="2020-02-11T13:17:08.000Z">2020-02-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-11T13:17:08.000Z" title="2020-02-11T13:17:08.000Z">2020-02-11</time></span><span class="level-item"><a class="link-muted" href="categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">a few seconds read (About 0 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/11/Whitening-transformation/">Whitening transformation</a></h1><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-09T04:23:13.000Z" title="2020-02-09T04:23:13.000Z">2020-02-08</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-09T04:23:13.000Z" title="2020-02-09T04:23:13.000Z">2020-02-08</time></span><span class="level-item"><a class="link-muted" href="categories/Spark/">Spark</a></span><span class="level-item">7 minutes read (About 1116 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/08/Spark-Structured-Streaming/">Spark Structured Streaming</a></h1><div class="content"><h2 id="spark-structured-streaming"><a class="markdownIt-Anchor" href="#spark-structured-streaming"></a> Spark Structured Streaming</h2>
<p>Recently reading a blog <a href="https://hackersandslackers.com/structured-streaming-in-pyspark/" target="_blank" rel="noopener">Structured Streaming in PySpark</a><br />
It’s implemented in Databricks platform. Then I try to reimplement in my local Spark.<br />
Some tricky issue happend during my work.</p>
<h2 id="reading-data"><a class="markdownIt-Anchor" href="#reading-data"></a> Reading Data</h2>
<pre><code class="hljs python"><span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession
<span class="hljs-keyword">from</span> pyspark.sql.types <span class="hljs-keyword">import</span> TimestampType, StringType, StructType, StructField

spark = SparkSession.builder.appName(<span class="hljs-string">"Test Streaming"</span>).enableHiveSupport().getOrCreate()

json_schema = StructType([
    StructField(<span class="hljs-string">"time"</span>, TimestampType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"customer"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"action"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"device"</span>, StringType(), <span class="hljs-literal">True</span>)
])

file_path = <span class="hljs-string">"local_file_path&lt;file:///..."</span></code></pre>
<h4 id="read-json-as-same-as-method-in-the-blog"><a class="markdownIt-Anchor" href="#read-json-as-same-as-method-in-the-blog"></a> read json as same as method in the blog</h4>
<pre><code class="hljs python">input = spark.read.schema(json_schema).json(file_path)

input.show()
<span class="hljs-comment"># +----+--------+------+------+</span>
<span class="hljs-comment"># |time|customer|action|device|</span>
<span class="hljs-comment"># +----+--------+------+------+</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># +----+--------+------+------+</span>
input.count()
<span class="hljs-comment"># 20000</span></code></pre>
<p>All values are null, however, the count is right. It means spark has already read all data but the schema is not correctly mapped.</p>
<h4 id="read-a-single-json-file-to-check-schema"><a class="markdownIt-Anchor" href="#read-a-single-json-file-to-check-schema"></a> read a single json file to check schema</h4>
<pre><code class="hljs python">input = spark.read.schema(json_schema).json(file_path+<span class="hljs-string">'/1.json'</span>)

input.show()

<span class="hljs-comment"># +----+--------+------+------+</span>
<span class="hljs-comment"># |time|customer|action|device|</span>
<span class="hljs-comment"># +----+--------+------+------+</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># |null|    null|  null|  null|</span>
<span class="hljs-comment"># +----+--------+------+------+</span>

<span class="hljs-comment"># same error</span>
<span class="hljs-comment"># Then I drop schema option and use inferSchema</span>
input = spark.read.json(file_path+<span class="hljs-string">'/1.json'</span>)

input.show()

<span class="hljs-comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span>
<span class="hljs-comment"># |     _corrupt_record|     action|         customer|              device|           time|</span>
<span class="hljs-comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span>
<span class="hljs-comment"># |[&#123;"time":"3:57:09...|       null|             null|                null|           null|</span>
<span class="hljs-comment"># |                null|  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|</span>
<span class="hljs-comment"># |                null|   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|</span>
<span class="hljs-comment"># |                null|  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|</span>
<span class="hljs-comment"># |                null|  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|</span>
<span class="hljs-comment"># |                null|  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|</span>
<span class="hljs-comment"># |                null|low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|</span>
<span class="hljs-comment"># |                null|   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|</span>
<span class="hljs-comment"># |                null|  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|</span>
<span class="hljs-comment"># |                null|  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|</span>
<span class="hljs-comment"># |                null|   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|</span>
<span class="hljs-comment"># |                null|low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|</span>
<span class="hljs-comment"># |                null|low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|</span>
<span class="hljs-comment"># |                null|  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|</span>
<span class="hljs-comment"># |                null|   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|</span>
<span class="hljs-comment"># |                null|  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|</span>
<span class="hljs-comment"># |                null|   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|</span>
<span class="hljs-comment"># |                null|low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|</span>
<span class="hljs-comment"># |                null|  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|</span>
<span class="hljs-comment"># |                null|low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|</span>
<span class="hljs-comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></code></pre>
<p>A weird column is <em>_corrupt_record</em> and first value is <strong>[{“time”:&quot;3:57:09…</strong> in this column.<br />
Go back to check source file and notice that it’s a list of object in json file.</p>
<h6 id="remove-span-stylecolorred-span-and-span-stylecolorred-span-in-source-file"><a class="markdownIt-Anchor" href="#remove-span-stylecolorred-span-and-span-stylecolorred-span-in-source-file"></a> Remove  <span style='color:red'> <em>[</em> </span> and <span style='color:red'><em>]</em> </span> in source file</h6>
<pre><code class="hljs python">input = spark.read.json(file_path+<span class="hljs-string">'/1.json'</span>)

input.show()

<span class="hljs-comment"># +-----------+-----------------+--------------------+---------------+</span>
<span class="hljs-comment"># |     action|         customer|              device|           time|</span>
<span class="hljs-comment"># +-----------+-----------------+--------------------+---------------+</span>
<span class="hljs-comment"># |  power off|      Alexi Barts|  GreenIQ Controller| 3:57:09.000 PM|</span>
<span class="hljs-comment"># |  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|</span>
<span class="hljs-comment"># |   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|</span>
<span class="hljs-comment"># |  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|</span>
<span class="hljs-comment"># |  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|</span>
<span class="hljs-comment"># |  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|</span>
<span class="hljs-comment"># |low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|</span>
<span class="hljs-comment"># |   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|</span>
<span class="hljs-comment"># |  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|</span>
<span class="hljs-comment"># |  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|</span>
<span class="hljs-comment"># |   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|</span>
<span class="hljs-comment"># |low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|</span>
<span class="hljs-comment"># |low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|</span>
<span class="hljs-comment"># |  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|</span>
<span class="hljs-comment"># |   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|</span>
<span class="hljs-comment"># |  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|</span>
<span class="hljs-comment"># |   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|</span>
<span class="hljs-comment"># |low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|</span>
<span class="hljs-comment"># |  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|</span>
<span class="hljs-comment"># |low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|</span>
<span class="hljs-comment"># +-----------+-----------------+--------------------+---------------+</span></code></pre>
<p>Woo, the dataframe is correct. Let’s check schema</p>
<pre><code class="hljs python">input.printSchema()
<span class="hljs-comment"># root</span>
<span class="hljs-comment">#  |-- action: string (nullable = true)</span>
<span class="hljs-comment">#  |-- customer: string (nullable = true)</span>
<span class="hljs-comment">#  |-- device: string (nullable = true)</span>
<span class="hljs-comment">#  |-- time: string (nullable = true)</span></code></pre>
<p>So far I manually modify source file and drop external schema to obtain a corret dataframe. Is there anyway to<br />
read these files without these steps.</p>
<h6 id="add-one-feature-span-stylecolorbluemultilinespan"><a class="markdownIt-Anchor" href="#add-one-feature-span-stylecolorbluemultilinespan"></a> add one feature <span style='color:blue'>multiLine</span></h6>
<p>Read the file without schema but add one feature <strong>multiLine</strong></p>
<pre><code class="hljs python">input = spark.read.json(<span class="hljs-string">"file:///path/pyspark_test_data"</span>, multiLine=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># OR input = spark.read.option('multiLine', True).json("file:///path/pyspark_test_data")</span>

<span class="hljs-comment"># +-----------+--------------------+--------------------+---------------+</span>
<span class="hljs-comment"># |     action|            customer|              device|           time|</span>
<span class="hljs-comment"># +-----------+--------------------+--------------------+---------------+</span>
<span class="hljs-comment"># |   power on|     Raynor Blaskett|Nest T3021US Ther...| 3:35:09.000 AM|</span>
<span class="hljs-comment"># |   power on|Stafford Blakebrough|  GreenIQ Controller|10:59:46.000 AM|</span>
<span class="hljs-comment"># |   power on|      Alex Woolcocks|Nest T3021US Ther...| 6:26:36.000 PM|</span>
<span class="hljs-comment"># |   power on|      Clarice Nayshe|Footbot Air Quali...| 4:46:28.000 AM|</span>
<span class="hljs-comment"># |  power off|      Killie Pirozzi|Footbot Air Quali...| 8:58:43.000 AM|</span>
<span class="hljs-comment"># |   power on|    Lynne Dymidowicz|Footbot Air Quali...| 4:20:49.000 PM|</span>
<span class="hljs-comment"># |   power on|       Shaina Dowyer|             ecobee4| 3:41:33.000 AM|</span>
<span class="hljs-comment"># |low battery|       Barbee Melato| August Doorbell Cam|10:40:24.000 PM|</span>
<span class="hljs-comment"># |  power off|        Clem Westcot|Nest T3021US Ther...|11:13:38.000 PM|</span>
<span class="hljs-comment"># |  power off|       Kerri Galfour|         Amazon Echo|10:12:15.000 PM|</span>
<span class="hljs-comment"># |low battery|        Trev Ashmore|  GreenIQ Controller|11:04:41.000 AM|</span>
<span class="hljs-comment"># |   power on|      Coral Jahnisch| August Doorbell Cam| 3:06:31.000 AM|</span>
<span class="hljs-comment"># |   power on|      Feliza Cowdrey|Nest T3021US Ther...| 2:49:02.000 AM|</span>
<span class="hljs-comment"># |  power off|   Amabelle De Haven|Footbot Air Quali...|12:11:59.000 PM|</span>
<span class="hljs-comment"># |  power off|     Benton Redbourn|Nest T3021US Ther...| 3:57:39.000 AM|</span>
<span class="hljs-comment"># |low battery|        Asher Potten| August Doorbell Cam| 1:34:44.000 AM|</span>
<span class="hljs-comment"># |low battery|    Lorianne Hullyer| August Doorbell Cam| 7:26:42.000 PM|</span>
<span class="hljs-comment"># |  power off|     Ruperto Aldcorn|Footbot Air Quali...| 3:54:49.000 AM|</span>
<span class="hljs-comment"># |   power on|   Agatha Di Giacomo|Footbot Air Quali...| 7:15:20.000 AM|</span>
<span class="hljs-comment"># |   power on|    Eunice Penwright|             ecobee4|11:14:14.000 PM|</span>
<span class="hljs-comment"># +-----------+--------------------+--------------------+---------------+</span>

input.printSchema()

<span class="hljs-comment"># root</span>
<span class="hljs-comment">#  |-- action: string (nullable = true)</span>
<span class="hljs-comment">#  |-- customer: string (nullable = true)</span>
<span class="hljs-comment">#  |-- device: string (nullable = true)</span>
<span class="hljs-comment">#  |-- time: string (nullable = true)</span></code></pre>
<h4 id="change-the-schema"><a class="markdownIt-Anchor" href="#change-the-schema"></a> change the schema</h4>
<p>Set time as <em>StringType</em></p>
<pre><code class="hljs python">json_schema = StructType([
    StructField(<span class="hljs-string">"time"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"customer"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"action"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"device"</span>, StringType(), <span class="hljs-literal">True</span>)
])


input = spark.read.schema(json_schema).json(<span class="hljs-string">"file:///path/pyspark_test_data"</span>, multiLine=<span class="hljs-literal">True</span>)

input.show()

<span class="hljs-comment"># +---------------+--------------------+-----------+--------------------+</span>
<span class="hljs-comment"># |           time|            customer|     action|              device|</span>
<span class="hljs-comment"># +---------------+--------------------+-----------+--------------------+</span>
<span class="hljs-comment"># | 3:35:09.000 AM|     Raynor Blaskett|   power on|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |10:59:46.000 AM|Stafford Blakebrough|   power on|  GreenIQ Controller|</span>
<span class="hljs-comment"># | 6:26:36.000 PM|      Alex Woolcocks|   power on|Nest T3021US Ther...|</span>
<span class="hljs-comment"># | 4:46:28.000 AM|      Clarice Nayshe|   power on|Footbot Air Quali...|</span>
<span class="hljs-comment"># | 8:58:43.000 AM|      Killie Pirozzi|  power off|Footbot Air Quali...|</span>
<span class="hljs-comment"># | 4:20:49.000 PM|    Lynne Dymidowicz|   power on|Footbot Air Quali...|</span>
<span class="hljs-comment"># | 3:41:33.000 AM|       Shaina Dowyer|   power on|             ecobee4|</span>
<span class="hljs-comment"># |10:40:24.000 PM|       Barbee Melato|low battery| August Doorbell Cam|</span>
<span class="hljs-comment"># |11:13:38.000 PM|        Clem Westcot|  power off|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |10:12:15.000 PM|       Kerri Galfour|  power off|         Amazon Echo|</span>
<span class="hljs-comment"># |11:04:41.000 AM|        Trev Ashmore|low battery|  GreenIQ Controller|</span>
<span class="hljs-comment"># | 3:06:31.000 AM|      Coral Jahnisch|   power on| August Doorbell Cam|</span>
<span class="hljs-comment"># | 2:49:02.000 AM|      Feliza Cowdrey|   power on|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |12:11:59.000 PM|   Amabelle De Haven|  power off|Footbot Air Quali...|</span>
<span class="hljs-comment"># | 3:57:39.000 AM|     Benton Redbourn|  power off|Nest T3021US Ther...|</span>
<span class="hljs-comment"># | 1:34:44.000 AM|        Asher Potten|low battery| August Doorbell Cam|</span>
<span class="hljs-comment"># | 7:26:42.000 PM|    Lorianne Hullyer|low battery| August Doorbell Cam|</span>
<span class="hljs-comment"># | 3:54:49.000 AM|     Ruperto Aldcorn|  power off|Footbot Air Quali...|</span>
<span class="hljs-comment"># | 7:15:20.000 AM|   Agatha Di Giacomo|   power on|Footbot Air Quali...|</span>
<span class="hljs-comment"># |11:14:14.000 PM|    Eunice Penwright|   power on|             ecobee4|</span>
<span class="hljs-comment"># +---------------+--------------------+-----------+--------------------+</span></code></pre>
<p>Pyspark can load json files successfully without TimestampType. However, how to handle timestamp issue in this job?</p>
<h4 id="timestamptype"><a class="markdownIt-Anchor" href="#timestamptype"></a> TimestampType</h4>
<p>In offical document, the class <em>pyspark.sql.DataFrameReader</em> has one parameter</p>
<ul>
<li>timestampFormat</li>
</ul>
<blockquote>
<p>sets the string that indicates a timestamp format.</p>
<p>Custom date formats follow the formats at java.text.SimpleDateFormat.</p>
<p>This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss.SSSXXX.</p>
</blockquote>
<pre><code class="hljs python">input = spark.read.schema(schema).option(<span class="hljs-string">"multiLine"</span>, <span class="hljs-literal">True</span>).json(<span class="hljs-string">"file:///path/pyspark_test_data"</span>, timestampFormat=<span class="hljs-string">"h:mm:ss.SSS aa"</span>)

input.show()
<span class="hljs-comment"># +-------------------+--------------------+-----------+--------------------+</span>
<span class="hljs-comment"># |               time|            customer|     action|              device|</span>
<span class="hljs-comment"># +-------------------+--------------------+-----------+--------------------+</span>
<span class="hljs-comment"># |1970-01-01 03:35:09|     Raynor Blaskett|   power on|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |1970-01-01 10:59:46|Stafford Blakebrough|   power on|  GreenIQ Controller|</span>
<span class="hljs-comment"># |1970-01-01 18:26:36|      Alex Woolcocks|   power on|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |1970-01-01 04:46:28|      Clarice Nayshe|   power on|Footbot Air Quali...|</span>
<span class="hljs-comment"># |1970-01-01 08:58:43|      Killie Pirozzi|  power off|Footbot Air Quali...|</span>
<span class="hljs-comment"># |1970-01-01 16:20:49|    Lynne Dymidowicz|   power on|Footbot Air Quali...|</span>
<span class="hljs-comment"># |1970-01-01 03:41:33|       Shaina Dowyer|   power on|             ecobee4|</span>
<span class="hljs-comment"># |1970-01-01 22:40:24|       Barbee Melato|low battery| August Doorbell Cam|</span>
<span class="hljs-comment"># |1970-01-01 23:13:38|        Clem Westcot|  power off|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |1970-01-01 22:12:15|       Kerri Galfour|  power off|         Amazon Echo|</span>
<span class="hljs-comment"># |1970-01-01 11:04:41|        Trev Ashmore|low battery|  GreenIQ Controller|</span>
<span class="hljs-comment"># |1970-01-01 03:06:31|      Coral Jahnisch|   power on| August Doorbell Cam|</span>
<span class="hljs-comment"># |1970-01-01 02:49:02|      Feliza Cowdrey|   power on|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |1970-01-01 12:11:59|   Amabelle De Haven|  power off|Footbot Air Quali...|</span>
<span class="hljs-comment"># |1970-01-01 03:57:39|     Benton Redbourn|  power off|Nest T3021US Ther...|</span>
<span class="hljs-comment"># |1970-01-01 01:34:44|        Asher Potten|low battery| August Doorbell Cam|</span>
<span class="hljs-comment"># |1970-01-01 19:26:42|    Lorianne Hullyer|low battery| August Doorbell Cam|</span>
<span class="hljs-comment"># |1970-01-01 03:54:49|     Ruperto Aldcorn|  power off|Footbot Air Quali...|</span>
<span class="hljs-comment"># |1970-01-01 07:15:20|   Agatha Di Giacomo|   power on|Footbot Air Quali...|</span>
<span class="hljs-comment"># |1970-01-01 23:14:14|    Eunice Penwright|   power on|             ecobee4|</span>
<span class="hljs-comment"># +-------------------+--------------------+-----------+--------------------+</span></code></pre>
<p>All yyyy-MM-dd are 1970-01-01 because source file only hh-mm-ss.<br />
These source files are in wrong format in Windows.</p>
<h2 id="streaming-our-data"><a class="markdownIt-Anchor" href="#streaming-our-data"></a> Streaming Our Data</h2>
<pre><code class="hljs python"><span class="hljs-keyword">from</span> pyspark.sql <span class="hljs-keyword">import</span> SparkSession
<span class="hljs-keyword">from</span> pyspark.sql.types <span class="hljs-keyword">import</span> TimestampType, StringType, StructType, StructField


spark = SparkSession.builder.appName(<span class="hljs-string">"Test Streaming"</span>).enableHiveSupport().getOrCreate()

json_schema = StructType([
    StructField(<span class="hljs-string">"time"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"customer"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"action"</span>, StringType(), <span class="hljs-literal">True</span>),
    StructField(<span class="hljs-string">"device"</span>, StringType(), <span class="hljs-literal">True</span>)
])

streamingDF = spark.readStream.schema(json_schema) \
              .option(<span class="hljs-string">"maxFilesPerTrigger"</span>, <span class="hljs-number">1</span>) \
              .option(<span class="hljs-string">"multiLine"</span>, <span class="hljs-literal">True</span>) \
              .json(<span class="hljs-string">"file:///path/pyspark_test_data"</span>)

streamingActionCountsDF = streamingDF.groupBy(<span class="hljs-string">'action'</span>).count()
<span class="hljs-comment"># streamingActionCountsDF.isStreaming</span>
spark.conf.set(<span class="hljs-string">"spark.sql.shuffle.partitions"</span>, <span class="hljs-string">"2"</span>)


<span class="hljs-comment"># View stream in real-time</span>
<span class="hljs-comment"># query = streamingActionCountsDF.writeStream \</span>
<span class="hljs-comment">#         .format("memory").queryName("counts").outputMode("complete").start()</span>

<span class="hljs-comment"># format choice:</span>
<span class="hljs-comment"># parquet</span>
<span class="hljs-comment"># kafka</span>
<span class="hljs-comment"># console</span>
<span class="hljs-comment"># memory</span>

<span class="hljs-comment"># query = streamingActionCountsDF.writeStream \</span>
<span class="hljs-comment">#         .format("console").queryName("counts").outputMode("complete").start()</span>

query = streamingActionCountsDF.writeStream.format(<span class="hljs-string">"console"</span>) \
        .queryName(<span class="hljs-string">"counts"</span>).outputMode(<span class="hljs-string">"complete"</span>).start().awaitTermination(timeout=<span class="hljs-number">10</span>)
<span class="hljs-comment"># Output Mode choice:</span>
<span class="hljs-comment"># append</span>
<span class="hljs-comment"># complete</span>
<span class="hljs-comment"># update</span></code></pre>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-04T13:15:15.000Z" title="2020-02-04T13:15:15.000Z">2020-02-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-04T13:15:15.000Z" title="2020-02-04T13:15:15.000Z">2020-02-04</time></span><span class="level-item"><a class="link-muted" href="categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">a minute read (About 182 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/04/Batch-Normalization/">Batch Normalization</a></h1><div class="content"><p>Batch Normalization is one of important parts in our NN.</p>
<h2 id="why-need-normalization"><a class="markdownIt-Anchor" href="#why-need-normalization"></a> Why need Normalization</h2>
<p>This paper title tells me the reason<br />
<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>
<ul>
<li>accelerating traning</li>
<li>reduce internal covariate shift</li>
</ul>
<h4 id="independent-and-identically-distributed-iid"><a class="markdownIt-Anchor" href="#independent-and-identically-distributed-iid"></a> Independent and identically distributed (IID)</h4>
<p>If our data is independent and identically distributed, training model can be simplified and its predictive ability is improved.<br />
One important step of data preparation is <strong>whitening</strong> which is used to</p>
<h6 id="whitening"><a class="markdownIt-Anchor" href="#whitening"></a> Whitening</h6>
<ul>
<li>reduce features’ coralation     =&gt; Independent</li>
<li>all features have zero mean and unit variances =&gt; Identically distributed</li>
</ul>
<h4 id="internal-covariate-shift-ics"><a class="markdownIt-Anchor" href="#internal-covariate-shift-ics"></a> Internal Covariate Shift (ICS)</h4>
<p>What is problem of ICS? Generally data is not IID</p>
<ul>
<li>Previous layer should update hyper-parameters to adjust new data so that reduce learning speed</li>
<li>Get stuck in the saturation region as the network grows deeper and network stop learning earlier</li>
</ul>
<h6 id="covariate-shift"><a class="markdownIt-Anchor" href="#covariate-shift"></a> Covariate Shift</h6>
<blockquote>
<p>What is covariate shift? While in the process <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">X \rightarrow Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mrel">→</span><span class="mord mathit" style="margin-right:0.22222em;">Y</span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>P</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mo>=</mo><msup><mi>P</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P^{train}(y|x) = P^{test}(y|x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8746639999999999em;"></span><span class="strut bottom" style="height:1.1246639999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">a</span><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mi>u</mi><mi>t</mi><mspace width="0.277778em"></mspace><msup><mi>P</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>(</mo><mi>x</mi><mo>)</mo><mo>≠</mo><msup><mi>P</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">but\; P^{train}(x) \neq P^{test}(x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8746639999999999em;"></span><span class="strut bottom" style="height:1.1246639999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">b</span><span class="mord mathit">u</span><span class="mord mathit">t</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">a</span><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
</blockquote>
<h1 id="todo"><a class="markdownIt-Anchor" href="#todo"></a> ToDo</h1>
<h2 id="normalizations"><a class="markdownIt-Anchor" href="#normalizations"></a> Normalizations</h2>
<ul>
<li>weight scale invariance</li>
<li>data scale invariance</li>
</ul>
<h4 id="batch-normalization"><a class="markdownIt-Anchor" href="#batch-normalization"></a> Batch Normalization</h4>
<h4 id="layer-normalization"><a class="markdownIt-Anchor" href="#layer-normalization"></a> Layer Normalization</h4>
<h4 id="weight-normalization"><a class="markdownIt-Anchor" href="#weight-normalization"></a> Weight Normalization</h4>
<h4 id="cosine-normalization"><a class="markdownIt-Anchor" href="#cosine-normalization"></a> Cosine Normalization</h4>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-03T02:04:06.000Z" title="2020-02-03T02:04:06.000Z">2020-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-03T02:04:06.000Z" title="2020-02-03T02:04:06.000Z">2020-02-02</time></span><span class="level-item"><a class="link-muted" href="categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">9 minutes read (About 1312 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/02/Gradient-Descent/">Gradient Descent</a></h1><div class="content"><h1 id="gradient-based-optimization-algorithms"><a class="markdownIt-Anchor" href="#gradient-based-optimization-algorithms"></a> gradient-based optimization algorithms</h1>
<h2 id="gradient-descent-variants"><a class="markdownIt-Anchor" href="#gradient-descent-variants"></a> Gradient Descent variants</h2>
<h4 id="batch-gradient-descent-bgd"><a class="markdownIt-Anchor" href="#batch-gradient-descent-bgd"></a> Batch Gradient Descent (BGD)</h4>
<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ</p>
<p>Batch gradient descent is guaranteed to converge</p>
<ul>
<li>to the global minimum for convex error surfaces</li>
<li>to a local minimum for non-convex surfaces</li>
</ul>
<h4 id="stochastic-gradient-descent-sgd"><a class="markdownIt-Anchor" href="#stochastic-gradient-descent-sgd"></a> Stochastic Gradient Descent (SGD)</h4>
<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update.<br />
SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.<br />
SGD performs frequent updates with a high variance that cause the objective function to <em>fluctuate</em> heavily.<br />
While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation,</p>
<ul>
<li>enables it to jump to new and potentially better local minima</li>
<li>this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting</li>
</ul>
<p>when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a <em>local</em> or the <em>global</em> minimum for <em>non-convex</em> and <em>convex</em> optimization respectively.</p>
<h4 id="mini-batch-gradient-descent-mb-gd"><a class="markdownIt-Anchor" href="#mini-batch-gradient-descent-mb-gd"></a> Mini-batch Gradient Descent (MB-GD)</h4>
<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples</p>
<ul>
<li>reduces the variance of the parameter updates, which can lead to more stable convergence</li>
<li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient</li>
<li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li>
</ul>
<h4 id="challenges"><a class="markdownIt-Anchor" href="#challenges"></a> Challenges</h4>
<ul>
<li><strong>Choosing a proper learning rate can be difficult.</strong></li>
</ul>
<blockquote>
<p>A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p>
</blockquote>
<ul>
<li><strong>Learning rete schedules try to adjust the learning rate during training</strong></li>
</ul>
<blockquote>
<p>e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics</p>
</blockquote>
<ul>
<li><strong>The same learning rate applies to all parameter updates</strong></li>
</ul>
<blockquote>
<p>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features</p>
</blockquote>
<ul>
<li><strong>Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima</strong></li>
</ul>
<blockquote>
<p>The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p>
</blockquote>
<h2 id="gradient-descent-optimization-algorithms"><a class="markdownIt-Anchor" href="#gradient-descent-optimization-algorithms"></a> Gradient Descent Optimization Algorithms</h2>
<p>We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton’s method.</p>
<h4 id="momentum"><a class="markdownIt-Anchor" href="#momentum"></a> Momentum</h4>
<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima.</p>
<p>Some implementations exchange the signs in the equations. The momentum term γ is usually set to 0.9 or a similar value.</p>
<p>When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill,<br />
becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1).<br />
<em>The same thing happens to our parameter updates</em>:</p>
<blockquote>
<p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain <em>faster convergence and reduced oscillation</em>.</p>
</blockquote>
<h4 id="nesterov-accelerated-gradient"><a class="markdownIt-Anchor" href="#nesterov-accelerated-gradient"></a> Nesterov Accelerated Gradient</h4>
<p>We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.<br />
Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience.<br />
We know that we will use our momentum term γvθ<sub>t-1</sub> to move the parameters θ.<br />
Computing θ−γv<sub>t-1</sub> thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update),<br />
a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient<br />
<em>not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters</em></p>
<p>we are able to adapt our updates to the slope of our error function and speed up SGD in turn,<br />
we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance</p>
<p>The distinction between Momentum method and Nesterov Accelerated Gradient updates was</p>
<ul>
<li>Both methods are distinct only when the learning rate η is reasonably large.</li>
<li>When the learning rate η is relatively large, Nesterov Accelerated Gradients allows larger decay rate α than Momentum method, while preventing oscillations.</li>
<li>Both Momentum method and Nesterov Accelerated Gradient <strong>become equivalent when η is small</strong></li>
</ul>
<h4 id="adagrad"><a class="markdownIt-Anchor" href="#adagrad"></a> Adagrad</h4>
<p>Adagrad is an algorithm for gradient-based optimization that does just this:<br />
It adapts the learning rate to the parameters,</p>
<ul>
<li>performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features,</li>
<li>and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</li>
</ul>
<p>For this reason, <strong>it is well-suited for dealing with sparse data.</strong></p>
<p>Previously, we performed an update for all parameters θ at once as every parameter θ<sub>i</sub> used the same learning rate η.<br />
As Adagrad uses a different learning rate for every parameter θ<sub>i</sub> at every time step t, we first show Adagrad’s per-parameter update, which we then vectorize.<br />
For brevity, we use gt to denote the gradient at time step t. g<sub>t,i</sub> is then the partial derivative of the objective function w.r.t. to the parameter θ<sub>i</sub> at time step t</p>
<p>In its update rule, Adagrad modifies the general learning rate η at each time step t for every parameter θ<sub>i</sub> based on the past gradients that have been computed for θ<sub>i</sub></p>
<p>θ<sub>t+1,i</sub>=θ<sub>t,i</sub>−η/√(G<sub>t,ii</sub>+ϵ)⋅g<sub>t,i</sub></p>
<p>G<sub>t</sub>∈R<sup>d×d</sup> here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θ<sub>i</sub> up to time step t,<br />
while ϵ is a smoothing term that avoids division by zero.<br />
<strong>Interestingly, without the square root operation, the algorithm performs much worse.</strong></p>
<ul>
<li>One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate</li>
<li>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator</li>
</ul>
<blockquote>
<p>Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>
</blockquote>
<h4 id="adadelta"><a class="markdownIt-Anchor" href="#adadelta"></a> Adadelta</h4>
<p>Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate.<br />
Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p>
<p>Instead of inefficiently storing w previous squared gradients,<br />
the sum of gradients is recursively defined as a decaying average of all past squared gradients.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-01T15:50:08.822Z" title="2020-02-01T15:50:08.822Z">2020-02-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-01T15:50:08.822Z" title="2020-02-01T15:50:08.822Z">2020-02-01</time></span><span class="level-item"><a class="link-muted" href="categories/Hexo/">Hexo</a></span><span class="level-item">a minute read (About 123 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="2020/02/01/hello-world/">Hello World</a></h1><div class="content"><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start"><a class="markdownIt-Anchor" href="#quick-start"></a> Quick Start</h2>
<h3 id="create-a-new-post"><a class="markdownIt-Anchor" href="#create-a-new-post"></a> Create a new post</h3>
<pre><code class="hljs bash">$ hexo new <span class="hljs-string">"My New Post"</span></code></pre>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="run-server"><a class="markdownIt-Anchor" href="#run-server"></a> Run server</h3>
<pre><code class="hljs bash">$ hexo server</code></pre>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="generate-static-files"><a class="markdownIt-Anchor" href="#generate-static-files"></a> Generate static files</h3>
<pre><code class="hljs bash">$ hexo generate</code></pre>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="deploy-to-remote-sites"><a class="markdownIt-Anchor" href="#deploy-to-remote-sites"></a> Deploy to remote sites</h3>
<pre><code class="hljs bash">$ hexo deploy</code></pre>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com/"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="index.html"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="categories/DataFactory/"><span class="level-start"><span class="level-item">DataFactory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="categories/Hexo/"><span class="level-start"><span class="level-item">Hexo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="categories/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="categories/airflow/"><span class="level-start"><span class="level-item">airflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-11-19T02:27:29.000Z">2020-11-18</time></p><p class="title"><a href="2020/11/18/Azure-Data-Factory-Data-Flow/">Azure Data Factory (Data Flow)</a></p><p class="categories"><a href="categories/DataFactory/">DataFactory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-01T23:35:36.000Z">2020-03-01</time></p><p class="title"><a href="2020/03/01/Spark-Dataframe-window-function/">Spark Dataframe window function</a></p><p class="categories"><a href="categories/Spark/">Spark</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-21T19:10:36.000Z">2020-02-21</time></p><p class="title"><a href="2020/02/21/Spark-Optimizaion/">Spark Optimizaion</a></p><p class="categories"><a href="categories/Spark/">Spark</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-12T03:20:40.000Z">2020-02-11</time></p><p class="title"><a href="2020/02/11/Airflow-1/">Airflow-- 1</a></p><p class="categories"><a href="categories/airflow/">airflow</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-11T13:17:08.000Z">2020-02-11</time></p><p class="title"><a href="2020/02/11/Whitening-transformation/">Whitening transformation</a></p><p class="categories"><a href="categories/Machine-Learning/">Machine Learning</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="tags/Azure/"><span class="tag">Azure</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="tags/Dataframe/"><span class="tag">Dataframe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="tags/Optimizer/"><span class="tag">Optimizer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="tags/Streaming/"><span class="tag">Streaming</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="index.html"><img src="/img/logo.svg" alt="Bin&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2020 Bin Zhang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>