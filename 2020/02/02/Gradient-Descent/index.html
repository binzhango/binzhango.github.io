<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Gradient Descent - Bin&#039;s Blog</title><link rel="manifest" href="../../../../manifest.json"><meta name="application-name" content="Bin&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Bin&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="gradient-based optimization algorithms  Gradient Descent variants  Batch Gradient Descent (BGD) Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t."><meta property="og:type" content="blog"><meta property="og:title" content="Gradient Descent"><meta property="og:url" content="https://github.com/binzhango/binzhango.github.io/2020/02/02/Gradient-Descent/"><meta property="og:site_name" content="Bin&#039;s Blog"><meta property="og:description" content="gradient-based optimization algorithms  Gradient Descent variants  Batch Gradient Descent (BGD) Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://github.com/binzhango/binzhango.github.io/img/og_image.png"><meta property="article:published_time" content="2020-02-03T02:04:06.000Z"><meta property="article:modified_time" content="2020-02-03T02:04:06.000Z"><meta property="article:author" content="Bin Zhang"><meta property="article:tag" content="Optimizer"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="../../../../img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/binzhango/binzhango.github.io/2020/02/02/Gradient-Descent/"},"headline":"Bin's Blog","image":["https://github.com/binzhango/binzhango.github.io/img/og_image.png"],"datePublished":"2020-02-03T02:04:06.000Z","dateModified":"2020-02-03T02:04:06.000Z","author":{"@type":"Person","name":"Bin Zhang"},"description":"gradient-based optimization algorithms  Gradient Descent variants  Batch Gradient Descent (BGD) Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t."}</script><link rel="canonical" href="https://github.com/binzhango/binzhango.github.io/2020/02/02/Gradient-Descent/"><link rel="icon" href="../../../../img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="../../../../css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="atom.xml" title="Bin's Blog" type="application/atom+xml">
<!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="../../../../index.html"><img src="../../../../img/logo.svg" alt="Bin&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="../../../../index.html">Home</a><a class="navbar-item" href="../../../../archives">Archives</a><a class="navbar-item" href="../../../../categories">Categories</a><a class="navbar-item" href="../../../../tags">Tags</a><a class="navbar-item" href="../../../../about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="../../../../https:/github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-02-03T02:04:06.000Z" title="2020-02-03T02:04:06.000Z">2020-02-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-03T02:04:06.000Z" title="2020-02-03T02:04:06.000Z">2020-02-02</time></span><span class="level-item"><a class="link-muted" href="../../../../categories/Machine-Learning/">Machine Learning</a></span><span class="level-item">9 minutes read (About 1312 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Gradient Descent</h1><div class="content"><h1 id="gradient-based-optimization-algorithms"><a class="markdownIt-Anchor" href="#gradient-based-optimization-algorithms"></a> gradient-based optimization algorithms</h1>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="gradient-descent-variants"><a class="markdownIt-Anchor" href="#gradient-descent-variants"></a> Gradient Descent variants</h2>
<h4 id="batch-gradient-descent-bgd"><a class="markdownIt-Anchor" href="#batch-gradient-descent-bgd"></a> Batch Gradient Descent (BGD)</h4>
<p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ</p>
<p>Batch gradient descent is guaranteed to converge</p>
<ul>
<li>to the global minimum for convex error surfaces</li>
<li>to a local minimum for non-convex surfaces</li>
</ul>
<h4 id="stochastic-gradient-descent-sgd"><a class="markdownIt-Anchor" href="#stochastic-gradient-descent-sgd"></a> Stochastic Gradient Descent (SGD)</h4>
<p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update.<br />
SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.<br />
SGD performs frequent updates with a high variance that cause the objective function to <em>fluctuate</em> heavily.<br />
While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation,</p>
<ul>
<li>enables it to jump to new and potentially better local minima</li>
<li>this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting</li>
</ul>
<p>when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a <em>local</em> or the <em>global</em> minimum for <em>non-convex</em> and <em>convex</em> optimization respectively.</p>
<h4 id="mini-batch-gradient-descent-mb-gd"><a class="markdownIt-Anchor" href="#mini-batch-gradient-descent-mb-gd"></a> Mini-batch Gradient Descent (MB-GD)</h4>
<p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples</p>
<ul>
<li>reduces the variance of the parameter updates, which can lead to more stable convergence</li>
<li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient</li>
<li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li>
</ul>
<h4 id="challenges"><a class="markdownIt-Anchor" href="#challenges"></a> Challenges</h4>
<ul>
<li><strong>Choosing a proper learning rate can be difficult.</strong></li>
</ul>
<blockquote>
<p>A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p>
</blockquote>
<ul>
<li><strong>Learning rete schedules try to adjust the learning rate during training</strong></li>
</ul>
<blockquote>
<p>e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics</p>
</blockquote>
<ul>
<li><strong>The same learning rate applies to all parameter updates</strong></li>
</ul>
<blockquote>
<p>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features</p>
</blockquote>
<ul>
<li><strong>Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima</strong></li>
</ul>
<blockquote>
<p>The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p>
</blockquote>
<h2 id="gradient-descent-optimization-algorithms"><a class="markdownIt-Anchor" href="#gradient-descent-optimization-algorithms"></a> Gradient Descent Optimization Algorithms</h2>
<p>We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton’s method.</p>
<h4 id="momentum"><a class="markdownIt-Anchor" href="#momentum"></a> Momentum</h4>
<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima.</p>
<p>Some implementations exchange the signs in the equations. The momentum term γ is usually set to 0.9 or a similar value.</p>
<p>When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill,<br />
becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1).<br />
<em>The same thing happens to our parameter updates</em>:</p>
<blockquote>
<p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain <em>faster convergence and reduced oscillation</em>.</p>
</blockquote>
<h4 id="nesterov-accelerated-gradient"><a class="markdownIt-Anchor" href="#nesterov-accelerated-gradient"></a> Nesterov Accelerated Gradient</h4>
<p>We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.<br />
Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience.<br />
We know that we will use our momentum term γvθ<sub>t-1</sub> to move the parameters θ.<br />
Computing θ−γv<sub>t-1</sub> thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update),<br />
a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient<br />
<em>not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters</em></p>
<p>we are able to adapt our updates to the slope of our error function and speed up SGD in turn,<br />
we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance</p>
<p>The distinction between Momentum method and Nesterov Accelerated Gradient updates was</p>
<ul>
<li>Both methods are distinct only when the learning rate η is reasonably large.</li>
<li>When the learning rate η is relatively large, Nesterov Accelerated Gradients allows larger decay rate α than Momentum method, while preventing oscillations.</li>
<li>Both Momentum method and Nesterov Accelerated Gradient <strong>become equivalent when η is small</strong></li>
</ul>
<h4 id="adagrad"><a class="markdownIt-Anchor" href="#adagrad"></a> Adagrad</h4>
<p>Adagrad is an algorithm for gradient-based optimization that does just this:<br />
It adapts the learning rate to the parameters,</p>
<ul>
<li>performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features,</li>
<li>and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</li>
</ul>
<p>For this reason, <strong>it is well-suited for dealing with sparse data.</strong></p>
<p>Previously, we performed an update for all parameters θ at once as every parameter θ<sub>i</sub> used the same learning rate η.<br />
As Adagrad uses a different learning rate for every parameter θ<sub>i</sub> at every time step t, we first show Adagrad’s per-parameter update, which we then vectorize.<br />
For brevity, we use gt to denote the gradient at time step t. g<sub>t,i</sub> is then the partial derivative of the objective function w.r.t. to the parameter θ<sub>i</sub> at time step t</p>
<p>In its update rule, Adagrad modifies the general learning rate η at each time step t for every parameter θ<sub>i</sub> based on the past gradients that have been computed for θ<sub>i</sub></p>
<p>θ<sub>t+1,i</sub>=θ<sub>t,i</sub>−η/√(G<sub>t,ii</sub>+ϵ)⋅g<sub>t,i</sub></p>
<p>G<sub>t</sub>∈R<sup>d×d</sup> here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θ<sub>i</sub> up to time step t,<br />
while ϵ is a smoothing term that avoids division by zero.<br />
<strong>Interestingly, without the square root operation, the algorithm performs much worse.</strong></p>
<ul>
<li>One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate</li>
<li>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator</li>
</ul>
<blockquote>
<p>Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>
</blockquote>
<h4 id="adadelta"><a class="markdownIt-Anchor" href="#adadelta"></a> Adadelta</h4>
<p>Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate.<br />
Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p>
<p>Instead of inefficiently storing w previous squared gradients,<br />
the sum of gradients is recursively defined as a decaying average of all past squared gradients.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Gradient Descent</p><p><a href="https://github.com/binzhango/binzhango.github.io/2020/02/02/Gradient-Descent/">https://github.com/binzhango/binzhango.github.io/2020/02/02/Gradient-Descent/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Bin Zhang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-02-02</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2020-02-02</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="../../../../https:/creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="../../../../https:/creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="../../../../https:/creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="../../../../tags/Optimizer/">Optimizer</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="../../../../" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="../../../../" alt="Alipay"></span></a><a class="button donate" href="../../../../" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="../../../../" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="../../../../" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="../../04/Batch-Normalization/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Batch Normalization</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="../../01/hello-world/"><span class="level-item">Hello World</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="../../../../img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="../../../../archives"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="../../../../categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="../../../../tags"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="../../../../https:/github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="../../../../https:/github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="../../../../https:/facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="../../../../https:/twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="../../../../https:/dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="../../../../index.html"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#gradient-based-optimization-algorithms"><span class="level-left"><span class="level-item">1</span><span class="level-item"> gradient-based optimization algorithms</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#gradient-descent-variants"><span class="level-left"><span class="level-item">1.1</span><span class="level-item"> Gradient Descent variants</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#challenges"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item"> Challenges</span></span></a></li></ul></li><li><a class="level is-mobile" href="#gradient-descent-optimization-algorithms"><span class="level-left"><span class="level-item">1.2</span><span class="level-item"> Gradient Descent Optimization Algorithms</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#adadelta"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item"> Adadelta</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="../../../../js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="../../../../categories/DataFactory/"><span class="level-start"><span class="level-item">DataFactory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="../../../../categories/Hexo/"><span class="level-start"><span class="level-item">Hexo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="../../../../categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../../../categories/Spark/"><span class="level-start"><span class="level-item">Spark</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../../../categories/airflow/"><span class="level-start"><span class="level-item">airflow</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-11-19T02:27:29.000Z">2020-11-18</time></p><p class="title"><a href="../../../11/18/Azure-Data-Factory-Data-Flow/">Azure Data Factory (Data Flow)</a></p><p class="categories"><a href="../../../../categories/DataFactory/">DataFactory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-03-01T23:35:36.000Z">2020-03-01</time></p><p class="title"><a href="../../../03/01/Spark-Dataframe-window-function/">Spark Dataframe window function</a></p><p class="categories"><a href="../../../../categories/Spark/">Spark</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-21T19:10:36.000Z">2020-02-21</time></p><p class="title"><a href="../../21/Spark-Optimizaion/">Spark Optimizaion</a></p><p class="categories"><a href="../../../../categories/Spark/">Spark</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-12T03:20:40.000Z">2020-02-11</time></p><p class="title"><a href="../../11/Airflow-1/">Airflow-- 1</a></p><p class="categories"><a href="../../../../categories/airflow/">airflow</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-02-11T13:17:08.000Z">2020-02-11</time></p><p class="title"><a href="../../11/Whitening-transformation/">Whitening transformation</a></p><p class="categories"><a href="../../../../categories/Machine-Learning/">Machine Learning</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="../../../../archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="../../../../archives/2020/03/"><span class="level-start"><span class="level-item">March 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="../../../../archives/2020/02/"><span class="level-start"><span class="level-item">February 2020</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="../../../../tags/Azure/"><span class="tag">Azure</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="../../../../tags/Dataframe/"><span class="tag">Dataframe</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="../../../../tags/Optimization/"><span class="tag">Optimization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="../../../../tags/Optimizer/"><span class="tag">Optimizer</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="../../../../tags/Streaming/"><span class="tag">Streaming</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="../../../../index.html"><img src="../../../../img/logo.svg" alt="Bin&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2020 Bin Zhang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="../../../../https:/creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="../../../../https:/creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="../../../../https:/github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="../../../../js/column.js"></script><script src="../../../../js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="../../../../js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="../../../../js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="../../../../js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"../../../../content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({{ JSON.stringify(config) }});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="{{ src }}">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>