<!DOCTYPE html>
<html lang="en" >
<head>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
	<meta name="description" content="">
	<meta name="keywords" content="">
	<title>Archive - Bin&#39;s Blog</title>
    <link rel="alternate" href="" type="application/atom+xml"/>
	<link rel="shortcut icon" href=""/>
	
<link rel="stylesheet" href="/css/style.css">

	
<link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
	<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="main-con">

        <div class="nav cl">
    <ul class="cl nav-list">
        
            <li>
                
                    <a href="/" class="">
                        <i class="fa fa-home"></i> 
                        <span>主页</span>
                    </a>
                
            </li>
        
            <li>
                
                    <a href="/archives/" class="active">
                        <i class=" fa-"></i> 
                        <span>归档</span>
                    </a>
                
            </li>
        
            <li>
                
                    
                    <a href="javascript:void(0)" class="">
                    
                        <i class=" fa-"></i> 
                        <span>关于</span>
                        <span class="drop-flag fa fa-angle-down"></span>
                    </a>
                    <dl>
                        
                            <li>
                                <a href="/about" class="">
                                    <i class=" fa-"></i>
                                    <span>关于本站</span>
                                </a>
                            </li>
                        
                    </dl>
                
            </li>
        
    </ul>
    <ul class="cl nav-tool">
        
            <li>
                <a href="/github">
                    <i class="fa fa-github"></i>
                </a>
            </li>
        
            <li>
                <a href="/mail">
                    <i class="fa fa-envelope"></i>
                </a>
            </li>
        
            <li>
                <a href="/twitter">
                    <i class="fa fa-twitter"></i>
                </a>
            </li>
        
        
        <li>
            <a href="javascript:void(0)" class="nav-search-btn">
                <i class="fa fa-search"></i>
            </a>
        </li>
        
    </ul>
    <form action="//google.com/search" method="get" accept-charset="UTF-8" class="nav-search"><input type="search" name="q" class="nav-search-input" placeholder="search..."><input type="hidden" name="sitesearch" value="https://github.com/binzhango/binzhango.github.io"></form>
</div>

        <header class="top" id="fallEle" style="background-image: url(/imgs/head.jpg)">
    <i class="fa fa-bars" id="media-toggle" style="display: none"></i>
    <div class="top-info cl fadeToBottom">
        <h2 class="site-name"><a href="/">mokusei</a> <small id="type-data">This is a hexo theme</small></h2>
    </div>
</header>

        
<div class="con-wrap fadeToTop archive">
    
    <section class="article-area">
    
        
        
            
                <div class="archive-year"><span class="year-block">2020</span></div>

            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/11/18/Azure-Data-Factory-Data-Flow/" title="Azure Data Factory (Data Flow)">
                    Azure Data Factory (Data Flow)
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-11-18
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/DataFactory/">DataFactory</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <p>Recently I’m working in Azuer to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work.</p>
<h2 id="parameter-vs-variable"><a class="markdownIt-Anchor" href="#parameter-vs-variable"></a> parameter VS variable</h2>
<p>Pipeline parameter must be defined before you start pipeline, whereas variables are set and used in running time.</p>
<h2 id="get-metadata"><a class="markdownIt-Anchor" href="#get-metadata"></a> Get Metadata</h2>
<p>The most important setting is <strong>Augument</strong><br />
<img src="getmetadata.png" alt="Get Metadata" /></p>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/03/01/Spark-Dataframe-window-function/" title="Spark Dataframe window function">
                    Spark Dataframe window function
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-03-01
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Spark/">Spark</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="noopener">scala ref</a></p>
<h2 id="create-dataframe"><a class="markdownIt-Anchor" href="#create-dataframe"></a> create dataframe</h2>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/21/Spark-Optimizaion/" title="Spark Optimizaion">
                    Spark Optimizaion
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-21
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Spark/">Spark</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <h1 id="spark-run-faster-and-faster"><a class="markdownIt-Anchor" href="#spark-run-faster-and-faster"></a> Spark run faster and faster</h1>
<ul>
<li>Cluster Optimization</li>
<li>Parameters Optimization</li>
<li>Code Optimization</li>
</ul>
<h2 id="cluster-optimization"><a class="markdownIt-Anchor" href="#cluster-optimization"></a> Cluster Optimization</h2>
<h4 id="locality-level"><a class="markdownIt-Anchor" href="#locality-level"></a> Locality Level</h4>
<p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from closest to farthest:</p>
<ul>
<li><strong>PROCESS_LOCAL</strong> data is in the same JVM as the running code. This is the best locality possible</li>
<li><strong>NODE_LOCAL</strong> data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li>
<li><strong>NO_PREF</strong> data is accessed equally quickly from anywhere and has no locality preference</li>
<li><strong>RACK_LOCAL</strong> data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li>
<li><strong>ANY</strong> data is elsewhere on the network and not in the same rack</li>
</ul>
<p>Performance: PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL</p>
<h6 id="locality-settting"><a class="markdownIt-Anchor" href="#locality-settting"></a> Locality settting</h6>
<ul>
<li>spark.locality.wait.process</li>
<li>spark.locality.wait.node</li>
<li>spark.locality.wait.rack</li>
</ul>
<h4 id="data-format"><a class="markdownIt-Anchor" href="#data-format"></a> Data Format</h4>
<ul>
<li>text</li>
<li>orc</li>
<li>parquet</li>
<li>avro</li>
</ul>
<h6 id="format-setting"><a class="markdownIt-Anchor" href="#format-setting"></a> format setting</h6>
<ul>
<li>spark.sql.hive.convertCTAS</li>
<li>spark.sql.sources.default</li>
</ul>
<h4 id="parallelising"><a class="markdownIt-Anchor" href="#parallelising"></a> parallelising</h4>
<ul>
<li>spark.sql.shuffle.partitions : default is 200</li>
</ul>
<h4 id="computing"><a class="markdownIt-Anchor" href="#computing"></a> computing</h4>
<ul>
<li>–executor-memory : default is 1G</li>
<li>–executor-cores : default is 1<br />
if large memory cause resource throtle in cluster, if small memory cause task termination<br />
if more cores cause IO issue, if less cores slow dow computing</li>
</ul>
<h4 id="memory"><a class="markdownIt-Anchor" href="#memory"></a> memory</h4>
<ul>
<li>spark.executor.overhead.memory</li>
</ul>
<h4 id="table-join"><a class="markdownIt-Anchor" href="#table-join"></a> table join</h4>
<ul>
<li>spark.sql.autoBroadcastJoinThreshold : default 10M</li>
</ul>
<h4 id="predicate-push-down-in-spark-sql-queries"><a class="markdownIt-Anchor" href="#predicate-push-down-in-spark-sql-queries"></a> predicate push down in Spark SQL queries</h4>
<ul>
<li>spark.sql.parquet.filterPushdown : default True</li>
<li>spark.sql.orc.filterPushdown=true : default False</li>
</ul>
<h4 id="reuse-rdd"><a class="markdownIt-Anchor" href="#reuse-rdd"></a> reuse RDD</h4>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.persist(pyspark.StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure>
<h4 id="spark-operators"><a class="markdownIt-Anchor" href="#spark-operators"></a> Spark operators</h4>
<ul>
<li>
<p>shuffle operators</p>
<ul>
<li>avoid using <span style="color:blue"> <strong>reduceByKey</strong>, <strong>join</strong>, <strong>distinct</strong>, <strong>repartition</strong> etc</span></li>
<li>Broadcast small dataset</li>
</ul>
</li>
<li>
<p>High performance operator</p>
<ul>
<li>reduceByKey &gt; groupByKey (reduceByKey works at map side)</li>
<li>mapPartitions &gt; map (reduce function calls)</li>
<li>treeReduce &gt; reduce (treeReduce works at executor not driver)
<ul>
<li>treeReduce &amp; reduce return some result to driver</li>
<li>treeReduce does more work on the executors while reduce bring everything back to the driver.</li>
</ul>
</li>
<li>foreachPartitions &gt; foreach (reduce function calls)</li>
<li>filter -&gt; coalesce (reduce number of partitions and reduce tasks)</li>
<li>repartitionAndSortWithinPartitions &gt; repartition &amp; sort</li>
<li>broadcast (100M)</li>
</ul>
</li>
</ul>
<h4 id="shuffle"><a class="markdownIt-Anchor" href="#shuffle"></a> shuffle</h4>
<ul>
<li>spark.shuffle.sort.bypassMergeThreshold</li>
<li>spark.shuffle.io.retryWait</li>
<li>spark.shuffle.io.maxRetries</li>
</ul>
<p>TBC</p>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/11/Airflow-1/" title="Airflow-- 1">
                    Airflow-- 1
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-11
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/airflow/">airflow</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <h2 id="all-in-one"><a class="markdownIt-Anchor" href="#all-in-one"></a> all in one</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#调度机中的dag代码示例</span></span><br><span class="line"><span class="comment">#生产机中的dag要放置到与调度机同样的目录下，并且将执行过程增加到*_function()</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> airflow</span><br><span class="line"><span class="keyword">from</span> airflow.models <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.operators.python_operator <span class="keyword">import</span> PythonOperator</span><br><span class="line"> </span><br><span class="line">default_args = &#123;</span><br><span class="line">	<span class="string">'owner'</span>: <span class="string">'xiaoming'</span>,</span><br><span class="line">	<span class="string">'start_date'</span>: airflow.utils.dates.days_ago(<span class="number">1</span>),</span><br><span class="line">	<span class="string">'depends_on_past'</span>: <span class="literal">False</span>,</span><br><span class="line">    <span class="comment"># 失败发邮件</span></span><br><span class="line">	<span class="string">'email'</span>: [<span class="string">'xiaoming@163.com'</span>],</span><br><span class="line">	<span class="string">'email_on_failure'</span>: <span class="literal">True</span>,</span><br><span class="line">	<span class="string">'email_on_retry'</span>: <span class="literal">True</span>,</span><br><span class="line">	<span class="comment"># 重试相关</span></span><br><span class="line">	<span class="string">'retries'</span>: <span class="number">3</span>,</span><br><span class="line">	<span class="string">'retry_delay'</span>: timedelta(minutes=<span class="number">5</span>),</span><br><span class="line">	<span class="comment"># 并发限制</span></span><br><span class="line">	<span class="string">'pool'</span>: <span class="string">'data_hadoop_pool'</span>,</span><br><span class="line">	<span class="string">'priority_weight'</span>: <span class="number">900</span>,</span><br><span class="line">	<span class="comment"># 按机器名指定运行位置</span></span><br><span class="line">	<span class="string">'queue'</span>: <span class="string">'66.66.0.66:8080'</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">dag = DAG(</span><br><span class="line">    dag_id=<span class="string">'daily'</span>, </span><br><span class="line">    default_args=default_args, <span class="comment">#配置默认参数</span></span><br><span class="line">    schedule_interval=<span class="string">'0 13 * * *'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#生产机中，将具体执行过程放置在该函数下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_data_from_hdfs_function</span><span class="params">(ds, **kwargs)</span>:</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#生产机中，将具体执行过程放置在该函数下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">push_data_to_mysql_function</span><span class="params">(ds, **kwargs)</span>:</span></span><br><span class="line">	<span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">fetch_data_from_hdfs = PythonOperator(</span><br><span class="line">	task_id=<span class="string">'fetch_data_from_hdfs'</span>,</span><br><span class="line">	provide_context=<span class="literal">True</span>,</span><br><span class="line">	python_callable=fetch_data_from_hdfs_function,</span><br><span class="line">	dag=dag)</span><br><span class="line"> </span><br><span class="line">push_data_to_mysql = PythonOperator(</span><br><span class="line">	task_id=<span class="string">'push_data_to_mysql'</span>,</span><br><span class="line">	provide_context=<span class="literal">True</span>,</span><br><span class="line">	python_callable=push_data_to_mysql_function,</span><br><span class="line">	dag=dag)</span><br><span class="line"> </span><br><span class="line">fetch_data_from_hdfs &gt;&gt; push_data_to_mysql</span><br></pre></td></tr></table></figure>
<h2 id="update"><a class="markdownIt-Anchor" href="#update"></a> update</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#该task未修改参数，采用默认参数</span></span><br><span class="line">fetch_data_from_hdfs = PythonOperator(</span><br><span class="line">	task_id=<span class="string">'fetch_data_from_hdfs'</span>,</span><br><span class="line">	provide_context=<span class="literal">True</span>,</span><br><span class="line">	python_callable=fetch_data_from_hdfs_function,</span><br><span class="line">	dag=dag)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#该task修改通过指定参数，覆盖默认参数，调整调度行为</span></span><br><span class="line">push_data_to_mysql = PythonOperator(</span><br><span class="line">    task_id=<span class="string">'push_data_to_mysql'</span>,</span><br><span class="line">    queue=<span class="string">'77.66.0.66:8080'</span>, <span class="comment">#通过修改参数，调整调度</span></span><br><span class="line">    pool=<span class="string">'data_mysql_pool'</span>, <span class="comment">#通过修改参数，调整调度</span></span><br><span class="line">    provide_context=<span class="literal">True</span>,</span><br><span class="line">    python_callable=push_data_to_mysql_function,</span><br><span class="line">    dag=dag)</span><br></pre></td></tr></table></figure>
<h2 id="decouple"><a class="markdownIt-Anchor" href="#decouple"></a> decouple</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xx.fetch_data_from_hdfs <span class="comment">#将包装成函数的业务代码引入</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#生产机中，将具体执行过程放置在该函数下</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_data_from_hdfs_function</span><span class="params">(ds, **kwargs)</span>:</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> fetch_data_from_hdfs: <span class="comment">#判断业务代码是否执行成功，不成功报错</span></span><br><span class="line">        <span class="keyword">raise</span> AirflowException(<span class="string">'run fail: fetch_data_from_hdfs'</span>)</span><br><span class="line"> </span><br><span class="line">fetch_data_from_hdfs = PythonOperator(</span><br><span class="line">	task_id=<span class="string">'fetch_data_from_hdfs'</span>,</span><br><span class="line">	provide_context=<span class="literal">True</span>,</span><br><span class="line">	python_callable=fetch_data_from_hdfs_function,</span><br><span class="line">	dag=dag)</span><br></pre></td></tr></table></figure>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/11/Whitening-transformation/" title="Whitening transformation">
                    Whitening transformation
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-11
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Machine Learning/">Machine Learning</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            
        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/08/Spark-Structured-Streaming/" title="Spark Structured Streaming">
                    Spark Structured Streaming
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-08
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Spark/">Spark</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <h2 id="spark-structured-streaming"><a class="markdownIt-Anchor" href="#spark-structured-streaming"></a> Spark Structured Streaming</h2>
<p>Recently reading a blog <a href="https://hackersandslackers.com/structured-streaming-in-pyspark/" target="_blank" rel="noopener">Structured Streaming in PySpark</a><br />
It’s implemented in Databricks platform. Then I try to reimplement in my local Spark.<br />
Some tricky issue happend during my work.</p>
<h2 id="reading-data"><a class="markdownIt-Anchor" href="#reading-data"></a> Reading Data</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType, StringType, StructType, StructField</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"Test Streaming"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line">json_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"time"</span>, TimestampType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"customer"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"action"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"device"</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">"local_file_path&lt;file:///..."</span></span><br></pre></td></tr></table></figure>
<h4 id="read-json-as-same-as-method-in-the-blog"><a class="markdownIt-Anchor" href="#read-json-as-same-as-method-in-the-blog"></a> read json as same as method in the blog</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.schema(json_schema).json(file_path)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |time|customer|action|device|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line">input.count()</span><br><span class="line"><span class="comment"># 20000</span></span><br></pre></td></tr></table></figure>
<p>All values are null, however, the count is right. It means spark has already read all data but the schema is not correctly mapped.</p>
<h4 id="read-a-single-json-file-to-check-schema"><a class="markdownIt-Anchor" href="#read-a-single-json-file-to-check-schema"></a> read a single json file to check schema</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.schema(json_schema).json(file_path+<span class="string">'/1.json'</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |time|customer|action|device|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># same error</span></span><br><span class="line"><span class="comment"># Then I drop schema option and use inferSchema</span></span><br><span class="line">input = spark.read.json(file_path+<span class="string">'/1.json'</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |     _corrupt_record|     action|         customer|              device|           time|</span></span><br><span class="line"><span class="comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |[&#123;"time":"3:57:09...|       null|             null|                null|           null|</span></span><br><span class="line"><span class="comment"># |                null|  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|</span></span><br><span class="line"><span class="comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></span><br></pre></td></tr></table></figure>
<p>A weird column is <em>_corrupt_record</em> and first value is <strong>[{“time”:&quot;3:57:09…</strong> in this column.<br />
Go back to check source file and notice that it’s a list of object in json file.</p>
<h6 id="remove-span-stylecolorred-span-and-span-stylecolorred-span-in-source-file"><a class="markdownIt-Anchor" href="#remove-span-stylecolorred-span-and-span-stylecolorred-span-in-source-file"></a> Remove  <span style='color:red'> <em>[</em> </span> and <span style='color:red'><em>]</em> </span> in source file</h6>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.json(file_path+<span class="string">'/1.json'</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |     action|         customer|              device|           time|</span></span><br><span class="line"><span class="comment"># +-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |  power off|      Alexi Barts|  GreenIQ Controller| 3:57:09.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|</span></span><br><span class="line"><span class="comment"># |   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|</span></span><br><span class="line"><span class="comment"># +-----------+-----------------+--------------------+---------------+</span></span><br></pre></td></tr></table></figure>
<p>Woo, the dataframe is correct. Let’s check schema</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input.printSchema()</span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- action: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- customer: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- device: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- time: string (nullable = true)</span></span><br></pre></td></tr></table></figure>
<p>So far I manually modify source file and drop external schema to obtain a corret dataframe. Is there anyway to<br />
read these files without these steps.</p>
<h6 id="add-one-feature-span-stylecolorbluemultilinespan"><a class="markdownIt-Anchor" href="#add-one-feature-span-stylecolorbluemultilinespan"></a> add one feature <span style='color:blue'>multiLine</span></h6>
<p>Read the file without schema but add one feature <strong>multiLine</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.json(<span class="string">"file:///path/pyspark_test_data"</span>, multiLine=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># OR input = spark.read.option('multiLine', True).json("file:///path/pyspark_test_data")</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----------+--------------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |     action|            customer|              device|           time|</span></span><br><span class="line"><span class="comment"># +-----------+--------------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |   power on|     Raynor Blaskett|Nest T3021US Ther...| 3:35:09.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|Stafford Blakebrough|  GreenIQ Controller|10:59:46.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|      Alex Woolcocks|Nest T3021US Ther...| 6:26:36.000 PM|</span></span><br><span class="line"><span class="comment"># |   power on|      Clarice Nayshe|Footbot Air Quali...| 4:46:28.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off|      Killie Pirozzi|Footbot Air Quali...| 8:58:43.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|    Lynne Dymidowicz|Footbot Air Quali...| 4:20:49.000 PM|</span></span><br><span class="line"><span class="comment"># |   power on|       Shaina Dowyer|             ecobee4| 3:41:33.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|       Barbee Melato| August Doorbell Cam|10:40:24.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|        Clem Westcot|Nest T3021US Ther...|11:13:38.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|       Kerri Galfour|         Amazon Echo|10:12:15.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|        Trev Ashmore|  GreenIQ Controller|11:04:41.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|      Coral Jahnisch| August Doorbell Cam| 3:06:31.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|      Feliza Cowdrey|Nest T3021US Ther...| 2:49:02.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off|   Amabelle De Haven|Footbot Air Quali...|12:11:59.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|     Benton Redbourn|Nest T3021US Ther...| 3:57:39.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|        Asher Potten| August Doorbell Cam| 1:34:44.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|    Lorianne Hullyer| August Doorbell Cam| 7:26:42.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|     Ruperto Aldcorn|Footbot Air Quali...| 3:54:49.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|   Agatha Di Giacomo|Footbot Air Quali...| 7:15:20.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|    Eunice Penwright|             ecobee4|11:14:14.000 PM|</span></span><br><span class="line"><span class="comment"># +-----------+--------------------+--------------------+---------------+</span></span><br><span class="line"></span><br><span class="line">input.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- action: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- customer: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- device: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- time: string (nullable = true)</span></span><br></pre></td></tr></table></figure>
<h4 id="change-the-schema"><a class="markdownIt-Anchor" href="#change-the-schema"></a> change the schema</h4>
<p>Set time as <em>StringType</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">json_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"time"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"customer"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"action"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"device"</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input = spark.read.schema(json_schema).json(<span class="string">"file:///path/pyspark_test_data"</span>, multiLine=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +---------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># |           time|            customer|     action|              device|</span></span><br><span class="line"><span class="comment"># +---------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># | 3:35:09.000 AM|     Raynor Blaskett|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |10:59:46.000 AM|Stafford Blakebrough|   power on|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># | 6:26:36.000 PM|      Alex Woolcocks|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># | 4:46:28.000 AM|      Clarice Nayshe|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 8:58:43.000 AM|      Killie Pirozzi|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 4:20:49.000 PM|    Lynne Dymidowicz|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 3:41:33.000 AM|       Shaina Dowyer|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># |10:40:24.000 PM|       Barbee Melato|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |11:13:38.000 PM|        Clem Westcot|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |10:12:15.000 PM|       Kerri Galfour|  power off|         Amazon Echo|</span></span><br><span class="line"><span class="comment"># |11:04:41.000 AM|        Trev Ashmore|low battery|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># | 3:06:31.000 AM|      Coral Jahnisch|   power on| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># | 2:49:02.000 AM|      Feliza Cowdrey|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |12:11:59.000 PM|   Amabelle De Haven|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 3:57:39.000 AM|     Benton Redbourn|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># | 1:34:44.000 AM|        Asher Potten|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># | 7:26:42.000 PM|    Lorianne Hullyer|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># | 3:54:49.000 AM|     Ruperto Aldcorn|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 7:15:20.000 AM|   Agatha Di Giacomo|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |11:14:14.000 PM|    Eunice Penwright|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># +---------------+--------------------+-----------+--------------------+</span></span><br></pre></td></tr></table></figure>
<p>Pyspark can load json files successfully without TimestampType. However, how to handle timestamp issue in this job?</p>
<h4 id="timestamptype"><a class="markdownIt-Anchor" href="#timestamptype"></a> TimestampType</h4>
<p>In offical document, the class <em>pyspark.sql.DataFrameReader</em> has one parameter</p>
<ul>
<li>timestampFormat</li>
</ul>
<blockquote>
<p>sets the string that indicates a timestamp format.</p>
<p>Custom date formats follow the formats at java.text.SimpleDateFormat.</p>
<p>This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss.SSSXXX.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.schema(schema).option(<span class="string">"multiLine"</span>, <span class="literal">True</span>).json(<span class="string">"file:///path/pyspark_test_data"</span>, timestampFormat=<span class="string">"h:mm:ss.SSS aa"</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"><span class="comment"># +-------------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># |               time|            customer|     action|              device|</span></span><br><span class="line"><span class="comment"># +-------------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:35:09|     Raynor Blaskett|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 10:59:46|Stafford Blakebrough|   power on|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># |1970-01-01 18:26:36|      Alex Woolcocks|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 04:46:28|      Clarice Nayshe|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 08:58:43|      Killie Pirozzi|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 16:20:49|    Lynne Dymidowicz|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:41:33|       Shaina Dowyer|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># |1970-01-01 22:40:24|       Barbee Melato|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 23:13:38|        Clem Westcot|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 22:12:15|       Kerri Galfour|  power off|         Amazon Echo|</span></span><br><span class="line"><span class="comment"># |1970-01-01 11:04:41|        Trev Ashmore|low battery|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:06:31|      Coral Jahnisch|   power on| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 02:49:02|      Feliza Cowdrey|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 12:11:59|   Amabelle De Haven|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:57:39|     Benton Redbourn|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 01:34:44|        Asher Potten|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 19:26:42|    Lorianne Hullyer|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:54:49|     Ruperto Aldcorn|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 07:15:20|   Agatha Di Giacomo|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 23:14:14|    Eunice Penwright|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># +-------------------+--------------------+-----------+--------------------+</span></span><br></pre></td></tr></table></figure>
<p>All yyyy-MM-dd are 1970-01-01 because source file only hh-mm-ss.<br />
These source files are in wrong format in Windows.</p>
<h2 id="streaming-our-data"><a class="markdownIt-Anchor" href="#streaming-our-data"></a> Streaming Our Data</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType, StringType, StructType, StructField</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"Test Streaming"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line">json_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"time"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"customer"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"action"</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">"device"</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">streamingDF = spark.readStream.schema(json_schema) \</span><br><span class="line">              .option(<span class="string">"maxFilesPerTrigger"</span>, <span class="number">1</span>) \</span><br><span class="line">              .option(<span class="string">"multiLine"</span>, <span class="literal">True</span>) \</span><br><span class="line">              .json(<span class="string">"file:///path/pyspark_test_data"</span>)</span><br><span class="line"></span><br><span class="line">streamingActionCountsDF = streamingDF.groupBy(<span class="string">'action'</span>).count()</span><br><span class="line"><span class="comment"># streamingActionCountsDF.isStreaming</span></span><br><span class="line">spark.conf.set(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"2"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># View stream in real-time</span></span><br><span class="line"><span class="comment"># query = streamingActionCountsDF.writeStream \</span></span><br><span class="line"><span class="comment">#         .format("memory").queryName("counts").outputMode("complete").start()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># format choice:</span></span><br><span class="line"><span class="comment"># parquet</span></span><br><span class="line"><span class="comment"># kafka</span></span><br><span class="line"><span class="comment"># console</span></span><br><span class="line"><span class="comment"># memory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># query = streamingActionCountsDF.writeStream \</span></span><br><span class="line"><span class="comment">#         .format("console").queryName("counts").outputMode("complete").start()</span></span><br><span class="line"></span><br><span class="line">query = streamingActionCountsDF.writeStream.format(<span class="string">"console"</span>) \</span><br><span class="line">        .queryName(<span class="string">"counts"</span>).outputMode(<span class="string">"complete"</span>).start().awaitTermination(timeout=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># Output Mode choice:</span></span><br><span class="line"><span class="comment"># append</span></span><br><span class="line"><span class="comment"># complete</span></span><br><span class="line"><span class="comment"># update</span></span><br></pre></td></tr></table></figure>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/04/Batch-Normalization/" title="Batch Normalization">
                    Batch Normalization
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-04
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Machine Learning/">Machine Learning</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <p>Batch Normalization is one of important parts in our NN.</p>
<h2 id="why-need-normalization"><a class="markdownIt-Anchor" href="#why-need-normalization"></a> Why need Normalization</h2>
<p>This paper title tells me the reason<br />
<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>
<ul>
<li>accelerating traning</li>
<li>reduce internal covariate shift</li>
</ul>
<h4 id="independent-and-identically-distributed-iid"><a class="markdownIt-Anchor" href="#independent-and-identically-distributed-iid"></a> Independent and identically distributed (IID)</h4>
<p>If our data is independent and identically distributed, training model can be simplified and its predictive ability is improved.<br />
One important step of data preparation is <strong>whitening</strong> which is used to</p>
<h6 id="whitening"><a class="markdownIt-Anchor" href="#whitening"></a> Whitening</h6>
<ul>
<li>reduce features’ coralation     =&gt; Independent</li>
<li>all features have zero mean and unit variances =&gt; Identically distributed</li>
</ul>
<h4 id="internal-covariate-shift-ics"><a class="markdownIt-Anchor" href="#internal-covariate-shift-ics"></a> Internal Covariate Shift (ICS)</h4>
<p>What is problem of ICS? Generally data is not IID</p>
<ul>
<li>Previous layer should update hyper-parameters to adjust new data so that reduce learning speed</li>
<li>Get stuck in the saturation region as the network grows deeper and network stop learning earlier</li>
</ul>
<h6 id="covariate-shift"><a class="markdownIt-Anchor" href="#covariate-shift"></a> Covariate Shift</h6>
<blockquote>
<p>What is covariate shift? While in the process <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><mo>→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">X \rightarrow Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07847em;">X</span><span class="mrel">→</span><span class="mord mathit" style="margin-right:0.22222em;">Y</span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>P</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mo>=</mo><msup><mi>P</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo>(</mo><mi>y</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P^{train}(y|x) = P^{test}(y|x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8746639999999999em;"></span><span class="strut bottom" style="height:1.1246639999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">a</span><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathrm">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mi>u</mi><mi>t</mi><mspace width="0.277778em"></mspace><msup><mi>P</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msup><mo>(</mo><mi>x</mi><mo>)</mo><mo>≠</mo><msup><mi>P</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msup><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">but\; P^{train}(x) \neq P^{test}(x)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8746639999999999em;"></span><span class="strut bottom" style="height:1.1246639999999999em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">b</span><span class="mord mathit">u</span><span class="mord mathit">t</span><span class="mord mspace thickspace"></span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">a</span><span class="mord mathit">i</span><span class="mord mathit">n</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">P</span><span class="vlist"><span style="top:-0.413em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">e</span><span class="mord mathit">s</span><span class="mord mathit">t</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p>
</blockquote>
<h1 id="todo"><a class="markdownIt-Anchor" href="#todo"></a> ToDo</h1>
<h2 id="normalizations"><a class="markdownIt-Anchor" href="#normalizations"></a> Normalizations</h2>
<ul>
<li>weight scale invariance</li>
<li>data scale invariance</li>
</ul>
<h4 id="batch-normalization"><a class="markdownIt-Anchor" href="#batch-normalization"></a> Batch Normalization</h4>
<h4 id="layer-normalization"><a class="markdownIt-Anchor" href="#layer-normalization"></a> Layer Normalization</h4>
<h4 id="weight-normalization"><a class="markdownIt-Anchor" href="#weight-normalization"></a> Weight Normalization</h4>
<h4 id="cosine-normalization"><a class="markdownIt-Anchor" href="#cosine-normalization"></a> Cosine Normalization</h4>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/02/Gradient-Descent/" title="Gradient Descent">
                    Gradient Descent
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-02
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Machine Learning/">Machine Learning</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <h1 id="gradient-based-optimization-algorithms"><a href="#gradient-based-optimization-algorithms" class="headerlink" title="gradient-based optimization algorithms"></a>gradient-based optimization algorithms</h1><h2 id="Gradient-Descent-variants"><a href="#Gradient-Descent-variants" class="headerlink" title="Gradient Descent variants"></a>Gradient Descent variants</h2><h4 id="Batch-Gradient-Descent-BGD"><a href="#Batch-Gradient-Descent-BGD" class="headerlink" title="Batch Gradient Descent (BGD)"></a>Batch Gradient Descent (BGD)</h4><p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ</p>
<p>Batch gradient descent is guaranteed to converge </p>
<ul>
<li>to the global minimum for convex error surfaces</li>
<li>to a local minimum for non-convex surfaces</li>
</ul>
<h4 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h4><p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update.<br>SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.<br>SGD performs frequent updates with a high variance that cause the objective function to <em>fluctuate</em> heavily.<br>While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation,</p>
<ul>
<li>enables it to jump to new and potentially better local minima</li>
<li>this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting</li>
</ul>
<p>when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a <em>local</em> or the <em>global</em> minimum for <em>non-convex</em> and <em>convex</em> optimization respectively.</p>
<h4 id="Mini-batch-Gradient-Descent-MB-GD"><a href="#Mini-batch-Gradient-Descent-MB-GD" class="headerlink" title="Mini-batch Gradient Descent (MB-GD)"></a>Mini-batch Gradient Descent (MB-GD)</h4><p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples</p>
<ul>
<li>reduces the variance of the parameter updates, which can lead to more stable convergence</li>
<li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient</li>
<li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li>
</ul>
<h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><ul>
<li><p><strong>Choosing a proper learning rate can be difficult.</strong></p>
<blockquote>
<p>A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p>
</blockquote>
</li>
<li><p><strong>Learning rete schedules try to adjust the learning rate during training</strong></p>
<blockquote>
<p>e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics</p>
</blockquote>
</li>
<li><p><strong>The same learning rate applies to all parameter updates</strong></p>
<blockquote>
<p>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features</p>
</blockquote>
</li>
<li><p><strong>Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima</strong></p>
<blockquote>
<p>The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p>
</blockquote>
</li>
</ul>
<h2 id="Gradient-Descent-Optimization-Algorithms"><a href="#Gradient-Descent-Optimization-Algorithms" class="headerlink" title="Gradient Descent Optimization Algorithms"></a>Gradient Descent Optimization Algorithms</h2><p>We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton’s method.</p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima.</p>
<p>Some implementations exchange the signs in the equations. The momentum term γ is usually set to 0.9 or a similar value.</p>
<p>When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill,<br>becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1).<br><em>The same thing happens to our parameter updates</em>: </p>
<blockquote>
<p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain <em>faster convergence and reduced oscillation</em>.</p>
</blockquote>
<h4 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h4><p>We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.<br>Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience.<br>We know that we will use our momentum term γvθ<sub>t-1</sub> to move the parameters θ.<br>Computing θ−γv<sub>t-1</sub> thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update),<br>a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient<br><em>not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters</em></p>
<p>we are able to adapt our updates to the slope of our error function and speed up SGD in turn,<br>we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance</p>
<p>The distinction between Momentum method and Nesterov Accelerated Gradient updates was</p>
<ul>
<li>Both methods are distinct only when the learning rate η is reasonably large. </li>
<li>When the learning rate η is relatively large, Nesterov Accelerated Gradients allows larger decay rate α than Momentum method, while preventing oscillations. </li>
<li>Both Momentum method and Nesterov Accelerated Gradient <strong>become equivalent when η is small</strong></li>
</ul>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>Adagrad is an algorithm for gradient-based optimization that does just this:<br>It adapts the learning rate to the parameters, </p>
<ul>
<li>performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, </li>
<li>and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</li>
</ul>
<p>For this reason, <strong>it is well-suited for dealing with sparse data.</strong></p>
<p>Previously, we performed an update for all parameters θ at once as every parameter θ<sub>i</sub> used the same learning rate η.<br>As Adagrad uses a different learning rate for every parameter θ<sub>i</sub> at every time step t, we first show Adagrad’s per-parameter update, which we then vectorize.<br>For brevity, we use gt to denote the gradient at time step t. g<sub>t,i</sub> is then the partial derivative of the objective function w.r.t. to the parameter θ<sub>i</sub> at time step t</p>
<p>In its update rule, Adagrad modifies the general learning rate η at each time step t for every parameter θ<sub>i</sub> based on the past gradients that have been computed for θ<sub>i</sub></p>
<p>θ<sub>t+1,i</sub>=θ<sub>t,i</sub>−η/√(G<sub>t,ii</sub>+ϵ)⋅g<sub>t,i</sub></p>
<p>G<sub>t</sub>∈R<sup>d×d</sup> here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θ<sub>i</sub> up to time step t,<br>while ϵ is a smoothing term that avoids division by zero.<br><strong>Interestingly, without the square root operation, the algorithm performs much worse.</strong></p>
<ul>
<li>One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate</li>
<li>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator<blockquote>
<p>Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p>
</blockquote>
</li>
</ul>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate.<br>Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p>
<p>Instead of inefficiently storing w previous squared gradients,<br>the sum of gradients is recursively defined as a decaying average of all past squared gradients. </p>

        </div>
        
            
        
        
    </div>
</article>

        
            
            <article class="article">
    <div class="article-wrap">
        
            <h2 class="article-title cl">
                <a href="/2020/02/01/hello-world/" title="Hello World">
                    Hello World
                </a>
            </h2>
        

        <ul class="article-extra" style="margin-bottom: 20px">
            
            <li class="article-time">
                2020-02-01
            </li>
            

            
                
                <li class="article-category">
                    <ol class="category-list cl">
                        <i class="fa fa-folder-o"></i>
                        
                            
                            <li><a href="/categories/Hexo/">Hexo</a></li>
                            
                        
                    </ol>
                </li>
                
            


        </ul>
        <div class="article-description">
            
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

        </div>
        
            
        
        
    </div>
</article>

        
        <ul class="pagination clear">
    <span class="page-number current">1</span>
</ul>
    </section>
    
<section class="tool-area">

    <div class="toolbar">
        

        
        <div class="widget-post widget" style="order: 1 ">
            <h2 class="widget-title"><i class="fa fa-file-text"></i> Recent articles</h2>
            <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/11/18/Azure-Data-Factory-Data-Flow/">Azure Data Factory (Data Flow)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/03/01/Spark-Dataframe-window-function/">Spark Dataframe window function</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/21/Spark-Optimizaion/">Spark Optimizaion</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/11/Airflow-1/">Airflow-- 1</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/11/Whitening-transformation/">Whitening transformation</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/08/Spark-Structured-Streaming/">Spark Structured Streaming</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/04/Batch-Normalization/">Batch Normalization</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/02/Gradient-Descent/">Gradient Descent</a></li></ul>
        </div>
        

        
        <div class="widget-tags widget" style="order: 2 ">
            <h2 class="widget-title"><i class="fa fa-tags"></i> Tag</h2>
            <a href="/tags/Azure/" style="font-size: 10px;">Azure</a> <a href="/tags/Dataframe/" style="font-size: 10px;">Dataframe</a> <a href="/tags/Optimization/" style="font-size: 10px;">Optimization</a> <a href="/tags/Optimizer/" style="font-size: 20px;">Optimizer</a> <a href="/tags/Streaming/" style="font-size: 10px;">Streaming</a>
        </div>
        

        
        <div class="widget-categories widget" style="order: 3 ">
            <h2 class="widget-title"><i class="fa fa-folder-open"></i> Categories</h2>
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DataFactory/">DataFactory</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/airflow/">airflow</a><span class="category-list-count">1</span></li></ul>
        </div>
        

    </div>
</section>
</div>

    
        <footer class="footer">
	<p class="footer-intro">
		
		@2020 Bin&#39;s Blog.
	</p>
	<p class="footer-intro">
			Powered By <a href="https://hexo.io/zh-cn/" target="_blank">hexo</a>
			theme <a href="https://github.com/iengu/hexo-theme-mokusei" target="_blank">mokusei</a> by <a href="https://www.iengu.com" target="blank">iengu</a>
	</p>
</footer>




        <div class="extend-tools" id="extend-tools" style="display: none;">
    <ul>
        <li class="tools-returnTop" title="Back to top"><i class="fa fa-angle-double-up"></i></li>
    </ul>
</div>

	</div>

	
<script src="/js/org/jquery.min.js"></script>

    
<script src="/js/extend.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

</body>
</html>