{"pages":[],"posts":[{"title":"Airflow-- 1","text":"all in one #调度机中的dag代码示例 #生产机中的dag要放置到与调度机同样的目录下，并且将执行过程增加到*_function() import airflow from airflow.models import DAG from airflow.operators.python_operator import PythonOperator default_args = { 'owner': 'xiaoming', 'start_date': airflow.utils.dates.days_ago(1), 'depends_on_past': False, # 失败发邮件 'email': ['xiaoming@163.com'], 'email_on_failure': True, 'email_on_retry': True, # 重试相关 'retries': 3, 'retry_delay': timedelta(minutes=5), # 并发限制 'pool': 'data_hadoop_pool', 'priority_weight': 900, # 按机器名指定运行位置 'queue': '66.66.0.66:8080' } dag = DAG( dag_id='daily', default_args=default_args, #配置默认参数 schedule_interval='0 13 * * *') #生产机中，将具体执行过程放置在该函数下 def fetch_data_from_hdfs_function(ds, **kwargs): pass #生产机中，将具体执行过程放置在该函数下 def push_data_to_mysql_function(ds, **kwargs): pass fetch_data_from_hdfs = PythonOperator( task_id='fetch_data_from_hdfs', provide_context=True, python_callable=fetch_data_from_hdfs_function, dag=dag) push_data_to_mysql = PythonOperator( task_id='push_data_to_mysql', provide_context=True, python_callable=push_data_to_mysql_function, dag=dag) fetch_data_from_hdfs &gt;&gt; push_data_to_mysql update #该task未修改参数，采用默认参数 fetch_data_from_hdfs = PythonOperator( task_id='fetch_data_from_hdfs', provide_context=True, python_callable=fetch_data_from_hdfs_function, dag=dag) #该task修改通过指定参数，覆盖默认参数，调整调度行为 push_data_to_mysql = PythonOperator( task_id='push_data_to_mysql', queue='77.66.0.66:8080', #通过修改参数，调整调度 pool='data_mysql_pool', #通过修改参数，调整调度 provide_context=True, python_callable=push_data_to_mysql_function, dag=dag) decouple import xx.fetch_data_from_hdfs #将包装成函数的业务代码引入 #生产机中，将具体执行过程放置在该函数下 def fetch_data_from_hdfs_function(ds, **kwargs): if not fetch_data_from_hdfs: #判断业务代码是否执行成功，不成功报错 raise AirflowException('run fail: fetch_data_from_hdfs') fetch_data_from_hdfs = PythonOperator( task_id='fetch_data_from_hdfs', provide_context=True, python_callable=fetch_data_from_hdfs_function, dag=dag)","link":"2020/02/11/Airflow-1/"},{"title":"Batch Normalization","text":"Batch Normalization is one of important parts in our NN. Why need Normalization This paper title tells me the reason Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift accelerating traning reduce internal covariate shift Independent and identically distributed (IID) If our data is independent and identically distributed, training model can be simplified and its predictive ability is improved. One important step of data preparation is whitening which is used to Whitening reduce features’ coralation =&gt; Independent all features have zero mean and unit variances =&gt; Identically distributed Internal Covariate Shift (ICS) What is problem of ICS? Generally data is not IID Previous layer should update hyper-parameters to adjust new data so that reduce learning speed Get stuck in the saturation region as the network grows deeper and network stop learning earlier Covariate Shift What is covariate shift? While in the process X→YX \\rightarrow YX→Y Ptrain(y∣x)=Ptest(y∣x)P^{train}(y|x) = P^{test}(y|x) P​train​​(y∣x)=P​test​​(y∣x) butPtrain(x)≠Ptest(x)but\\; P^{train}(x) \\neq P^{test}(x) butP​train​​(x)≠P​test​​(x) ToDo Normalizations weight scale invariance data scale invariance Batch Normalization Layer Normalization Weight Normalization Cosine Normalization","link":"2020/02/04/Batch-Normalization/"},{"title":"Azure Data Factory (Data Flow)","text":"Recently I’m working in Azure to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work. Task Process CSV files and merge different system files into one file Source: CSV files with filename format (abcd_yyyymmdd_uuid.csv), where abcd is system id. a_20180101_9ca2bed1-2ed0-eaeb-8401-784f43755025.csv a_20180101_cca2bed1-aed0-11eb-8401-784f73755025.csv b_20190202_ece2bed1-2ed0-abeb-8401-784f43755025.csv c_20180101_ada2bed1-2ed0-22eb-8401-784f43755025.csv Sink: yyyymmdd.csv 20180101.csv 20190202.csv ADF Pipeline Activities Get Metadata Input: source directory/parameters Output: metadata of each object Get Metadata activity iterate source directory to obtain each object. The most important one is Argument ForEach Input: output of Get Metadata Output: None ForEach activity is used to process each object in source direcoty. @activity('Get Metadata1').output.childItems Set Variables It’s convenient to predefine a value used in next step. Dataflow","link":"2020/11/18/Azure-Data-Factory-Data-Flow/"},{"title":"Whitening transformation","text":"","link":"2020/02/11/Whitening-transformation/"},{"title":"Spark Dataframe window function","text":"scala ref create dataframe","link":"2020/03/01/Spark-Dataframe-window-function/"},{"title":"Gradient Descent","text":"gradient-based optimization algorithms Gradient Descent variants Batch Gradient Descent (BGD) Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces to a local minimum for non-convex surfaces Stochastic Gradient Descent (SGD) Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily. While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation, enables it to jump to new and potentially better local minima this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. Mini-batch Gradient Descent (MB-GD) Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples reduces the variance of the parameter updates, which can lead to more stable convergence can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used Challenges Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge. Learning rete schedules try to adjust the learning rate during training e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics The same learning rate applies to all parameter updates If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions. Gradient Descent Optimization Algorithms We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton’s method. Momentum SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. Some implementations exchange the signs in the equations. The momentum term γ is usually set to 0.9 or a similar value. When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation. Nesterov Accelerated Gradient We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again. Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term γvθt-1 to move the parameters θ. Computing θ−γvt-1 thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance The distinction between Momentum method and Nesterov Accelerated Gradient updates was Both methods are distinct only when the learning rate η is reasonably large. When the learning rate η is relatively large, Nesterov Accelerated Gradients allows larger decay rate α than Momentum method, while preventing oscillations. Both Momentum method and Nesterov Accelerated Gradient become equivalent when η is small Adagrad Adagrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data. Previously, we performed an update for all parameters θ at once as every parameter θi used the same learning rate η. As Adagrad uses a different learning rate for every parameter θi at every time step t, we first show Adagrad’s per-parameter update, which we then vectorize. For brevity, we use gt to denote the gradient at time step t. gt,i is then the partial derivative of the objective function w.r.t. to the parameter θi at time step t In its update rule, Adagrad modifies the general learning rate η at each time step t for every parameter θi based on the past gradients that have been computed for θi θt+1,i=θt,i−η/√(Gt,ii+ϵ)⋅gt,i Gt∈Rd×d here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θi up to time step t, while ϵ is a smoothing term that avoids division by zero. Interestingly, without the square root operation, the algorithm performs much worse. One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate Adagrad’s main weakness is its accumulation of the squared gradients in the denominator Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw. Adadelta Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w. Instead of inefficiently storing w previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients.","link":"2020/02/02/Gradient-Descent/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post $ hexo new \"My New Post\" More info: Writing Run server $ hexo server More info: Server Generate static files $ hexo generate More info: Generating Deploy to remote sites $ hexo deploy More info: Deployment","link":"2020/02/01/hello-world/"},{"title":"Spark Optimizaion","text":"Spark run faster and faster Cluster Optimization Parameters Optimization Code Optimization Cluster Optimization Locality Level Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from closest to farthest: PROCESS_LOCAL data is in the same JVM as the running code. This is the best locality possible NODE_LOCAL data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes NO_PREF data is accessed equally quickly from anywhere and has no locality preference RACK_LOCAL data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch ANY data is elsewhere on the network and not in the same rack Performance: PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL Locality settting spark.locality.wait.process spark.locality.wait.node spark.locality.wait.rack Data Format text orc parquet avro format setting spark.sql.hive.convertCTAS spark.sql.sources.default parallelising spark.sql.shuffle.partitions : default is 200 computing –executor-memory : default is 1G –executor-cores : default is 1 if large memory cause resource throtle in cluster, if small memory cause task termination if more cores cause IO issue, if less cores slow dow computing memory spark.executor.overhead.memory table join spark.sql.autoBroadcastJoinThreshold : default 10M predicate push down in Spark SQL queries spark.sql.parquet.filterPushdown : default True spark.sql.orc.filterPushdown=true : default False reuse RDD df.persist(pyspark.StorageLevel.MEMORY_ONLY) Spark operators shuffle operators avoid using reduceByKey, join, distinct, repartition etc Broadcast small dataset High performance operator reduceByKey &gt; groupByKey (reduceByKey works at map side) mapPartitions &gt; map (reduce function calls) treeReduce &gt; reduce (treeReduce works at executor not driver) treeReduce &amp; reduce return some result to driver treeReduce does more work on the executors while reduce bring everything back to the driver. foreachPartitions &gt; foreach (reduce function calls) filter -&gt; coalesce (reduce number of partitions and reduce tasks) repartitionAndSortWithinPartitions &gt; repartition &amp; sort broadcast (100M) shuffle spark.shuffle.sort.bypassMergeThreshold spark.shuffle.io.retryWait spark.shuffle.io.maxRetries TBC","link":"2020/02/21/Spark-Optimizaion/"},{"title":"Spark Structured Streaming","text":"Spark Structured Streaming Recently reading a blog Structured Streaming in PySpark It’s implemented in Databricks platform. Then I try to reimplement in my local Spark. Some tricky issue happend during my work. Reading Data from pyspark.sql import SparkSession from pyspark.sql.types import TimestampType, StringType, StructType, StructField spark = SparkSession.builder.appName(\"Test Streaming\").enableHiveSupport().getOrCreate() json_schema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"customer\", StringType(), True), StructField(\"action\", StringType(), True), StructField(\"device\", StringType(), True) ]) file_path = \"local_file_path&lt;file:///...\" read json as same as method in the blog input = spark.read.schema(json_schema).json(file_path) input.show() # +----+--------+------+------+ # |time|customer|action|device| # +----+--------+------+------+ # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # +----+--------+------+------+ input.count() # 20000 All values are null, however, the count is right. It means spark has already read all data but the schema is not correctly mapped. read a single json file to check schema input = spark.read.schema(json_schema).json(file_path+'/1.json') input.show() # +----+--------+------+------+ # |time|customer|action|device| # +----+--------+------+------+ # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # |null| null| null| null| # +----+--------+------+------+ # same error # Then I drop schema option and use inferSchema input = spark.read.json(file_path+'/1.json') input.show() # +--------------------+-----------+-----------------+--------------------+---------------+ # | _corrupt_record| action| customer| device| time| # +--------------------+-----------+-----------------+--------------------+---------------+ # |[{\"time\":\"3:57:09...| null| null| null| null| # | null| power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM| # | null| power on| Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM| # | null| power off| Kippar McCaughen| ecobee4| 5:40:19.000 PM| # | null| power off| Sidney Jotham| GreenIQ Controller| 4:54:28.000 PM| # | null| power off| Fanya Menzies| ecobee4| 3:12:48.000 PM| # | null|low battery| Jeanne Gresch| ecobee4| 5:39:47.000 PM| # | null| power on| Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM| # | null| power off| Merwyn Mix| Amazon Echo| 9:23:41.000 PM| # | null| power off| Angelico Conrath| Amazon Echo| 4:53:13.000 AM| # | null| power on| Gilda Emmett| August Doorbell Cam|12:32:29.000 AM| # | null|low battery| Austine Davsley| ecobee4| 3:35:12.000 AM| # | null|low battery| Zackariah Thoday| Amazon Echo| 1:26:13.000 PM| # | null| power off| Ewen Gillson| Amazon Echo| 7:47:20.000 AM| # | null| power on| Itch Durnill| ecobee4| 4:45:55.000 AM| # | null| power off| Winni Dow| GreenIQ Controller| 4:12:54.000 AM| # | null| power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM| # | null|low battery| Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM| # | null| power off| Christie Karran|Footbot Air Quali...| 9:38:13.000 PM| # | null|low battery| Evonne Guest| Amazon Echo| 8:02:21.000 AM| # +--------------------+-----------+-----------------+--------------------+---------------+ A weird column is _corrupt_record and first value is [{“time”:&quot;3:57:09… in this column. Go back to check source file and notice that it’s a list of object in json file. Remove [ and ] in source file input = spark.read.json(file_path+'/1.json') input.show() # +-----------+-----------------+--------------------+---------------+ # | action| customer| device| time| # +-----------+-----------------+--------------------+---------------+ # | power off| Alexi Barts| GreenIQ Controller| 3:57:09.000 PM| # | power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM| # | power on| Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM| # | power off| Kippar McCaughen| ecobee4| 5:40:19.000 PM| # | power off| Sidney Jotham| GreenIQ Controller| 4:54:28.000 PM| # | power off| Fanya Menzies| ecobee4| 3:12:48.000 PM| # |low battery| Jeanne Gresch| ecobee4| 5:39:47.000 PM| # | power on| Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM| # | power off| Merwyn Mix| Amazon Echo| 9:23:41.000 PM| # | power off| Angelico Conrath| Amazon Echo| 4:53:13.000 AM| # | power on| Gilda Emmett| August Doorbell Cam|12:32:29.000 AM| # |low battery| Austine Davsley| ecobee4| 3:35:12.000 AM| # |low battery| Zackariah Thoday| Amazon Echo| 1:26:13.000 PM| # | power off| Ewen Gillson| Amazon Echo| 7:47:20.000 AM| # | power on| Itch Durnill| ecobee4| 4:45:55.000 AM| # | power off| Winni Dow| GreenIQ Controller| 4:12:54.000 AM| # | power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM| # |low battery| Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM| # | power off| Christie Karran|Footbot Air Quali...| 9:38:13.000 PM| # |low battery| Evonne Guest| Amazon Echo| 8:02:21.000 AM| # +-----------+-----------------+--------------------+---------------+ Woo, the dataframe is correct. Let’s check schema input.printSchema() # root # |-- action: string (nullable = true) # |-- customer: string (nullable = true) # |-- device: string (nullable = true) # |-- time: string (nullable = true) So far I manually modify source file and drop external schema to obtain a corret dataframe. Is there anyway to read these files without these steps. add one feature multiLine Read the file without schema but add one feature multiLine input = spark.read.json(\"file:///path/pyspark_test_data\", multiLine=True) # OR input = spark.read.option('multiLine', True).json(\"file:///path/pyspark_test_data\") # +-----------+--------------------+--------------------+---------------+ # | action| customer| device| time| # +-----------+--------------------+--------------------+---------------+ # | power on| Raynor Blaskett|Nest T3021US Ther...| 3:35:09.000 AM| # | power on|Stafford Blakebrough| GreenIQ Controller|10:59:46.000 AM| # | power on| Alex Woolcocks|Nest T3021US Ther...| 6:26:36.000 PM| # | power on| Clarice Nayshe|Footbot Air Quali...| 4:46:28.000 AM| # | power off| Killie Pirozzi|Footbot Air Quali...| 8:58:43.000 AM| # | power on| Lynne Dymidowicz|Footbot Air Quali...| 4:20:49.000 PM| # | power on| Shaina Dowyer| ecobee4| 3:41:33.000 AM| # |low battery| Barbee Melato| August Doorbell Cam|10:40:24.000 PM| # | power off| Clem Westcot|Nest T3021US Ther...|11:13:38.000 PM| # | power off| Kerri Galfour| Amazon Echo|10:12:15.000 PM| # |low battery| Trev Ashmore| GreenIQ Controller|11:04:41.000 AM| # | power on| Coral Jahnisch| August Doorbell Cam| 3:06:31.000 AM| # | power on| Feliza Cowdrey|Nest T3021US Ther...| 2:49:02.000 AM| # | power off| Amabelle De Haven|Footbot Air Quali...|12:11:59.000 PM| # | power off| Benton Redbourn|Nest T3021US Ther...| 3:57:39.000 AM| # |low battery| Asher Potten| August Doorbell Cam| 1:34:44.000 AM| # |low battery| Lorianne Hullyer| August Doorbell Cam| 7:26:42.000 PM| # | power off| Ruperto Aldcorn|Footbot Air Quali...| 3:54:49.000 AM| # | power on| Agatha Di Giacomo|Footbot Air Quali...| 7:15:20.000 AM| # | power on| Eunice Penwright| ecobee4|11:14:14.000 PM| # +-----------+--------------------+--------------------+---------------+ input.printSchema() # root # |-- action: string (nullable = true) # |-- customer: string (nullable = true) # |-- device: string (nullable = true) # |-- time: string (nullable = true) change the schema Set time as StringType json_schema = StructType([ StructField(\"time\", StringType(), True), StructField(\"customer\", StringType(), True), StructField(\"action\", StringType(), True), StructField(\"device\", StringType(), True) ]) input = spark.read.schema(json_schema).json(\"file:///path/pyspark_test_data\", multiLine=True) input.show() # +---------------+--------------------+-----------+--------------------+ # | time| customer| action| device| # +---------------+--------------------+-----------+--------------------+ # | 3:35:09.000 AM| Raynor Blaskett| power on|Nest T3021US Ther...| # |10:59:46.000 AM|Stafford Blakebrough| power on| GreenIQ Controller| # | 6:26:36.000 PM| Alex Woolcocks| power on|Nest T3021US Ther...| # | 4:46:28.000 AM| Clarice Nayshe| power on|Footbot Air Quali...| # | 8:58:43.000 AM| Killie Pirozzi| power off|Footbot Air Quali...| # | 4:20:49.000 PM| Lynne Dymidowicz| power on|Footbot Air Quali...| # | 3:41:33.000 AM| Shaina Dowyer| power on| ecobee4| # |10:40:24.000 PM| Barbee Melato|low battery| August Doorbell Cam| # |11:13:38.000 PM| Clem Westcot| power off|Nest T3021US Ther...| # |10:12:15.000 PM| Kerri Galfour| power off| Amazon Echo| # |11:04:41.000 AM| Trev Ashmore|low battery| GreenIQ Controller| # | 3:06:31.000 AM| Coral Jahnisch| power on| August Doorbell Cam| # | 2:49:02.000 AM| Feliza Cowdrey| power on|Nest T3021US Ther...| # |12:11:59.000 PM| Amabelle De Haven| power off|Footbot Air Quali...| # | 3:57:39.000 AM| Benton Redbourn| power off|Nest T3021US Ther...| # | 1:34:44.000 AM| Asher Potten|low battery| August Doorbell Cam| # | 7:26:42.000 PM| Lorianne Hullyer|low battery| August Doorbell Cam| # | 3:54:49.000 AM| Ruperto Aldcorn| power off|Footbot Air Quali...| # | 7:15:20.000 AM| Agatha Di Giacomo| power on|Footbot Air Quali...| # |11:14:14.000 PM| Eunice Penwright| power on| ecobee4| # +---------------+--------------------+-----------+--------------------+ Pyspark can load json files successfully without TimestampType. However, how to handle timestamp issue in this job? TimestampType In offical document, the class pyspark.sql.DataFrameReader has one parameter timestampFormat sets the string that indicates a timestamp format. Custom date formats follow the formats at java.text.SimpleDateFormat. This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss.SSSXXX. input = spark.read.schema(schema).option(\"multiLine\", True).json(\"file:///path/pyspark_test_data\", timestampFormat=\"h:mm:ss.SSS aa\") input.show() # +-------------------+--------------------+-----------+--------------------+ # | time| customer| action| device| # +-------------------+--------------------+-----------+--------------------+ # |1970-01-01 03:35:09| Raynor Blaskett| power on|Nest T3021US Ther...| # |1970-01-01 10:59:46|Stafford Blakebrough| power on| GreenIQ Controller| # |1970-01-01 18:26:36| Alex Woolcocks| power on|Nest T3021US Ther...| # |1970-01-01 04:46:28| Clarice Nayshe| power on|Footbot Air Quali...| # |1970-01-01 08:58:43| Killie Pirozzi| power off|Footbot Air Quali...| # |1970-01-01 16:20:49| Lynne Dymidowicz| power on|Footbot Air Quali...| # |1970-01-01 03:41:33| Shaina Dowyer| power on| ecobee4| # |1970-01-01 22:40:24| Barbee Melato|low battery| August Doorbell Cam| # |1970-01-01 23:13:38| Clem Westcot| power off|Nest T3021US Ther...| # |1970-01-01 22:12:15| Kerri Galfour| power off| Amazon Echo| # |1970-01-01 11:04:41| Trev Ashmore|low battery| GreenIQ Controller| # |1970-01-01 03:06:31| Coral Jahnisch| power on| August Doorbell Cam| # |1970-01-01 02:49:02| Feliza Cowdrey| power on|Nest T3021US Ther...| # |1970-01-01 12:11:59| Amabelle De Haven| power off|Footbot Air Quali...| # |1970-01-01 03:57:39| Benton Redbourn| power off|Nest T3021US Ther...| # |1970-01-01 01:34:44| Asher Potten|low battery| August Doorbell Cam| # |1970-01-01 19:26:42| Lorianne Hullyer|low battery| August Doorbell Cam| # |1970-01-01 03:54:49| Ruperto Aldcorn| power off|Footbot Air Quali...| # |1970-01-01 07:15:20| Agatha Di Giacomo| power on|Footbot Air Quali...| # |1970-01-01 23:14:14| Eunice Penwright| power on| ecobee4| # +-------------------+--------------------+-----------+--------------------+ All yyyy-MM-dd are 1970-01-01 because source file only hh-mm-ss. These source files are in wrong format in Windows. Streaming Our Data from pyspark.sql import SparkSession from pyspark.sql.types import TimestampType, StringType, StructType, StructField spark = SparkSession.builder.appName(\"Test Streaming\").enableHiveSupport().getOrCreate() json_schema = StructType([ StructField(\"time\", StringType(), True), StructField(\"customer\", StringType(), True), StructField(\"action\", StringType(), True), StructField(\"device\", StringType(), True) ]) streamingDF = spark.readStream.schema(json_schema) \\ .option(\"maxFilesPerTrigger\", 1) \\ .option(\"multiLine\", True) \\ .json(\"file:///path/pyspark_test_data\") streamingActionCountsDF = streamingDF.groupBy('action').count() # streamingActionCountsDF.isStreaming spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\") # View stream in real-time # query = streamingActionCountsDF.writeStream \\ # .format(\"memory\").queryName(\"counts\").outputMode(\"complete\").start() # format choice: # parquet # kafka # console # memory # query = streamingActionCountsDF.writeStream \\ # .format(\"console\").queryName(\"counts\").outputMode(\"complete\").start() query = streamingActionCountsDF.writeStream.format(\"console\") \\ .queryName(\"counts\").outputMode(\"complete\").start().awaitTermination(timeout=10) # Output Mode choice: # append # complete # update","link":"2020/02/08/Spark-Structured-Streaming/"}],"tags":[{"name":"Optimizer","slug":"Optimizer","link":"tags/Optimizer/"},{"name":"Azure","slug":"Azure","link":"tags/Azure/"},{"name":"Dataframe","slug":"Dataframe","link":"tags/Dataframe/"},{"name":"Optimization","slug":"Optimization","link":"tags/Optimization/"},{"name":"Streaming","slug":"Streaming","link":"tags/Streaming/"}],"categories":[{"name":"airflow","slug":"airflow","link":"categories/airflow/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"categories/Machine-Learning/"},{"name":"DataFactory","slug":"DataFactory","link":"categories/DataFactory/"},{"name":"Spark","slug":"Spark","link":"categories/Spark/"},{"name":"Hexo","slug":"Hexo","link":"categories/Hexo/"}]}