<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Bin&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://binzhango.github.io/"/>
  <updated>2020-12-08T17:11:13.640Z</updated>
  <id>https://binzhango.github.io/</id>
  
  <author>
    <name>Bin Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Azure Data Factory (Data Flow)</title>
    <link href="https://binzhango.github.io/2020/11/18/Azure-Data-Factory-Data-Flow/"/>
    <id>https://binzhango.github.io/2020/11/18/Azure-Data-Factory-Data-Flow/</id>
    <published>2020-11-19T02:27:29.000Z</published>
    <updated>2020-12-08T17:11:13.640Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I’m working in Azure to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work.</p><a id="more"></a><h1 id="Task1"><a href="#Task1" class="headerlink" title="Task1"></a>Task1</h1><p>Process CSV files and merge different system files into one file</p><ul><li>Source: CSV files with filename format (<em>abcd_yyyymmdd_uuid.csv</em>), where abcd is system id.<ul><li>a_20180101_9ca2bed1-2ed0-eaeb-8401-784f43755025.csv</li><li>a_20180101_cca2bed1-aed0-11eb-8401-784f73755025.csv</li><li>b_20190202_ece2bed1-2ed0-abeb-8401-784f43755025.csv</li><li>c_20180101_ada2bed1-2ed0-22eb-8401-784f43755025.csv</li></ul></li><li>Sink: yyyymmdd.csv<ul><li>20180101.csv</li><li>20190202.csv</li></ul></li></ul><h2 id="ADF-Pipeline"><a href="#ADF-Pipeline" class="headerlink" title="ADF Pipeline"></a>ADF Pipeline</h2><p><img src="/images/adf_pipeline.png" alt="Pipeline" title="Pipeline"><br><img src="/images/parameter.png" alt="" title="parameters"><br><img src="/images/variables.png" alt="" title="variables"></p><h2 id="Activities"><a href="#Activities" class="headerlink" title="Activities"></a>Activities</h2><h3 id="Get-Metadata"><a href="#Get-Metadata" class="headerlink" title="Get Metadata"></a>Get Metadata</h3><ul><li>Input: source directory/parameters</li><li>Output: metadata of each object</li></ul><p>Get Metadata activity iterate source directory to obtain each object. The most important one is <strong>Argument</strong><br><img src="/images/getmetadata.png" alt="Get Metadata" title="Get Metadata"></p><h3 id="ForEach"><a href="#ForEach" class="headerlink" title="ForEach"></a>ForEach</h3><ul><li>Input: output of <em>Get Metadata</em></li><li>Output: None</li></ul><p>ForEach activity is used to process each object in source direcoty.</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@activity(<span class="string">'Get Metadata1'</span>).output.childItems</span><br></pre></td></tr></table></figure><p><img src="/images/foreach.png" alt="" title="ForEach"></p><h3 id="Set-Variables"><a href="#Set-Variables" class="headerlink" title="Set Variables"></a>Set Variables</h3><p>It’s convenient to predefine a value used in next step.</p><p><img src="/images/variable_activity.png" alt="" title="Set Variable"></p><h3 id="Dataflow"><a href="#Dataflow" class="headerlink" title="Dataflow"></a>Dataflow</h3><p><img src="/images/dataflow.png" alt="" title="Dataflow"></p><p>The dataflow merge all files with same date, and source1 and sink are the same destination.<br>So, initially source1 is empty and check this options.<br><img src="/images/source1.png" alt="" title="source1"></p><p>The only configuration in Sink is the <span style="color: rgb(0, 200,200)"> <em>File name option</em> </span><br><img src="/images/sink.png" alt="" title="sink"></p><h4 id="Aggregation-of-filenames"><a href="#Aggregation-of-filenames" class="headerlink" title="Aggregation of filenames"></a>Aggregation of filenames</h4><p>The last problem in dataflow is how to merge files with same date in dataflow, which means we firstly find out all these files.<br>The solution to this problems is regex expression.</p><p><img src="/images/source2.png" alt="" title="source2"></p><h1 id="Task2"><a href="#Task2" class="headerlink" title="Task2"></a>Task2</h1><p>Generally CSV file has a header and we can process it easily in ADF. However, a special case is a large CSV file has multiple different headers and we need to automatically split it into regular csv files with headers respectively.</p><ul><li><p>Sample data:</p><blockquote><p>h1,h1_col1,h1_col2,h1_col3<br>h2,h2_col1,h2_col2,h2_col3,h2_col4,h2_col5<br>h3,h3_col1,h3_col2<br>h1,h1_row1_1,h1_row1_2,h1_row1_3<br>h1,h1_row2_1,h1_row2_2,h1_row2_3<br>h1,h1_row3_1,h1_row3_2,h1_row3_3<br>h2,h2_row1_1,h2_row1_2,h2_row1_3,h2_row1_4,h2_row1_5<br>h2,h2_row2_1,h2_row2_2,h2_row2_3,h2_row2_4,h2_row2_5<br>h2,h2_row3_1,h2_row3_2,h2_row3_3,h2_row3_4,h2_row3_5<br>h2,h2_row4_1,h2_row4_2,h2_row4_3,h2_row4_4,h2_row4_5<br>h2,h2_row5_1,h2_row5_2,h2_row5_3,h2_row5_4,h2_row5_5<br>h3,h3_row1_1,h3_row1_2<br>h3,h3_row2_1,h3_row2_2</p></blockquote></li><li><p>Explanation:</p><ul><li>header format: <em>header name</em>, <em>columns names</em></li><li>3 headers : h1, h2 and h3</li><li>the 1st column of each row is header name and rest of columns are values</li></ul></li><li><p>Output:</p><ul><li>h1 file<blockquote><p>h1_col1,h1_col2,h1_col3<br>h1_row1_1,h1_row1_2,h1_row1_3<br>h1_row2_1,h1_row2_2,h1_row2_3<br>h1_row3_1,h1_row3_2,h1_row3_3</p></blockquote></li><li>h2 file<blockquote><p>h2_col1,h2_col2,h2_col3,h2_col4,h2_col5<br>h2_row1_1,h2_row1_2,h2_row1_3,h2_row1_4,h2_row1_5<br>h2_row2_1,h2_row2_2,h2_row2_3,h2_row2_4,h2_row2_5<br>h2_row3_1,h2_row3_2,h2_row3_3,h2_row3_4,h2_row3_5<br>h2_row4_1,h2_row4_2,h2_row4_3,h2_row4_4,h2_row4_5<br>h2_row5_1,h2_row5_2,h2_row5_3,h2_row5_4,h2_row5_5</p></blockquote></li><li>h3 file<blockquote><p>h3_col1,h3_col2<br>h3_row1_1,h3_row1_2<br>h3_row2_1,h3_row2_2</p></blockquote></li></ul></li></ul><h2 id="Dataflow-1"><a href="#Dataflow-1" class="headerlink" title="Dataflow"></a>Dataflow</h2><p><img src="/images/dataflow1.png" alt="" title="dataflow1"></p><p>The dataset used in <em>source</em> and <em>sink</em> must uncheck this </p><p><img src="/images/dataset1.png" alt="" title="dataset"></p><h3 id="DerivedColumn"><a href="#DerivedColumn" class="headerlink" title="DerivedColumn"></a>DerivedColumn</h3><p>Because no header is in the dataset, ADF automatically assign a column name to each one.<br>The column name format is <strong>_col<span style="color: rgb(0, 200,200)">index</span>_</strong></p><p>In this task the header column is <span style="color: rgb(0, 200,200)">_col0_</span> and we can map this one to another name like <strong>filename</strong><br><img src="/images/derived_col" alt="" title="col"></p><h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><p><img src="/images/sink1.png" alt="" title="sink1"></p><p>This dataflow will automatically split composite CSV file into different files and save them at container root path. To save them at another directory, you can add folder name to the mapping column name in DerivedColumn activity.</p><h2 id="Trigger"><a href="#Trigger" class="headerlink" title="Trigger"></a>Trigger</h2><p>We use blob event trigger to implement automation. Once uploading a new file is done, these pipeline will process it automatically.<br><a href="https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger" target="_blank" rel="external">How to create event trigger</a></p><p>Two values in trigger are used by pipeline</p><ul><li>@triggerBody().folderPath : /container name/folder/</li><li>@triggerBody().fileName : blob name</li></ul><h3 id="Pandas-Processing"><a href="#Pandas-Processing" class="headerlink" title="Pandas Processing"></a>Pandas Processing</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">'sample.csv'</span>, sep=<span class="string">'^([^,]+),'</span>,engine=<span class="string">'python'</span>, header=<span class="keyword">None</span>)</span><br><span class="line">df.drop(df.columns[<span class="number">0</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">heads = df[df.columns[<span class="number">0</span>]].unique()</span><br><span class="line">d = dict(tuple(df.groupby(df.columns[<span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> h <span class="keyword">in</span> heads:</span><br><span class="line">    outputfile = d[h]</span><br><span class="line">    outputfile.drop(outputfile.columns[<span class="number">0</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">    outputfile.to_csv(<span class="string">'&#123;0&#125;.csv'</span>.format(h), sep=<span class="string">' '</span>, index=<span class="keyword">False</span>, header=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Recently I’m working in Azure to implement ETL jobs. The main tool is ADF (Azure Data Factory). This post show some solutions to resolve issue in my work.&lt;/p&gt;
    
    </summary>
    
      <category term="DataFactory" scheme="https://binzhango.github.io/categories/DataFactory/"/>
    
    
      <category term="Azure" scheme="https://binzhango.github.io/tags/Azure/"/>
    
  </entry>
  
  <entry>
    <title>Spark Dataframe window function</title>
    <link href="https://binzhango.github.io/2020/03/01/Spark-Dataframe-window-function/"/>
    <id>https://binzhango.github.io/2020/03/01/Spark-Dataframe-window-function/</id>
    <published>2020-03-01T23:35:36.000Z</published>
    <updated>2020-11-26T12:21:26.451Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" target="_blank" rel="external">scala ref</a></p><h2 id="create-dataframe"><a href="#create-dataframe" class="headerlink" title="create dataframe"></a>create dataframe</h2><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html&quot;&gt;scala ref&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;create-dataframe&quot;&gt;&lt;a href=&quot;#create-dataframe&quot; class=&quot;headerlink&quot; title=&quot;create dataframe&quot;&gt;&lt;/a&gt;create dataframe&lt;/h2&gt;
    
    </summary>
    
      <category term="Spark" scheme="https://binzhango.github.io/categories/Spark/"/>
    
    
      <category term="Dataframe" scheme="https://binzhango.github.io/tags/Dataframe/"/>
    
  </entry>
  
  <entry>
    <title>Spark Optimizaion</title>
    <link href="https://binzhango.github.io/2020/02/21/Spark-Optimizaion/"/>
    <id>https://binzhango.github.io/2020/02/21/Spark-Optimizaion/</id>
    <published>2020-02-21T19:10:36.000Z</published>
    <updated>2020-11-26T12:22:36.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Spark-run-faster-and-faster"><a href="#Spark-run-faster-and-faster" class="headerlink" title="Spark run faster and faster"></a>Spark run faster and faster</h1><ul><li>Cluster Optimization</li><li>Parameters Optimization</li><li>Code Optimization<a id="more"></a></li></ul><h2 id="Cluster-Optimization"><a href="#Cluster-Optimization" class="headerlink" title="Cluster Optimization"></a>Cluster Optimization</h2><h4 id="Locality-Level"><a href="#Locality-Level" class="headerlink" title="Locality Level"></a>Locality Level</h4><p>Data locality is how close data is to the code processing it. There are several levels of locality based on the data’s current location. In order from closest to farthest:</p><ul><li><strong>PROCESS_LOCAL</strong> data is in the same JVM as the running code. This is the best locality possible</li><li><strong>NODE_LOCAL</strong> data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li><li><strong>NO_PREF</strong> data is accessed equally quickly from anywhere and has no locality preference</li><li><strong>RACK_LOCAL</strong> data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li><li><strong>ANY</strong> data is elsewhere on the network and not in the same rack</li></ul><p>Performance: PROCESS_LOCAL &gt; NODE_LOCAL &gt; NO_PREF &gt; RACK_LOCAL</p><h6 id="Locality-settting"><a href="#Locality-settting" class="headerlink" title="Locality settting"></a>Locality settting</h6><ul><li>spark.locality.wait.process</li><li>spark.locality.wait.node</li><li>spark.locality.wait.rack</li></ul><h4 id="Data-Format"><a href="#Data-Format" class="headerlink" title="Data Format"></a>Data Format</h4><ul><li>text</li><li>orc</li><li>parquet</li><li>avro<h6 id="format-setting"><a href="#format-setting" class="headerlink" title="format setting"></a>format setting</h6></li><li>spark.sql.hive.convertCTAS</li><li>spark.sql.sources.default</li></ul><h4 id="parallelising"><a href="#parallelising" class="headerlink" title="parallelising"></a>parallelising</h4><ul><li>spark.sql.shuffle.partitions : default is 200</li></ul><h4 id="computing"><a href="#computing" class="headerlink" title="computing"></a>computing</h4><ul><li>–executor-memory : default is 1G</li><li>–executor-cores : default is 1<br>if large memory cause resource throtle in cluster, if small memory cause task termination<br>if more cores cause IO issue, if less cores slow dow computing</li></ul><h4 id="memory"><a href="#memory" class="headerlink" title="memory"></a>memory</h4><ul><li>spark.executor.overhead.memory</li></ul><h4 id="table-join"><a href="#table-join" class="headerlink" title="table join"></a>table join</h4><ul><li>spark.sql.autoBroadcastJoinThreshold : default 10M</li></ul><h4 id="predicate-push-down-in-Spark-SQL-queries"><a href="#predicate-push-down-in-Spark-SQL-queries" class="headerlink" title="predicate push down in Spark SQL queries"></a>predicate push down in Spark SQL queries</h4><ul><li>spark.sql.parquet.filterPushdown : default True</li><li>spark.sql.orc.filterPushdown=true : default False</li></ul><h4 id="reuse-RDD"><a href="#reuse-RDD" class="headerlink" title="reuse RDD"></a>reuse RDD</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.persist(pyspark.StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure><h4 id="Spark-operators"><a href="#Spark-operators" class="headerlink" title="Spark operators"></a>Spark operators</h4><ul><li><p>shuffle operators</p><ul><li>avoid using <span style="color:blue"> <strong>reduceByKey</strong>, <strong>join</strong>, <strong>distinct</strong>, <strong>repartition</strong> etc</span></li><li>Broadcast small dataset</li></ul></li><li><p>High performance operator</p><ul><li>reduceByKey &gt; groupByKey (reduceByKey works at map side)</li><li>mapPartitions &gt; map (reduce function calls)</li><li>treeReduce &gt; reduce (treeReduce works at executor not driver)<ul><li>treeReduce &amp; reduce return some result to driver</li><li>treeReduce does more work on the executors while reduce bring everything back to the driver.</li></ul></li><li>foreachPartitions &gt; foreach (reduce function calls)</li><li>filter -&gt; coalesce (reduce number of partitions and reduce tasks)</li><li>repartitionAndSortWithinPartitions &gt; repartition &amp; sort</li><li>broadcast (100M)</li></ul></li></ul><h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><ul><li>spark.shuffle.sort.bypassMergeThreshold</li><li>spark.shuffle.io.retryWait</li><li>spark.shuffle.io.maxRetries</li></ul><p>TBC</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Spark-run-faster-and-faster&quot;&gt;&lt;a href=&quot;#Spark-run-faster-and-faster&quot; class=&quot;headerlink&quot; title=&quot;Spark run faster and faster&quot;&gt;&lt;/a&gt;Spark run faster and faster&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Cluster Optimization&lt;/li&gt;
&lt;li&gt;Parameters Optimization&lt;/li&gt;
&lt;li&gt;Code Optimization
    
    </summary>
    
      <category term="Spark" scheme="https://binzhango.github.io/categories/Spark/"/>
    
    
      <category term="Optimization" scheme="https://binzhango.github.io/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>Airflow-- 1</title>
    <link href="https://binzhango.github.io/2020/02/11/Airflow-1/"/>
    <id>https://binzhango.github.io/2020/02/11/Airflow-1/</id>
    <published>2020-02-12T03:20:40.000Z</published>
    <updated>2020-12-05T22:39:10.561Z</updated>
    
    <content type="html"><![CDATA[<h2 id="all-in-one"><a href="#all-in-one" class="headerlink" title="all in one"></a>all in one</h2><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"><span class="keyword">import</span> airflow</span><br><span class="line"><span class="keyword">from</span> airflow.models <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.operators.python_operator <span class="keyword">import</span> PythonOperator</span><br><span class="line">  </span><br><span class="line">default_args = &#123;</span><br><span class="line"><span class="string">'owner'</span>: <span class="string">'ABC'</span>,</span><br><span class="line"><span class="string">'start_date'</span>: airflow.utils.dates.days_ago(<span class="number">1</span>),</span><br><span class="line"><span class="string">'depends_on_past'</span>: <span class="keyword">False</span>,</span><br><span class="line">    <span class="comment"># failure email</span></span><br><span class="line"><span class="string">'email'</span>: [<span class="string">'abc@xxx.com'</span>],</span><br><span class="line"><span class="string">'email_on_failure'</span>: <span class="keyword">True</span>,</span><br><span class="line"><span class="string">'email_on_retry'</span>: <span class="keyword">True</span>,</span><br><span class="line"><span class="string">'retries'</span>: <span class="number">3</span>,</span><br><span class="line"><span class="string">'retry_delay'</span>: timedelta(minutes=<span class="number">5</span>),</span><br><span class="line"><span class="string">'pool'</span>: <span class="string">'data_hadoop_pool'</span>,</span><br><span class="line"><span class="string">'priority_weight'</span>: <span class="number">900</span>,</span><br><span class="line"><span class="string">'queue'</span>: <span class="string">'66.66.0.66:8080'</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">dag = DAG(</span><br><span class="line">    dag_id=<span class="string">'daily'</span>, </span><br><span class="line">    default_args=default_args,</span><br><span class="line">    schedule_interval=<span class="string">'0 13 * * *'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_data_from_hdfs_function</span><span class="params">(ds, **kwargs)</span>:</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">push_data_to_mysql_function</span><span class="params">(ds, **kwargs)</span>:</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">fetch_data_from_hdfs = PythonOperator(</span><br><span class="line">task_id=<span class="string">'fetch_data_from_hdfs'</span>,</span><br><span class="line">provide_context=<span class="keyword">True</span>,</span><br><span class="line">python_callable=fetch_data_from_hdfs_function,</span><br><span class="line">dag=dag)</span><br><span class="line"> </span><br><span class="line">push_data_to_mysql = PythonOperator(</span><br><span class="line">task_id=<span class="string">'push_data_to_mysql'</span>,</span><br><span class="line">provide_context=<span class="keyword">True</span>,</span><br><span class="line">python_callable=push_data_to_mysql_function,</span><br><span class="line">dag=dag)</span><br><span class="line"> </span><br><span class="line">fetch_data_from_hdfs &gt;&gt; push_data_to_mysql</span><br></pre></td></tr></table></figure><h2 id="update"><a href="#update" class="headerlink" title="update"></a>update</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#default parameters</span></span><br><span class="line">fetch_data_from_hdfs = PythonOperator(</span><br><span class="line">task_id=<span class="string">'fetch_data_from_hdfs'</span>,</span><br><span class="line">provide_context=<span class="keyword">True</span>,</span><br><span class="line">python_callable=fetch_data_from_hdfs_function,</span><br><span class="line">dag=dag)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#overwrite parameters</span></span><br><span class="line">push_data_to_mysql = PythonOperator(</span><br><span class="line">    task_id=<span class="string">'push_data_to_mysql'</span>,</span><br><span class="line">    queue=<span class="string">'77.66.0.66:8080'</span>, <span class="comment">#update</span></span><br><span class="line">    pool=<span class="string">'data_mysql_pool'</span>, <span class="comment">#update</span></span><br><span class="line">    provide_context=<span class="keyword">True</span>,</span><br><span class="line">    python_callable=push_data_to_mysql_function,</span><br><span class="line">    dag=dag)</span><br></pre></td></tr></table></figure><h2 id="decouple"><a href="#decouple" class="headerlink" title="decouple"></a>decouple</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xx.fetch_data_from_hdfs </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_data_from_hdfs_function</span><span class="params">(ds, **kwargs)</span>:</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> fetch_data_from_hdfs: </span><br><span class="line">        <span class="keyword">raise</span> AirflowException(<span class="string">'run fail: fetch_data_from_hdfs'</span>)</span><br><span class="line"> </span><br><span class="line">fetch_data_from_hdfs = PythonOperator(</span><br><span class="line">task_id=<span class="string">'fetch_data_from_hdfs'</span>,</span><br><span class="line">provide_context=<span class="keyword">True</span>,</span><br><span class="line">python_callable=fetch_data_from_hdfs_function,</span><br><span class="line">dag=dag)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;all-in-one&quot;&gt;&lt;a href=&quot;#all-in-one&quot; class=&quot;headerlink&quot; title=&quot;all in one&quot;&gt;&lt;/a&gt;all in one&lt;/h2&gt;
    
    </summary>
    
      <category term="airflow" scheme="https://binzhango.github.io/categories/airflow/"/>
    
    
  </entry>
  
  <entry>
    <title>Whitening transformation</title>
    <link href="https://binzhango.github.io/2020/02/11/Whitening-transformation/"/>
    <id>https://binzhango.github.io/2020/02/11/Whitening-transformation/</id>
    <published>2020-02-11T13:17:08.000Z</published>
    <updated>2020-11-26T12:22:32.442Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="Machine Learning" scheme="https://binzhango.github.io/categories/Machine-Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark Structured Streaming</title>
    <link href="https://binzhango.github.io/2020/02/08/Spark-Structured-Streaming/"/>
    <id>https://binzhango.github.io/2020/02/08/Spark-Structured-Streaming/</id>
    <published>2020-02-09T04:23:13.000Z</published>
    <updated>2020-11-26T12:22:26.423Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark-Structured-Streaming"><a href="#Spark-Structured-Streaming" class="headerlink" title="Spark Structured Streaming"></a>Spark Structured Streaming</h2><p>Recently reading a blog <a href="https://hackersandslackers.com/structured-streaming-in-pyspark/" target="_blank" rel="external">Structured Streaming in PySpark</a><br>It’s implemented in Databricks platform. Then I try to implement in my local Spark.<br>Some tricky issue happened during my work.</p><a id="more"></a><h2 id="Reading-Data"><a href="#Reading-Data" class="headerlink" title="Reading Data"></a>Reading Data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType, StringType, StructType, StructField</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"Test Streaming"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line">json_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"time"</span>, TimestampType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"customer"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"action"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"device"</span>, StringType(), <span class="keyword">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">"local_file_path&lt;file:///..."</span></span><br></pre></td></tr></table></figure><h4 id="read-json-as-same-as-method-in-the-blog"><a href="#read-json-as-same-as-method-in-the-blog" class="headerlink" title="read json as same as method in the blog"></a>read json as same as method in the blog</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.schema(json_schema).json(file_path)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |time|customer|action|device|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line">input.count()</span><br><span class="line"><span class="comment"># 20000</span></span><br></pre></td></tr></table></figure><p>All values are null, however, the count is right. It means spark has already read all data but the schema is not correctly mapped.</p><h4 id="read-a-single-json-file-to-check-schema"><a href="#read-a-single-json-file-to-check-schema" class="headerlink" title="read a single json file to check schema"></a>read a single json file to check schema</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.schema(json_schema).json(file_path+<span class="string">'/1.json'</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |time|customer|action|device|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># |null|    null|  null|  null|</span></span><br><span class="line"><span class="comment"># +----+--------+------+------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># same error</span></span><br><span class="line"><span class="comment"># Then I drop schema option and use inferSchema</span></span><br><span class="line">input = spark.read.json(file_path+<span class="string">'/1.json'</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |     _corrupt_record|     action|         customer|              device|           time|</span></span><br><span class="line"><span class="comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |[&#123;"time":"3:57:09...|       null|             null|                null|           null|</span></span><br><span class="line"><span class="comment"># |                null|  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|</span></span><br><span class="line"><span class="comment"># |                null|   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|</span></span><br><span class="line"><span class="comment"># |                null|low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|</span></span><br><span class="line"><span class="comment"># +--------------------+-----------+-----------------+--------------------+---------------+</span></span><br></pre></td></tr></table></figure><p>A weird column is <em>_corrupt_record</em> and first value is <strong>[{“time”:”3:57:09…</strong> in this column.<br>Go back to check source file and notice that it’s a list of object in json file.</p><h6 id="Remove-and-in-source-file"><a href="#Remove-and-in-source-file" class="headerlink" title="Remove   [  and ]  in source file"></a>Remove  <span style="color:red"> <em>[</em> </span> and <span style="color:red"><em>]</em> </span> in source file</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.json(file_path+<span class="string">'/1.json'</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |     action|         customer|              device|           time|</span></span><br><span class="line"><span class="comment"># +-----------+-----------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |  power off|      Alexi Barts|  GreenIQ Controller| 3:57:09.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|   Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off| Kippar McCaughen|             ecobee4| 5:40:19.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|    Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|    Fanya Menzies|             ecobee4| 3:12:48.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|    Jeanne Gresch|             ecobee4| 5:39:47.000 PM|</span></span><br><span class="line"><span class="comment"># |   power on|    Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|       Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off| Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|     Gilda Emmett| August Doorbell Cam|12:32:29.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|  Austine Davsley|             ecobee4| 3:35:12.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery| Zackariah Thoday|         Amazon Echo| 1:26:13.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|     Ewen Gillson|         Amazon Echo| 7:47:20.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|     Itch Durnill|             ecobee4| 4:45:55.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off|        Winni Dow|  GreenIQ Controller| 4:12:54.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|Talbot Valentelli| August Doorbell Cam| 7:35:23.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|    Vikki Muckeen| August Doorbell Cam| 1:17:30.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|  Christie Karran|Footbot Air Quali...| 9:38:13.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|     Evonne Guest|         Amazon Echo| 8:02:21.000 AM|</span></span><br><span class="line"><span class="comment"># +-----------+-----------------+--------------------+---------------+</span></span><br></pre></td></tr></table></figure><p>Woo, the dataframe is correct. Let’s check schema<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input.printSchema()</span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- action: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- customer: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- device: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- time: string (nullable = true)</span></span><br></pre></td></tr></table></figure></p><p>So far I manually modify source file and drop external schema to obtain a corret dataframe. Is there anyway to<br>read these files without these steps.</p><h6 id="add-one-feature-multiLine"><a href="#add-one-feature-multiLine" class="headerlink" title="add one feature multiLine"></a>add one feature <span style="color:blue">multiLine</span></h6><p>Read the file without schema but add one feature <strong>multiLine</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.json(<span class="string">"file:///path/pyspark_test_data"</span>, multiLine=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># OR input = spark.read.option('multiLine', True).json("file:///path/pyspark_test_data")</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># +-----------+--------------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |     action|            customer|              device|           time|</span></span><br><span class="line"><span class="comment"># +-----------+--------------------+--------------------+---------------+</span></span><br><span class="line"><span class="comment"># |   power on|     Raynor Blaskett|Nest T3021US Ther...| 3:35:09.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|Stafford Blakebrough|  GreenIQ Controller|10:59:46.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|      Alex Woolcocks|Nest T3021US Ther...| 6:26:36.000 PM|</span></span><br><span class="line"><span class="comment"># |   power on|      Clarice Nayshe|Footbot Air Quali...| 4:46:28.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off|      Killie Pirozzi|Footbot Air Quali...| 8:58:43.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|    Lynne Dymidowicz|Footbot Air Quali...| 4:20:49.000 PM|</span></span><br><span class="line"><span class="comment"># |   power on|       Shaina Dowyer|             ecobee4| 3:41:33.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|       Barbee Melato| August Doorbell Cam|10:40:24.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|        Clem Westcot|Nest T3021US Ther...|11:13:38.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|       Kerri Galfour|         Amazon Echo|10:12:15.000 PM|</span></span><br><span class="line"><span class="comment"># |low battery|        Trev Ashmore|  GreenIQ Controller|11:04:41.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|      Coral Jahnisch| August Doorbell Cam| 3:06:31.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|      Feliza Cowdrey|Nest T3021US Ther...| 2:49:02.000 AM|</span></span><br><span class="line"><span class="comment"># |  power off|   Amabelle De Haven|Footbot Air Quali...|12:11:59.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|     Benton Redbourn|Nest T3021US Ther...| 3:57:39.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|        Asher Potten| August Doorbell Cam| 1:34:44.000 AM|</span></span><br><span class="line"><span class="comment"># |low battery|    Lorianne Hullyer| August Doorbell Cam| 7:26:42.000 PM|</span></span><br><span class="line"><span class="comment"># |  power off|     Ruperto Aldcorn|Footbot Air Quali...| 3:54:49.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|   Agatha Di Giacomo|Footbot Air Quali...| 7:15:20.000 AM|</span></span><br><span class="line"><span class="comment"># |   power on|    Eunice Penwright|             ecobee4|11:14:14.000 PM|</span></span><br><span class="line"><span class="comment"># +-----------+--------------------+--------------------+---------------+</span></span><br><span class="line"></span><br><span class="line">input.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment">#  |-- action: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- customer: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- device: string (nullable = true)</span></span><br><span class="line"><span class="comment">#  |-- time: string (nullable = true)</span></span><br></pre></td></tr></table></figure><h4 id="change-the-schema"><a href="#change-the-schema" class="headerlink" title="change the schema"></a>change the schema</h4><p>Set time as <em>StringType</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">json_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"time"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"customer"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"action"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"device"</span>, StringType(), <span class="keyword">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">input = spark.read.schema(json_schema).json(<span class="string">"file:///path/pyspark_test_data"</span>, multiLine=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># +---------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># |           time|            customer|     action|              device|</span></span><br><span class="line"><span class="comment"># +---------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># | 3:35:09.000 AM|     Raynor Blaskett|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |10:59:46.000 AM|Stafford Blakebrough|   power on|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># | 6:26:36.000 PM|      Alex Woolcocks|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># | 4:46:28.000 AM|      Clarice Nayshe|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 8:58:43.000 AM|      Killie Pirozzi|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 4:20:49.000 PM|    Lynne Dymidowicz|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 3:41:33.000 AM|       Shaina Dowyer|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># |10:40:24.000 PM|       Barbee Melato|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |11:13:38.000 PM|        Clem Westcot|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |10:12:15.000 PM|       Kerri Galfour|  power off|         Amazon Echo|</span></span><br><span class="line"><span class="comment"># |11:04:41.000 AM|        Trev Ashmore|low battery|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># | 3:06:31.000 AM|      Coral Jahnisch|   power on| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># | 2:49:02.000 AM|      Feliza Cowdrey|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |12:11:59.000 PM|   Amabelle De Haven|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 3:57:39.000 AM|     Benton Redbourn|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># | 1:34:44.000 AM|        Asher Potten|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># | 7:26:42.000 PM|    Lorianne Hullyer|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># | 3:54:49.000 AM|     Ruperto Aldcorn|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># | 7:15:20.000 AM|   Agatha Di Giacomo|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |11:14:14.000 PM|    Eunice Penwright|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># +---------------+--------------------+-----------+--------------------+</span></span><br></pre></td></tr></table></figure></p><p>Pyspark can load json files successfully without TimestampType. However, how to handle timestamp issue in this job?</p><h4 id="TimestampType"><a href="#TimestampType" class="headerlink" title="TimestampType"></a>TimestampType</h4><p>In offical document, the class <em>pyspark.sql.DataFrameReader</em> has one parameter</p><ul><li>timestampFormat <blockquote><p>sets the string that indicates a timestamp format. </p><p>Custom date formats follow the formats at java.text.SimpleDateFormat. </p><p>This applies to timestamp type. If None is set, it uses the default value, yyyy-MM-dd’T’HH:mm:ss.SSSXXX.</p></blockquote></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input = spark.read.schema(schema).option(<span class="string">"multiLine"</span>, <span class="keyword">True</span>).json(<span class="string">"file:///path/pyspark_test_data"</span>, timestampFormat=<span class="string">"h:mm:ss.SSS aa"</span>)</span><br><span class="line"></span><br><span class="line">input.show()</span><br><span class="line"><span class="comment"># +-------------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># |               time|            customer|     action|              device|</span></span><br><span class="line"><span class="comment"># +-------------------+--------------------+-----------+--------------------+</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:35:09|     Raynor Blaskett|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 10:59:46|Stafford Blakebrough|   power on|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># |1970-01-01 18:26:36|      Alex Woolcocks|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 04:46:28|      Clarice Nayshe|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 08:58:43|      Killie Pirozzi|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 16:20:49|    Lynne Dymidowicz|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:41:33|       Shaina Dowyer|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># |1970-01-01 22:40:24|       Barbee Melato|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 23:13:38|        Clem Westcot|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 22:12:15|       Kerri Galfour|  power off|         Amazon Echo|</span></span><br><span class="line"><span class="comment"># |1970-01-01 11:04:41|        Trev Ashmore|low battery|  GreenIQ Controller|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:06:31|      Coral Jahnisch|   power on| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 02:49:02|      Feliza Cowdrey|   power on|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 12:11:59|   Amabelle De Haven|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:57:39|     Benton Redbourn|  power off|Nest T3021US Ther...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 01:34:44|        Asher Potten|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 19:26:42|    Lorianne Hullyer|low battery| August Doorbell Cam|</span></span><br><span class="line"><span class="comment"># |1970-01-01 03:54:49|     Ruperto Aldcorn|  power off|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 07:15:20|   Agatha Di Giacomo|   power on|Footbot Air Quali...|</span></span><br><span class="line"><span class="comment"># |1970-01-01 23:14:14|    Eunice Penwright|   power on|             ecobee4|</span></span><br><span class="line"><span class="comment"># +-------------------+--------------------+-----------+--------------------+</span></span><br></pre></td></tr></table></figure><p>All yyyy-MM-dd are 1970-01-01 because source file only hh-mm-ss.<br>These source files are in wrong format in Windows.</p><h2 id="Streaming-Our-Data"><a href="#Streaming-Our-Data" class="headerlink" title="Streaming Our Data"></a>Streaming Our Data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> TimestampType, StringType, StructType, StructField</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"Test Streaming"</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"></span><br><span class="line">json_schema = StructType([</span><br><span class="line">    StructField(<span class="string">"time"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"customer"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"action"</span>, StringType(), <span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"device"</span>, StringType(), <span class="keyword">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">streamingDF = spark.readStream.schema(json_schema) \</span><br><span class="line">              .option(<span class="string">"maxFilesPerTrigger"</span>, <span class="number">1</span>) \</span><br><span class="line">              .option(<span class="string">"multiLine"</span>, <span class="keyword">True</span>) \</span><br><span class="line">              .json(<span class="string">"file:///path/pyspark_test_data"</span>)</span><br><span class="line"></span><br><span class="line">streamingActionCountsDF = streamingDF.groupBy(<span class="string">'action'</span>).count()</span><br><span class="line"><span class="comment"># streamingActionCountsDF.isStreaming</span></span><br><span class="line">spark.conf.set(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="string">"2"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># View stream in real-time</span></span><br><span class="line"><span class="comment"># query = streamingActionCountsDF.writeStream \</span></span><br><span class="line"><span class="comment">#         .format("memory").queryName("counts").outputMode("complete").start()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># format choice:</span></span><br><span class="line"><span class="comment"># parquet</span></span><br><span class="line"><span class="comment"># kafka</span></span><br><span class="line"><span class="comment"># console</span></span><br><span class="line"><span class="comment"># memory</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># query = streamingActionCountsDF.writeStream \</span></span><br><span class="line"><span class="comment">#         .format("console").queryName("counts").outputMode("complete").start()</span></span><br><span class="line"></span><br><span class="line">query = streamingActionCountsDF.writeStream.format(<span class="string">"console"</span>) \</span><br><span class="line">        .queryName(<span class="string">"counts"</span>).outputMode(<span class="string">"complete"</span>).start().awaitTermination(timeout=<span class="number">10</span>)</span><br><span class="line"><span class="comment"># Output Mode choice:</span></span><br><span class="line"><span class="comment"># append</span></span><br><span class="line"><span class="comment"># complete</span></span><br><span class="line"><span class="comment"># update</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Spark-Structured-Streaming&quot;&gt;&lt;a href=&quot;#Spark-Structured-Streaming&quot; class=&quot;headerlink&quot; title=&quot;Spark Structured Streaming&quot;&gt;&lt;/a&gt;Spark Structured Streaming&lt;/h2&gt;&lt;p&gt;Recently reading a blog &lt;a href=&quot;https://hackersandslackers.com/structured-streaming-in-pyspark/&quot;&gt;Structured Streaming in PySpark&lt;/a&gt;&lt;br&gt;It’s implemented in Databricks platform. Then I try to implement in my local Spark.&lt;br&gt;Some tricky issue happened during my work.&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="https://binzhango.github.io/categories/Spark/"/>
    
    
      <category term="Streaming" scheme="https://binzhango.github.io/tags/Streaming/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization</title>
    <link href="https://binzhango.github.io/2020/02/04/Batch-Normalization/"/>
    <id>https://binzhango.github.io/2020/02/04/Batch-Normalization/</id>
    <published>2020-02-04T13:15:15.000Z</published>
    <updated>2020-11-26T03:44:08.744Z</updated>
    
    <content type="html"><![CDATA[<p>Batch Normalization is one of important parts in our NN.</p><h2 id="Why-need-Normalization"><a href="#Why-need-Normalization" class="headerlink" title="Why need Normalization"></a>Why need Normalization</h2><p>This paper title tells me the reason<br><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><ul><li>accelerating traning</li><li>reduce internal covariate shift</li></ul><a id="more"></a><h4 id="Independent-and-identically-distributed-IID"><a href="#Independent-and-identically-distributed-IID" class="headerlink" title="Independent and identically distributed (IID)"></a>Independent and identically distributed (IID)</h4><p>If our data is independent and identically distributed, training model can be simplified and its predictive ability is improved.<br>One important step of data preparation is <strong>whitening</strong> which is used to</p><h6 id="Whitening"><a href="#Whitening" class="headerlink" title="Whitening"></a>Whitening</h6><ul><li>reduce features’ coralation     =&gt; Independent</li><li>all features have zero mean and unit variances =&gt; Identically distributed</li></ul><h4 id="Internal-Covariate-Shift-ICS"><a href="#Internal-Covariate-Shift-ICS" class="headerlink" title="Internal Covariate Shift (ICS)"></a>Internal Covariate Shift (ICS)</h4><p>What is problem of ICS? Generally data is not IID</p><ul><li>Previous layer should update hyper-parameters to adjust new data so that reduce learning speed</li><li>Get stuck in the saturation region as the network grows deeper and network stop learning earlier</li></ul><h6 id="Covariate-Shift"><a href="#Covariate-Shift" class="headerlink" title="Covariate Shift"></a>Covariate Shift</h6><blockquote><p>What is covariate shift? While in the process $X \rightarrow Y$<br>$$P^{train}(y|x) = P^{test}(y|x)$$<br>$$but\; P^{train}(x) \neq P^{test}(x)$$</p></blockquote><h1 id="ToDo"><a href="#ToDo" class="headerlink" title="ToDo"></a>ToDo</h1><h2 id="Normalizations"><a href="#Normalizations" class="headerlink" title="Normalizations"></a>Normalizations</h2><ul><li>weight scale invariance</li><li>data scale invariance</li></ul><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><h4 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h4><h4 id="Weight-Normalization"><a href="#Weight-Normalization" class="headerlink" title="Weight Normalization"></a>Weight Normalization</h4><h4 id="Cosine-Normalization"><a href="#Cosine-Normalization" class="headerlink" title="Cosine Normalization"></a>Cosine Normalization</h4>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Batch Normalization is one of important parts in our NN.&lt;/p&gt;
&lt;h2 id=&quot;Why-need-Normalization&quot;&gt;&lt;a href=&quot;#Why-need-Normalization&quot; class=&quot;headerlink&quot; title=&quot;Why need Normalization&quot;&gt;&lt;/a&gt;Why need Normalization&lt;/h2&gt;&lt;p&gt;This paper title tells me the reason&lt;br&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;accelerating traning&lt;/li&gt;
&lt;li&gt;reduce internal covariate shift&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://binzhango.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Optimizer" scheme="https://binzhango.github.io/tags/Optimizer/"/>
    
  </entry>
  
  <entry>
    <title>Gradient Descent</title>
    <link href="https://binzhango.github.io/2020/02/02/Gradient-Descent/"/>
    <id>https://binzhango.github.io/2020/02/02/Gradient-Descent/</id>
    <published>2020-02-03T02:04:06.000Z</published>
    <updated>2020-11-26T03:44:19.457Z</updated>
    
    <content type="html"><![CDATA[<h1 id="gradient-based-optimization-algorithms"><a href="#gradient-based-optimization-algorithms" class="headerlink" title="gradient-based optimization algorithms"></a>gradient-based optimization algorithms</h1><a id="more"></a><h2 id="Gradient-Descent-variants"><a href="#Gradient-Descent-variants" class="headerlink" title="Gradient Descent variants"></a>Gradient Descent variants</h2><h4 id="Batch-Gradient-Descent-BGD"><a href="#Batch-Gradient-Descent-BGD" class="headerlink" title="Batch Gradient Descent (BGD)"></a>Batch Gradient Descent (BGD)</h4><p>Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ</p><p>Batch gradient descent is guaranteed to converge </p><ul><li>to the global minimum for convex error surfaces</li><li>to a local minimum for non-convex surfaces</li></ul><h4 id="Stochastic-Gradient-Descent-SGD"><a href="#Stochastic-Gradient-Descent-SGD" class="headerlink" title="Stochastic Gradient Descent (SGD)"></a>Stochastic Gradient Descent (SGD)</h4><p>Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update.<br>SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.<br>SGD performs frequent updates with a high variance that cause the objective function to <em>fluctuate</em> heavily.<br>While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation,</p><ul><li>enables it to jump to new and potentially better local minima</li><li>this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting</li></ul><p>when we slowly decrease the learning rate, SGD shows the same convergence behavior as batch gradient descent, almost certainly converging to a <em>local</em> or the <em>global</em> minimum for <em>non-convex</em> and <em>convex</em> optimization respectively.</p><h4 id="Mini-batch-Gradient-Descent-MB-GD"><a href="#Mini-batch-Gradient-Descent-MB-GD" class="headerlink" title="Mini-batch Gradient Descent (MB-GD)"></a>Mini-batch Gradient Descent (MB-GD)</h4><p>Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples</p><ul><li>reduces the variance of the parameter updates, which can lead to more stable convergence</li><li>can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient</li><li>Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used</li></ul><h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><ul><li><p><strong>Choosing a proper learning rate can be difficult.</strong></p><blockquote><p>A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</p></blockquote></li><li><p><strong>Learning rete schedules try to adjust the learning rate during training</strong></p><blockquote><p>e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics</p></blockquote></li><li><p><strong>The same learning rate applies to all parameter updates</strong></p><blockquote><p>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features</p></blockquote></li><li><p><strong>Minimizing high non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima</strong></p><blockquote><p>The difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</p></blockquote></li></ul><h2 id="Gradient-Descent-Optimization-Algorithms"><a href="#Gradient-Descent-Optimization-Algorithms" class="headerlink" title="Gradient Descent Optimization Algorithms"></a>Gradient Descent Optimization Algorithms</h2><p>We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton’s method.</p><h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima.</p><p>Some implementations exchange the signs in the equations. The momentum term γ is usually set to 0.9 or a similar value.</p><p>When using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill,<br>becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1).<br><em>The same thing happens to our parameter updates</em>: </p><blockquote><p>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain <em>faster convergence and reduced oscillation</em>.</p></blockquote><h4 id="Nesterov-Accelerated-Gradient"><a href="#Nesterov-Accelerated-Gradient" class="headerlink" title="Nesterov Accelerated Gradient"></a>Nesterov Accelerated Gradient</h4><p>We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.<br>Nesterov Accelerated Gradient (NAG) is a way to give our momentum term this kind of prescience.<br>We know that we will use our momentum term γvθ<sub>t-1</sub> to move the parameters θ.<br>Computing θ−γv<sub>t-1</sub> thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update),<br>a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient<br><em>not w.r.t. to our current parameters θ but w.r.t. the approximate future position of our parameters</em></p><p>we are able to adapt our updates to the slope of our error function and speed up SGD in turn,<br>we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance</p><p>The distinction between Momentum method and Nesterov Accelerated Gradient updates was</p><ul><li>Both methods are distinct only when the learning rate η is reasonably large. </li><li>When the learning rate η is relatively large, Nesterov Accelerated Gradients allows larger decay rate α than Momentum method, while preventing oscillations. </li><li>Both Momentum method and Nesterov Accelerated Gradient <strong>become equivalent when η is small</strong></li></ul><h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p>Adagrad is an algorithm for gradient-based optimization that does just this:<br>It adapts the learning rate to the parameters, </p><ul><li>performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, </li><li>and larger updates (i.e. high learning rates) for parameters associated with infrequent features.</li></ul><p>For this reason, <strong>it is well-suited for dealing with sparse data.</strong></p><p>Previously, we performed an update for all parameters θ at once as every parameter θ<sub>i</sub> used the same learning rate η.<br>As Adagrad uses a different learning rate for every parameter θ<sub>i</sub> at every time step t, we first show Adagrad’s per-parameter update, which we then vectorize.<br>For brevity, we use gt to denote the gradient at time step t. g<sub>t,i</sub> is then the partial derivative of the objective function w.r.t. to the parameter θ<sub>i</sub> at time step t</p><p>In its update rule, Adagrad modifies the general learning rate η at each time step t for every parameter θ<sub>i</sub> based on the past gradients that have been computed for θ<sub>i</sub></p><p>θ<sub>t+1,i</sub>=θ<sub>t,i</sub>−η/√(G<sub>t,ii</sub>+ϵ)⋅g<sub>t,i</sub></p><p>G<sub>t</sub>∈R<sup>d×d</sup> here is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. θ<sub>i</sub> up to time step t,<br>while ϵ is a smoothing term that avoids division by zero.<br><strong>Interestingly, without the square root operation, the algorithm performs much worse.</strong></p><ul><li>One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate</li><li>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator<blockquote><p>Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.</p></blockquote></li></ul><h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>Adadelta is an extension of Adagrad that seeks to its aggressive, monotonically decreasing learning rate.<br>Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</p><p>Instead of inefficiently storing w previous squared gradients,<br>the sum of gradients is recursively defined as a decaying average of all past squared gradients. </p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;gradient-based-optimization-algorithms&quot;&gt;&lt;a href=&quot;#gradient-based-optimization-algorithms&quot; class=&quot;headerlink&quot; title=&quot;gradient-based optimization algorithms&quot;&gt;&lt;/a&gt;gradient-based optimization algorithms&lt;/h1&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://binzhango.github.io/categories/Machine-Learning/"/>
    
    
      <category term="Optimizer" scheme="https://binzhango.github.io/tags/Optimizer/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://binzhango.github.io/2020/02/01/hello-world/"/>
    <id>https://binzhango.github.io/2020/02/01/hello-world/</id>
    <published>2020-02-01T15:50:08.822Z</published>
    <updated>2020-11-26T03:44:28.551Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><a id="more"></a><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;Quick-Start&quot;&gt;&lt;a href=&quot;#Quick-Start&quot; class=&quot;headerlink&quot; title=&quot;Quick Start&quot;&gt;&lt;/a&gt;Quick Start&lt;/h2&gt;&lt;h3 id=&quot;Create-a-new-post&quot;&gt;&lt;a href=&quot;#Create-a-new-post&quot; class=&quot;headerlink&quot; title=&quot;Create a new post&quot;&gt;&lt;/a&gt;Create a new post&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hexo new &lt;span class=&quot;string&quot;&gt;&quot;My New Post&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;a href=&quot;https://hexo.io/docs/writing.html&quot;&gt;Writing&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Hexo" scheme="https://binzhango.github.io/categories/Hexo/"/>
    
    
  </entry>
  
</feed>
